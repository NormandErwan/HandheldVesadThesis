\chapter{Revue de littérature}
\label{ch:litterature}

\section{Définitions}
\subsection{Définition de la réalité augmentée}
La réalité augmentée (RA) est, selon l'Office québécois de la langue française, une «~technique d'imagerie numérique […] permettant, grâce à un dispositif d'affichage transparent, de superposer à une image réelle des informations provenant d'une source numérique~» \citep{OfficeQuebecoisLangueFrancaiseRA2015}. La RA consiste donc à combiner du contenu virtuel, généré par un système informatique, à l'environnement réel d'un utilisateur, et cela en temps réel. Ainsi la RA permet d'\emph{augmenter la perception} du réel par les sens humains et permet d'\emph{augmenter les interactions} possibles d'un utilisateur avec l'environnement. \citep{Azuma1997}

- http://doc-ok.org/?p=337
    - Reprendre Azura 1997 : contenu 3D virtuel, aligné avec le contenu réel, en temps réel
    - Stereoscopie, effet de parallaxe)

\subsection{Les techniques de réalité augmentée}
% TODO : est-ce à la bonne place ? Faire plus de contenu pour faire le point pas seulement en output, mais aussi en input ? Relire \cite{BimberRaskar2005}

Il existe aujourd'hui plusieurs techniques de RA, classées en trois catégories \reffigureETS{BimberRaskar2005-Figure31}. La catégorie dominant actuellement le marché est celle des visiocasques (en anglais : \foreignlanguage{english}{Head-Mounted Display} ou foreignlanguage{english}{Head-Worn Display}) \citep{VanKrevelenPoelman2010} : on le voit par la présence médiatique des visiocasques grand public récents tels que l'Oculus Rift, le HTC Vive ou le Microsoft HoloLens. Les techniques de cette catégorie consistent à placer un casque devant les yeux de l'utilisateur pour y diffuser la RA. Un premier type de ces techniques, dites \foreignlanguage{english}{video see-through}, vont remplacer l'environnement visible par une image filmée et augmentée de cet environnement. Cela va se faire en utilisant des caméras à l'avant du casque, en modifiant les images filmées, pour les renvoyer à l'écran du casque. Un second type de ces techniques, dites \foreignlanguage{english}{optical see-through}, vont laisser voir à l'utilisateur directement l'environnement, et, par un jeu de miroirs et de lentilles, vont y superposer les images de RA.

\figureETS{BimberRaskar2005-Figure31.png}{Catégories des techniques d'affichages de RA.\\ Tiré de \citet[p. 72]{BimberRaskar2005}}{BimberRaskar2005-Figure31}

Seule la catégorie des visiocasques sera explorée dans cette revue de littérature. Elle est en effet celle qui est la plus utilisée en recherche actuellement et est la plus prometteuse pour réaliser de la RA utilisable au quotidien. \cite{CarmignianiFurhtAnisettiEtAl2011} De plus, des visiocasques sous forme de lentilles de contacts sont en développement et pourraient devenir la future technique de RA dominant le marché par la discrétion et la légèreté qu'elles permettent. \citep{VanKrevelenPoelman2010}


\section{Rapide historique de la réalité augmentée}
\subsection{Les débuts du domaine de recherche}
% TODO : améliorer le contenu avec \cite{Chalon2004}

Le domaine de recherche de la RA débute doucement dans les années 60. C'est \citet{Sutherland1968} qui développa le premier prototype de RA : ce visiocasque permettait déjà de visualiser du contenu virtuel en 3D, affiché face à l'utilisateur du visiocasque selon sa perspective de l'espace virtuel donc depuis la position de sa tête, lui donnant ainsi l'illusion d'un contenu virtuel réellement présent dans l'espace. La recherche se développe cependant par la suite lentement dans les décennies suivantes \citep{VanKrevelenPoelman2010} \citep{CarmignianiFurhtAnisettiEtAl2011}.

\subsection{L'établissement du domaine de recherche}
C'est dans les années 90 où la RA devient un domaine de recherche à part entière. Cela se voit tout d'abord par la création de plusieurs conférences dédiées, réunies aujourd'hui sous le nom de International Symposium on Mixed and Augmented Reality (ISMAR), une conférence désormais majeure pour la recherche et l'industrie \citep{AzumaBaillotBehringerEtAl2001}.

En outre, \citet{MilgramKishino1994} réalisent une clarification des concepts du domaine, encore en usage aujourd'hui, en proposant une échelle ordonnée nommée \foreignlanguage{english}{Reality-Virtuality Continuum} \reffigureETS{MilgramKishino1994-Figure1}, sur laquelle sont opposés deux extremums : les environnements réels et les environnements de réalité virtuelle (RV). Un environnement de RV est alors entendu comme immergeant totalement l'utilisateur dans un monde virtuel. Ainsi, tout environnement, tel que la RA, mélangeant à la fois des éléments réels et virtuels se situe donc entre ces deux extremums et est considéré comme de la réalité mixte (RM) \citep{MilgramKishino1994}.

Enfin, \citet{Azuma1997} propose une première définition formelle de la RA : il indique qu'un environnement RA doit 
\begin{enumerate}
  \item combiner des éléments réels et virtuels
  \item être interactif en temps réel
  \item les éléments virtuels et réels doivent être alignés dans l'environnement
\end{enumerate}.
\citeauthor{Azuma1997} réalise dans ce même article le premier état de l'art du domaine en détaillant les avancées de la RA et en analysant les défis à relever. Il relève en particulier que peu de prototypes de RA ont pu maturer jusqu'à un stade commercialisable, les raisons étant principalement d'ordre technologiques : les capteurs ne sont pas assez précis pour aligner les éléments réels et virtuels, et le temps réel est difficilement atteignable par le manque de puissance des face aux calculs nécessaires.

\figureETS{MilgramKishino1994-Figure1.png}{L'échelle \foreignlanguage{english}{Reality-Virtuality Continuum} de Milgram.\\ Tiré de \citet[p. 3]{MilgramKishino1994}}{MilgramKishino1994-Figure1}

\subsection{Développement du domaine de recherche vers une RA mobile}
À partir des années 2000, avec la révolution des téléphones intelligents, le RA est amenée à devenir mobile et portable pour pouvoir s'émanciper des simples prototypes des laboratoires. Dans un second état de l'art complétant le premier, le domaine s'étant développé rapidement depuis 1997, \citet{AzumaBaillotBehringerEtAl2001} notent que les progrès techniques sur les capteurs et sur les capacités de calculs permet désormais de concevoir des RA mobiles. Cela pouvait se réaliser, par exemple, par un PC portable embarqué avec des capteurs dans un sac à dos. Cependant, ce genre d'équipement était présentait les inconvénient d'être lourd et encombrant. \citep{DeSaChurchill2013}

Pour réaliser de telles RA mobiles, il a alors été intéressant d'utiliser les téléphones intelligents. En effet, leur puissance, leur légèreté, ainsi que leurs capacités ont explosés et leur utilisation massive a permis de concevoir des capteurs bons marchés et efficaces : les téléphones intelligents ont donc toutes les capacités pour pour créer des systèmes RA légers, et facilement portables par un utilisateur. \citep{ZhouDuhBillinghurst2008} \citep{DeSaChurchill2013} Pour \citet{VanKrevelenPoelman2010}, l'un des plus grands potentiels de la RA est sous une forme mobile. En effet, la RA pourrait être ainsi utilisée au quotidien de manière transparente, naturelle et légère. En outre, \citet{CarmignianiFurhtAnisettiEtAl2011} constatent dans leur état de l'art de la RA, qu'entre les années 2002 et 2010, de plus en plus de prototypes de RA sont réalisés sur des systèmes mobiles : car, c'est sous cette forme que la RA a le plus de chance de réussir auprès du grand public. Enfin, dans un état de l'art récent plus spécifique à la RA mobile, \citet{HuangHuiPeyloEtAl2013} concluent que malgré les challenges techniques encore non résolus de performances, de coûts, d'efficacité énergétique, ou de poids, la RA mobile a déjà montré qu'elle fonctionne et qu'elle peut devenir, pour ses utilisateurs, un moyen important d’interaction avec leur environnement. Ainsi, si la recherche dans la RA s'est beaucoup développée depuis 25 ans, elle s'est surtout consacrée et se consacre toujours à la résolution de problèmes techniques dans le but de la rendre réalisable.



\section{Conception d'interfaces humain-machine pour une réalité augmentée mobile, portable et spatiale}
\subsection{La recherche en interfaces humain-machine pour la réalité augmentée}
Si la RA est techniquement réalisable sur des appareils mobiles, les IHM proposés pour son utilisation ne sont pas encore matures. \citet{ZhouDuhBillinghurst2008} et \citet{DeSaChurchill2013} indiquent que trop peu de travaux ont été consacrés aux IHM et à l'expérience utilisateur en RA. C'est un challenge pourtant important, et cela reste un problème ouvert, car \citet{VanKrevelenPoelman2010} rappellent qu'aucun paradigme d'IHM pertinent n'a encore été trouvé pour la RA. En effet, la métaphore du bureau WIMP (pour \foreignlanguage{english}{windows}, \foreignlanguage{english}{icons}, \foreignlanguage{english}{menus} et \foreignlanguage{english}{pointing device}) utilisé par les IHM des systèmes d'exploitations des ordinateurs ne fonctionne pas en RA, tout comme les dispositifs d'entrées en 2D tels que la souris : ils sont fait pour fonctionner sous la contrainte d'un plan 2D et restreignent alors l'expérience de la RA. \citep{VanKrevelenPoelman2010} A l'inverse, les dispositifs d'interactions dit naturels, sans aucune contraintes dans l'espace, c'est-à-dire avec six degrés de libertés (6 DoF), sont, en réalité, difficiles à manipuler avec dans un environnement virtuel. \citet{ChanKaoChenEtAl2010} ont en particulier montré que sans retour tactile il est difficile pour une personne d'estimer sans erreur la profondeur. Un paradigme d'IHM différent et approprié pour la RA, est donc à déterminer.

Plusieurs types d'IHM pour la RA sont à explorer comme futures pistes de recherches. \citet{AzumaBaillotBehringerEtAl2001} en suggèrent deux~; premièrement, ils proposent d'utiliser conjointement plusieurs dispositifs d'entrée ou de sortie pour exploiter les avantages de chacun d'entre eux : ainsi il est possible pour l'utilisateur d'interagir, selon la tache, avec le ou les dispositif(s) présentant la meilleure interaction : ce sont des \emph{IHM hybrides}. \citep{ZhouDuhBillinghurst2008}  Cette idée rejoint celle des \emph{IHM multimodales}, développée par \citet{Oviatt2003}, qui propose de créer des systèmes réagissant à plusieurs modes d'entrées différents. Ainsi l'utilisateur peut agir sur le système par la voix, avec un ou des doigts, par des mouvements de main, avec les yeux, avec un stylet ou par des mouvements du corps. L'idée est que l'utilisateur peut choisir le mode d'entrée qu'il désire, ou en utiliser plusieurs à la fois. Chaque mode peut être ainsi utilisé en fonction de la tache, ou du contexte, et ce de manière totalement flexible. \citep{CarmignianiFurhtAnisettiEtAl2011}\\
Deuxièmement, \citet{AzumaBaillotBehringerEtAl2001} proposent d'utiliser les outils et objets de l'environnement réel pour interagir avec l'environnement virtuel, permettant ainsi de créer des \emph{IHM tangibles} (en anglais : \foreignlanguage{english}{Tangible User Interfaces (TUIs)}). Les objets physiques ont l'avantage d'êtres familiers, facile à utiliser et de présenter des contraintes physiques sur lesquelles l'IHM peut s'appuyer \citep{ZhouDuhBillinghurst2008}. La voie de recherche de la RA tangible a été initiée par \citet{FeinerMacIntyreHauptEtAl1993} qui ont proposé un prototype accrochant des fenêtres virtuelles aux objets~; \citeauthor{FeinerMacIntyreHauptEtAl1993} parlaient alors, à juste titre, de \emph{design spatial}. Cependant, \citeauthor{ZhouDuhBillinghurst2008} soulignent que l'utilisation de telle IHM pose le défi de faire comprendre à l'utilisateur les commandes possibles avec les objets physiques et les conséquences de ces actions.


\subsection{Le problème de recherche : l'utilisation conjointe des visiocasques et des téléphones intelligents pour la conception d'une interface humain-machine pour une réalité augmentée mobile, portable et spatiale}
L'utilisation de téléphones intelligents pour la RA présente des inconvénients importants \footnote{Cela pourrait être un facteur expliquant, entre autres, la encore faible adoption de la RA par le grand public.} pour une bonne utilisabilité au quotidien. On peut noter que leurs écrans sont petits \citep{DeSaChurchill2013} et que le téléphone doit être tenu à bout de bras à la hauteur des yeux pour que la caméra puisse être alignée avec le regard de l'utilisateur, ce qui peut se révéler fatiguant pour l'utilisateur et donc limiter l'usage du système de RA \citep{Hincapie-RamosGuoMoghadasianEtAl2014}. Ce type de système n'est donc pas viable pour une utilisation quotidienne.

Une alternative pourrait se trouver dans l'utilisation de visiocasques pour réaliser une expérience de RA mobile, portable, légère et transparente pour l'utilisateur. \citet{KoelleKranzMoeller2015} rappellent que l'expérimentation de Google Glass a permis de montrer, malgré les problèmes de vie privée et d'acceptation sociale qu'elle a posée, qu'il était intéressant de combiner un visiocasque avec un téléphone intelligent : le système proposait à l'utilisateur d'interagir avec son téléphone via le visiocasque par des interactions multimodales à la voix ou avec des gestes en l'air lus et décodés par la caméra du visiocasque. Ainsi, pour \citet{HuangHuiPeyloEtAl2013} l'avenir de la RA mobile se trouve dans l'utilisation de visiocasques de ce type. Ce point de vue est également défendu par \citet{SwanGabbard2005} pour qui l'information va se trouver révolutionnée dans l'utilisation d'ordinateurs mobiles, embarqués et portés par l'utilisateur, la RA étant une composante importante de ces nouveaux ordinateurs. De manière générale, \citet{SerranoEnsYangEtAl2015} soulignent que les visiocasques présentent les avantages, par rapport aux téléphones intelligents, de pouvoir laisser les mains libres et de pouvoir présenter en permanence de l'information aux yeux de l'utilisateur. Pour réaliser un tel visiocasque portable, les technologies des téléphones mobiles pourraient y être transférées et utilisées pour permettre à de tels visiocasques de créer une expérience de RA en autonomie.

S'ils sont techniquement réalisables, les possibilités des visiocasques ne sont pas pleinement exploitées. Une limite du Google Glass est que son IHM est affichée seulement sur un plan virtuel à une distance fixe des yeux. Ce n'est alors pas totalement de la RA au sens de la définition de \citet{AzumaBaillotBehringerEtAl2001}, car les éléments virtuels ne sont pas alignés avec les éléments réels. De plus, \citet{SerranoEnsYangEtAl2015} rappellent que ce plan peut faire occlusion avec l'environnement réel, et gêner la vue de l'utilisateur. En outre, les mains étant libres d'utiliser un autre système tel qu'un ordinateur de bureau ou téléphone intelligent, \citet{SerranoEnsYangEtAl2015b} proposent de faire travailler en synergie un système de RA d'un visiocasque avec les autres systèmes informatiques de l'utilisateur. Les téléphones intelligents étant actuellement très répandus, et les IHM pour la RA n'étant pas encore matures, la RA mobile ne va probablement pas remplacer les téléphones intelligents dans les prochaines années. Enfin, le téléphone étant tangible, il serait alors intéressant de mettre à profit un visiocasque pour augmenter l'écran du téléphone en RA avec du contenu virtuel et augmenter les interactions possibles avec le téléphone et le contenu virtuel de la RA. 

Certaines études \citep{EnsFinneganIrani2014} \citep{GrubertHeinischQuigleyEtAl2015} \citep{SerranoEnsYangEtAl2015} \citep{SerranoEnsYangEtAl2015b} ont proposé quelques solutions d'IHM pour une RA utilisant un téléphone intelligent, mais la première étude de cette liste n'a produit qu'une évaluation informelle tandis que les autres n'ont pas réalisé d'évaluation expérimentales. Nous souhaitons savoir quelle IHM serait la plus pertinente pour une telle RA. Pour cela nous souhaitons mener des évaluations pour comparer les différentes IHM possibles.
% TODO : « We believe that novel devices will not replace mouse/keyboard and touchscreens in the following decade. » \cite{JankowskiHachet2013} donc de manière générale le mobile va pas être remplacé. Ceci dit est-ce qu'on peut infirmer cette affirmation pour un tel système ? cad existe-t-il des "nouvelles" interactions qui fonctionneraient mieux dans ce système que simplement le touch (souris inaplicable pour cellulaire) ?


\subsection{Travaux reliés}
\subsubsection{Interfaces humain-machine multi-affichages}
\paragraph{\foreignlanguage{english}{The Personal Cockpit: A Spatial Interface for Effective Task Switching on Head-Worn Displays}}
\citet{EnsFinneganIrani2014} partent de ce même constat sur les IHM des visiocasques actuels : un visiocasque peut afficher du contenu en 3D dans l'environnement, et pourrait permettre d'exploiter l'espace de la vision périphérique de l'utilisateur. Ainsi, plusieurs fenêtres pourraient être affichées en même temps à des endroits différents dans l'espace de la vision d'un utilisateur.\\
Pour explorer les capacités des visiocasques, \citeauthor{EnsFinneganIrani2014} réalisent un prototype de RA affichant de multiples fenêtres dans les airs \reffigureETS{EnsFinneganIrani2014-Figure8}. Un utilisateur peut alors interagir directement avec les fenêtres par le toucher (c'est donc une \emph{interaction directe}). Ils explorent alors plusieurs facteurs de conception du prototype, au travers de quatre expériences : la taille d'affichage des fenêtres virtuelles, leur distance d'affichage, leur angle de placement par rapport à l'utilisateur, et enfin leur référentiel de placement des fenêtres virtuelle (fixées au corps ou fixées au monde). Les performances (temps et taux d'erreurs), la fatigue ainsi que les retours des utilisateurs étaient mesurés.\\ 
Leurs résultats ont montrés, entre autres, qu'un affichage courbe est important pour que toutes les fenêtres soient affichées à la même distance de l'utilisateur~; ce qui confirme les résultats de \citet{ShuppBallYostEtAl2006} pour les affichages virtuels. En outre, leur conception a montré qu'elle permettait un travail 40\% plus rapide qu'avec une fenêtre simple sur une tache multi-applications : en effet, leur prototype permet d'afficher plusieurs fenêtres simultanément et ainsi réduit le changement de fenêtre, donc d'application pour du multi-tâches, à un simple mouvement de tête. Ce résultat montre également que l'interaction directe est compatible avec du contenu virtuel sous forme de fenêtres. Enfin, le référentiel sur le corps avait de plus grands taux d'erreurs que le référentiel sur le monde, car l'action de sélection entraînait des perturbations involontaires des fenêtres par rapport à l'utilisateur.\\ 
Ce travail présente cependant quelques limites. Tout d'abord, leur IHM requiert des interactions directes, ce qui est à l'origine d'erreurs de sélection et de fatigue du bras des utilisateurs : il serait alors intéressant de savoir si un référentiel sur le corps permettrait d'être précis avec une interaction indirecte. En outre, le champ de vision était très limité : 30° à l'horizontal et 40° à la verticale, ce qui biaise les mesures de performances Enfin, le prototype a été réalisé dans un CAVE, c'est-à-dire des projections de l'image sur des écrans entourant l'utilisateur. Ainsi, il faudrait reproduire les expérimentations avec un visiocasque et différentes techniques d'interactions. On peut également se demander si l'utilisation de fenêtres est une représentation adaptée pour la RA, aucune contrainte d'écrans physiques n'étant présente, ou s'il existe une meilleure représentation de l'information virtuelle dans un espace de RA.

[Probleme de recherche]
Il y a une prolifération de HMD peu cher, qui permettent d'interagir avec du contenu en permanence, mais qui m'affiche que des interface en 2D à distance fixe de l'utilisateur. Il n'y a pas d'exploitation de la
 périphérie de l'utilisateur et peut créer des problèmes d'occlusion avec le monde réel. De plus, on retrouve le même problème que les smartphone où le multitache demande de switcher entre des applications, ce
qui coûte cher. Pourtant un HMD n'a pas de limites physiques et peut créer des IHM dans l'espace (en 3D et non plus sur un écran 2D), et composées de plusieurs fenêtres.

[Solution]
Explorer la conception d'interfaces spatiale mobiles, avec de multiples affichages flottants dans les airs à l'aide un prototype. Pour cela, déterminer quels sont les meilleurs facteurs de design pour concevoir
 un tel prototype, et les valider avec expérimentations empiriques. Enfin, faire des recommendations pour al conception d'IHM dans l'espace.
Les facteurs explorés sont :
- le FOV : un humain a 200° en horizontal, 130° en vertical, fovea est 3°h8°v, un fov mini pour un hmd est 40°h (si pour fovea) ou 60°h (pour immersion)
- le changement de contexte : nombre d'écran et multitache, les mvt de têtes doivent être minisé pour pas impacter perfs, et certaines taches valent le coup d'un écran étendu, consitance spatiale est bonne pour
 les perfs
- la séparation angulaire (angle de placement du contenu dans l'espace) : meilleur perfs si petits angles, jusqu'à 85° de rot h et 50° rot haut et 60-70° rot bas
- la taille d'affichage (ou résoluton) : plus haute meilleur pour navigation, certaines taches (par ex: recherche mais pas tracé de route) profitent d'un plus grand affichage, mais souvent le fait de pouvoir se déplacer dans l'espace est plus important que d'avoir un grand affichage
- la distance d'affichage des fenêtres : 0.25m minimum mais 1m recommandé pour écrans physiques, impacts sur perfs si affichages à distances différentes, en VR la prof est mal estimée
- méthode d'entrée : directe est plus naturelle mais pose des problémes car vr intangible et problèmes d'estimation de distances
- placement des affichages : sur le monde ou les objets, sur la tête, sur le corps, sur les mains

[Méthodologie]
Réaliser quatre expérimentation pour déterminer et valider ces paramètres :
1 Tache de recherche visuelle d'objet
VI : angle de placement (50,75,100,125,150\% du FoV) et distance placement (40,60,80,100 cm)
VC : FoV (40°x30°)
VD : temps d'essai, effort perçu

2 Tache de sélection d'une cible
VI : placement (world-, body-, view-fixed), distance placement (40, 50, 60 cm), location cible (centre, haut, bas, gauche, droite)
VC : FoV (40°x30°), 70\% du FoV, input direct
VD : temps d'essai, erreurs sélection, effort perçu

3 Tache de recherche visuelle d'objet
VI : angle de placement (50,75,100,125,150\% du FoV), distance placement (40,60,80,100 cm)
VC : FoV (40°x30°), 70\% du FoV, input direct, world-fixed, 50 cm, curved windows
VD : temps d'essai, erreurs sélection, effort perçu

4

[Résultats]
1 75\% pour temps, et inconfort à 40 cm malgré pas de diff donc 60 cm min
2 world-fixed le plus précis (pour les autres le manque de précision de l'outil de sélection contrebalançait la proprioception), mais pas de diff de temps + précision meilleure quand distance proche = plus le bras s'allonge, moins c'est précis
3
4

Autres : Les fenêtres VR sont compatibles avec input direct même si FoV limité. Un layout courbe fonctionne bien et limite les erreurs.

[Limites]
Simplement testé l'input direct. Que de l'intangible. FoV très limité et fixe. Il faudrait tester sur plus de taches pour voir si les résultats sont généralisables.

[Concepts-clés]
Head-worn display; head-mounted display; task switching; spatial input; spatial user interface; virtual window management; multi-display environment

[Futurs travaux]
Répliquer sur un vrai HMD, pas un CAVE comme dans les expériences. Mieux supporter l'input direct et tester d'autres techniques d'input. Mieux comprendre les limitations de la proprioception et de la fatigue utilisateur, donc permettre une pus grand stabilisation quand en mode body-fixed ou changer de mode en fonction du contexte.

[Références centrales]
[12] Steven Feiner, Blair MacIntyre, Marcus Haupt, Eliot Solomon, Windows on the world: 2D windows for 3D augmented reality, Proceedings of the 6th annual ACM symposium on User interface software and technology, p.145-155, December 1993, Atlanta, Georgia, USA
[13] George W. Fitzmaurice, Situated information spaces and spatially aware palmtop computers, Communications of the ACM, v.36 n.7, p.39-49, July 1993
[22] Frank Chun Yat Li, David Dearman, Khai N. Truong, Virtual shelves: interactions with orientation aware devices, Proceedings of the 22nd annual ACM symposium on User interface software and technology, October 04-07, 2009, Victoria, BC, Canada
[34] Lauren Shupp, Robert Ball, Beth Yost, John Booker, Chris North, Evaluation of viewport size and curvature of large, high-resolution displays, Proceedings of Graphics Interface 2006, June 07-09, 2006, Quebec, Canada

\figureETS{EnsFinneganIrani2014-Figure8.jpg}{Conception du \foreignlanguage{english}{Personal Cockpit}.\\ Tiré de \citet[p. 7]{EnsFinneganIrani2014}}{EnsFinneganIrani2014-Figure8}

\paragraph{\foreignlanguage{english}{MultiFi: Multi-Fidelity Interaction with Displays On and Around the Body}}
Dans un article plus récent, \cite{GrubertKranzQuigley2015} constatent qu'il devient commun d'avoir plusieurs appareils mobiles sur soi, tels que les téléphones intelligents, les tablettes ou les montres connectées. Cependant, ces appareils sont conçus pour être utilisés seuls et pas pour qu'un utilisateur puisse interagir en même temps sur leurs multiples affichages.\\
\citeauthor{GrubertKranzQuigley2015} proposent alors d'utiliser les visiocasques pour joindre les affichages et les interactions entre ces différents appareils : les entrées et les sorties de tous les appareils sont liées \reffigureETS{GrubertHeinischQuigleyEtAl2015}. Ainsi, ce travail prend la suite de celui de \citet{EnsFinneganIrani2014}, en situant les fenêtres virtuelles autour des affichages des appareils de l'utilisateur. L'objectif de leur système est de permettre des actions faciles et rapides dans les tâches qui impliquent de multiples appareils mobiles. Ils explorent alors dans cet article ce nouvel espace de conception créé et mènent des expérimentations auprès d'utilisateurs pour valider leurs propositions de conception par rapport aux appareils utilisés seuls.\\
Un résultat intéressant de leur exploration sont les degrés possibles de couplage des appareils. Dans le premier mode dit \emph{body-aligned}, le contenu virtuel a pour référence le corps de l'utilisateur, les appareils mobiles servant comme fenêtre haute résolution en pointant sur ce contenu : la technique utilisée est \emph{overview+detail} (Voir \citep{BergeSerranoPerelmanEtAl2014}). Dans le second mode dit \emph{device-aligned}, le contenu a pour référence un appareil mobile, le visiocasque augmentant son écran avec un affichage virtuel : le visiocasque étant de moins bonne résolution que l'appareil mobile, c'est une technique appelée \emph{focus+context} (Voir \cite{BaudischGoodStewart2001}). Enfin dans mode dit \emph{side-by-side}, les interactions entre les appareils se font sans lien spatial : par exemple, l'utilisateur se sert du téléphone pour interagir indirectement avec des informations affichées par son visiocasque.\\
Leurs résultats expérimentaux montrent que l'augmentation des appareils en RA par un visiocasque peut permettre des temps plus rapides par rapport aux appareils seuls (visiocasque seul ou téléphone seul), dans les taches de navigation et de sélection, mais au détriment d'un plus grand effort perçu par les utilisateurs. Un second résultat expérimental important est que les préférences des utilisateurs sont variées entre utiliser les appareils seuls, coupler les entrées d'un appareil aux sorties de l'autre sans lien spatial, et coupler spatialement les appareils : cela montre qu'il est important de proposer tout ce spectre pour que chaque utilisateur organise l'IHM selon ses préférences et selon le contexte.\\
Une limite importante de cet article sont de faibles résultats expérimentaux. En effet, le système testé était lourd, peu mobile, avait une grande latence (150 ms), un champ de vision limité : ainsi, les résultats ont seulement permis de montrer que coupler les affichages des appareils \emph{pourrait} permettre de meilleures performances que les appareils utilisés seuls. En outre, le système n'a été pensé que pour certaines tâches précises, et non pour un couplage global et constant dans le temps. Il serait donc intéressant de généraliser le concept, et de le répliquer avec un visiocasque plus léger et mobile pour obtenir des résultats avec de meilleures validités internes (en évitant les problèmes techniques des expérimentations de l'article) et externes (plus de taches testés, conception pensée pour un couplage global et constant dans le temps).

\figureETS{GrubertHeinischQuigleyEtAl2015.jpg}{Illustrations du fonctionnement de \foreignlanguage{english}{Multifi}.\\ Tiré de \citet{GrubertHeinischQuigleyEtAl2015}}{GrubertHeinischQuigleyEtAl2015}

\paragraph{\foreignlanguage{english}{Gluey: Developing a Head-Worn Display Interface to Unify the Interaction Experience in Distributed Display Environments}}
Dans un article suivant, \cite{SerranoEnsYangEtAl2015} généralisent le travail de conception réalisé avec MultiFi \citep{GrubertHeinischQuigleyEtAl2015}. Ils proposent pour cela Gluey : une IHM qui utilise le visiocasque comme affichage pour unifier les entrées et sorties de tous les appareils, qu'ils soient mobiles ou de bureau. \citeauthor{SerranoEnsYangEtAl2015} s'appuient pour cela sur les propriétés de l'affichage permanent aux yeux de l'utilisateur du visiocasque et de sa connaissance de sa position dans les environnements réels et virtuels. Ainsi, c'est un médium idéal de transmission de l'information et de redirection des entrées entre les appareils \reffigureETS{SerranoEnsYangEtAl2015a-Figure1}.
Un utilisateur peut donc interagir sur un appareil, et voir ses actions s'exécuter sur un autre appareil qu'il regarde. Il est intéressant que le système permette que n'importe quel appareil puisse être utilisé de manière transparente et flexible comme un dispositif d'entrée (souris, téléphone, tablette). Le système permet également de transmettre des données, par exemple pour copier des données ou les imprimer.\\
\citeauthor{SerranoEnsYangEtAl2015} présentent avec ce travail un ensemble de pistes de conception pour un tel système. Leur idée était de pouvoir créer des interactions invisibles entre les appareils, pour faciliter les taches demandant d'utiliser plusieurs appareils. Pour cela, l'IHM doit permettre de : 
\begin{itemize}
  \item rediriger les entrées entre les appareils
  \item migrer du contenu entre les appareils
  \item tous les appareils doivent être compatibles
  \item d'enregistrer de nouveaux appareils
  \item de tenir un modèle spatial du système
  \item les retours du système doit toujours être visibles (ici par le visiocasque)
  \item le système doit être mobile
\end{itemize}. Ainsi, MultiFi \citep{GrubertHeinischQuigleyEtAl2015} ne satisfait qu'aux critères 1, 5, 6 et 7.\\
Une limite toutefois de l'article relève dans la faiblesse de son évaluation, informelle, qui a seulement pu montrer que le concept et la preuve de concept réalisée étaient enthousiasmantes et intéressantes aux yeux des participants. Une évaluation formelle devrait être reconduite avec un prototype léger et totalement mobile. Le visiocasque présentant en outre des limites de champs de vision et de résolution : si ces caractéristiques vont s'améliorer dans le futur, nous pensons que leur impact sur la navigation et les performances des interactions des utilisateurs n'est pas encore bien connue.

\figureETS{SerranoEnsYangEtAl2015a-Figure1.jpg}{Illustration du concept de \foreignlanguage{english}{Gluey}.\\ Tiré de \citet[p. 1]{SerranoEnsYangEtAl2015}}{SerranoEnsYangEtAl2015a-Figure1}

\paragraph{\foreignlanguage{english}{Desktop-Gluey: Augmenting Desktop Environments with Wearable Devices}}
\citet{SerranoEnsYangEtAl2015b} ont par la suite complété leur travail, en proposant Desktop-Gluey : ce concept étend les possibilités de Gluey en permettant à l'utilisateur d'étendre les écrans physiques de ses appareils par des fenêtres virtuelles. Les fenêtres peuvent être arrangés dans l'espace autour des écrans physique par l'utilisateur. Le système autorise donc le travail collaboratif en permettant de partager des fenêtres virtuelles entre plusieurs utilisateurs \reffigureETS{SerranoEnsYangEtAl2015-Figure2}. Enfin, les fenêtres peuvent suivre l'utilisateur dans ses déplacements et lui permettre d'interagir avec en utilisant son téléphone, une tablette ou des gestes de la main ou pour interagir, créant ainsi un concept de bureau mobile \reffigureETS{SerranoEnsYangEtAl2015-Figure4}. La métaphore du bureau sur l'ordinateur personnel est donc reprise et augmentée à "partout" et en "tout temps" : tout dispositif d'entrée peut être utilisé et tout dispositif d'entrée peut être augmenté dans son affichage.\\
Le concept recoupe celui de MultiFi \citep{GrubertHeinischQuigleyEtAl2015}, et le généralise des appareils mobiles seuls à tous les appareils visibles par l'utilisateur. Une autre différence majeure est que MultiFi a été conçu pour faciliter les taches demandant d'utiliser plusieurs appareils, alors que Desktop-Gluey propose de travailler avec ces fenêtres virtuelles : on peut dire que c'est une application du Personal Cockpit \citep{EnsFinneganIrani2014} dans une IHM tangible et située autour d'appareils mobiles (comme MultiFi).\\
\citeauthor{SerranoEnsYangEtAl2015b} ne présentent dans cet article que le concept mais n'en réalise aucun prototype, ni aucune évaluation. Nous pensons qu'il serait intéressant de montrer que ce concept fonctionne et d'implémenter ce système de bureau virtuel et mobile, afin d'explorer son espace de conception et d'en ajuster au mieux les paramètres.

\figureETS{SerranoEnsYangEtAl2015-Figure2.jpg}{Illustration de l'utilisation de fenêtres virtuelles pour étendre les écrans physiques dans le concept de \foreignlanguage{english}{Desktop-Gluey}.\\ Tiré de \citet[p. 3]{SerranoEnsYangEtAl2015b}}{SerranoEnsYangEtAl2015-Figure2}

\figureETS{SerranoEnsYangEtAl2015-Figure4.jpg}{Illustration du concept de \foreignlanguage{english}{Desktop-Gluey} en mode mobile.\\ Tiré de \citet[p. 3]{SerranoEnsYangEtAl2015b}}{SerranoEnsYangEtAl2015-Figure4}


\iffalse
\subsubsection{IHM autres que du IHM multiaffichage}
\paragraph{}
IHM hybride : mobile pour entrée, et hmd pour sortie \cite{LeeBudhirajaBillinghurst2013}
\fi


\subsubsection{Taille de l'affichage}
Une des principales problématiques des téléphones intelligent est leur petit écran. Cela peut grandement impacter les performances des utilisateurs quand il s'agit de naviguer dans de grands espaces de contenu, comme, par exemple, des données de navigation.

whether existing research findings on desktop-size displays still apply to this new environment?

Bi and Balakrishnan [7] compare a large projected wall display with single and dual desktop monitors. Their results suggest that large displays facilitate tasks with multiple windows and rich information because they offer a more immersive experience, with enhanced peripheral awareness

Multiscale interfaces [9] were designed to visualize large quantities of data on displays that are too small. With few exceptions [18], multiscale interfaces have been deployed and studied on the desktop, presumably to obviate the need for large displays. However, the effect of display size on multiscale navigation has yet to be adequately investigate

Guiard et al.’s [13] comparison of small-to-medium display sizes for a pan-and-zoom target acquisition task shows a minor performance improvement for the larger display

Citer Liu et Chapuis qui ont comparé desktop avec wall sur une tâche de naviagation/classification : 
« while the desktop can be faster than the wall for simple tasks, the wall gains a sizable advantage as the task becomes more difficult. A follow-up study shows that other desktop techniques (overview+detail, lens) do not perform better than pan-and-zoom and are therefore slower than the wall for difficult tasks (manupilating elements in a complex decision making task tha requires expertise and quick access to full content) -> eg. third task of personal cockpit or google maps (the typical task with wedge, or overview+detail) »

\paragraph{}
Une approche est d'avoir recourt à des techniques de visualisation pour montrer à l'utilisateur des indications de la localisation d'objets hors-écran. Une de ces techniques est Halo, conçue par \citet{BaudischRosenholtz2003}, qui propose pour chaque objet hors-écran de tracer un cercle prenant l'objet comme centre et avec un rayon assez grand pour qu'une portion de l'arc soit visible sur l'écran. Ainsi l'utilisateur a des indices pour déduire la direction et la distance de l'objet. Leurs expérimentations ont montré que cette technique permettait des temps 16\% à 33\% plus rapides, et sans augmentation du taux d'erreur, par rapport à une technique plus classique (\emph{scaled arrows}) indiquant par des flèches sur l'écran la distance et la direction des objets hors-écrans. Cette technique a ensuite été perfectionnée par \citet{GustafsonBaudischGutwinEtAl2008} dans une seconde version nommée Wedge, cette seconde version permettant aux utilisateurs d'être significativement plus rapides qu'avec la première version. Enfin, \citet{BurigatChittaro2011} ont réalisé un comparatif de cette technique avec les techniques \foreignlanguage{english}{scaled arrows} et \emph{overview+detail} \reffigureETS{BurigatChittaro2011-Figure1}, cette dernière proposant une carte en miniature dans une vue sur un bord de l'écran montrant où regarde l'utilisateur sur cette carte. Les résultats de ce comparatif confirment que Wedge permet les temps les plus rapides quand les notions de distances sont importantes, mais indiquent que overview+detail est plus adapté quand les notions de structure de l'espace sont importantes, les auteurs concluant que toutes les techniques de visualisation ayant un impact positif sur les performances. Ainsi, il serait intéressant de savoir si un écran d'un appareil mobile augmenté de contenu virtuel permettrait de réaliser de meilleures performances par rapport à l'écran seul utilisé avec ces techniques.

\figureETS{BurigatChittaro2011-Figure1.jpg}{Les trois techniques de visualisation pour les éléments hors écran : (a) \foreignlanguage{english}{scaled arrows} (b) Wedge (c) overview+detail\\Tiré de \citet[p. 158]{BurigatChittaro2011}}{BurigatChittaro2011-Figure1}

\paragraph{}
Cette problématique a également touché les ordinateurs personnels dans les années 2000 quand leurs affichages n'étaient pas très grands (les écrans 15 et 17 pouces étaient courants) ni de très grande résolution.\\
les problématiques du mobile actuellement : petit écran face à grand contenu = incapacité à gérer de grandes infos\\
+ overview+detail \cite{BergeSerranoPerelmanEtAl2014} et \cite{RashidNacentaQuigley2012}\\
concevoir pour la périphérie (focus+context \cite{CockburnKarlsonBederson2009}

+ illumiroom \cite{JonesBenkoOfekEtAl2013})\\
avec wedge/halo \cite{BaudischRosenholtz2003} \cite{GustafsonBaudischGutwinEtAl2008} \cite{BurigatChittaro2011}\\

\paragraph{}
potentiel d'augmenter la taille l'affichage avec les travaux comparant le desktop aux murs \cite{LiuChapuisBeaudouin-LafonEtAl2014} \cite{ShuppBallYostEtAl2006} \cite{TanGergleScupelliEtAl2003}

\paragraph{}
pouvoir naviguer mieux dans les larges «information spaces» fov comparés \cite{RaedleJetterMuellerEtAl2014} (ici le sweet spot va converger vers le 130° de wide display et le design pour la périphérie ?)

\paragraph{}
explorer tout ce qui lié à \"SideSight: Multi-“touch” Interaction Around Small Devices \" : interactions avec petits écrans mais sans feedback -> nous ajoutons à ces recherches le feedback qui a la possibilité d'être permanent avec un HMD


\subsubsection{Multi-taches et changement de contexte}
concernant le multi-tache, c'est une problématique des systèmes RA (\cite{SchmalstiegFuhrmannHesinaEtAl2002} et background de \cite{EnsFinneganIrani2014}) : donc c'est naturel de lier le portable à la RA (TODO trouver un article le disant), surtout si on suit le \"information spaces\" de \cite{EnsHincapie-RamosIrani2014}\\
\cite{TanCzerwinski2003}\\
\cite{RashidNacentaQuigley2012a}


\subsubsection{Interactions multimodales}
problématique du multimodal : quelle interaction pour quelle tache ?

+ interactions directes (main)

+ interactions indirectes avec le regard

+ apprentissages interactions touch avec un smartphone


\subsubsection{Synchronisation des environnements réels et virtuels}
problématique de désynchronisation réel/virtuel peut être comblé avec cette RA mobile : \cite{Chalon2004} et Gluey qui fait des photos du réels pour les coller sur l'écran



\section{Facteurs de conception}
« However, to date, what is not well understood is which factors inhibit or support the interaction across multiple displays on and around the body i.e. “body proximate” displays. Within this paper we review four key design and technological challenges inherent in body proximate displayecosystems, i.e. combinations of wearable displays (e.g.,smartwatches and smartglasses) and handheld devices(e.g., tablets and smartphones). » \cite{GrubertKranzQuigley2015}\\
TODO : réviser l'article comme point de départ de cette section + les design factors de Personal Cockpit\\

Une solution pour un tel système de RA demande au préalable une connaissance des facteurs de conceptions
Conception (interfaces, interactions) importants et surtout les non-explorés encore = les sous-problèmes


\subsection{Matériel}
        FoV (Czerwinski, 2002) (Patterson, 2006) \cite{KishishitaKiyokawaOrloskyEtAl2014}\\

        Resolution\\

        Projets similaires\\
        - https://pdfs.semanticscholar.org/a53a/3d25b8258b5331c23056d4696691dabd8eb9.pdf\\
        - https://link.springer.com/content/pdf/10.1007/s11554-016-0640-9.pdf\\
        - AR-Rift: https://vr.cs.ucl.ac.uk/portfolio-item/ar-rift/\\
        - OculAR: https://arxiv.org/abs/1604.08848\\
        - AR-Rift 2: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892261\\
        - Impact of visual and experiential realism on distance perception in VR using a custom video see-through system: https://dl.acm.org/citation.cfm?id=3119892


\subsection{Visualisation, interface design}
        Reference frame for virtual content\\
            World-fixed \cite{EnsFinneganIrani2014}\\
            Body-fixed \cite{EnsFinneganIrani2014}\\
                + Head centered vs dominant hand centered\\
            Head-fixed \cite{EnsFinneganIrani2014}\\
            Phone-fixed\\
          Movability \cite{EnsHincapie-RamosIrani2014}\\
            Spatial consistent interface \cite{LiDearmanTruong2009}\\

        Context switching\\
            Number of displays \cite{RashidNacentaQuigley2012} \cite{CauchardLoechtefeldFraserEtAl2012}
            Design of context vs design with large one app display \cite{BallNorth2008}

        Content display\\
            Size of virtual elements = angular width \cite{ShuppBallYostEtAl2006} \cite{BallNorth2008}\\
            Distance of virtual content (Hezel, 1994) (Ankrum, 1999) (Tan, 2003) \cite{ChanKaoChenEtAl2010} \cite{EnsFinneganIrani2014} 
            Angular separation (Mayer, 1993) \cite{EnsFinneganIrani2014} \cite{KishishitaKiyokawaOrloskyEtAl2014} (Alger, 2015)\\ 
            Curved layout vs flat layout \cite{ShuppBallYostEtAl2006} 
            Direction of content (top, bottom, left, right) \cite{EnsFinneganIrani2014}\\
            Display continuity \cite{TanCzerwinski2003} \cite{RashidNacentaQuigley2012}\\
            Allocating space for new elements \cite{BellFeiner2000}\\

            2D vs 3D virtual content \cite{JansenDragicevicFekete2013} \cite{SerranoHildebrandtSubramanianEtAl2014}\\
            Same appareance for a same content accross outputs vs different appareance according to the output \cite{GrubertHeinischQuigleyEtAl2015}\\


\subsection{Interaction design}
      « tasks that occur in 3D applications, which are independent of the input device » \cite{JankowskiHachet2013}\\
      Reprendre \cite{Bernatchez2008} et \cite{JankowskiHachet2013}
      Technique (interactions spatiales)\\
            Mid-hair hand \cite{EnsFinneganIrani2014} \cite{ChanKaoChenEtAl2010} \cite{JonesSodhiForsythEtAl2012} + Blog de Leap Motion
            Gaze + taffi (HoloLens)\\
            Just touch on phone\\
            Gaze + touch on phone\\
            Baselines
              Phone without AR
              Phone without AR and without the HMD, but with the HMD-latency
              Phone without AR and without the HMD nor the HMD-latency

        Indirect (\cite{TeatherStuerzlinger2011}) vs direct (revoir ethereal planes)\\
        Tangibilité vs intangible, dans les techniques\\



\section{Évaluation d'une solution au problème de recherche}
\subsection{Évaluations des systèmes de Réalité Augmentée}
La conception de solution aux problèmes d'IHM pour la RA demande d'évaluer, formellement ou non, ces solutions. Pour cela, les méthodes d'évaluation des IHM traditionnelles peuvent être utilisées, en particulier des expérimentations utilisateurs (en anglais : \foreignlanguage{english}{user studies} ou \foreignlanguage{english}{user-based experimentations}). \citep{SwanGabbard2005}
Comment évaluer ? Taches de tests, et taches plus écologiques (proches des usages réels du quotidien) \cite{DeSaChurchill2013}
\cite{SwanGabbard2005} \cite{DuenserGrassetBillinghurst2008}

\subsubsection{Types d'évaluations}
Perception : taches de bas-niveau pour comprendre comment la perception et la cognition humaine fonctionne dans les contextes de RA

Performance utilisateur : examine les performances dans les taches utilisateurs en RA, dans des applications spécifiques, pour déterminer comment la technologie peut impacter ces taches (\textbf{limite} de notre étude ici : on ne fait que ça dans nos taches d'évaluation

Collaboration : examine des interactions et communications utilisateurs entre plusieurs personnes

Utilisabilité : idem au type `Performance` mais sans mesure, plutôt identifier les problèmes avec l'utilisabilité du système

\subsubsection{Méthodes d'évaluations}
Mesures objectives\\
  Surtout temps de complétion et taux d'erreur, ou encore score, position, nombre d'actions, mouvements \cite{DuenserGrassetBillinghurst2008}

Mesures subjectives\\
  Questionnaires, notes utilisateurs, retours utilisateurs \cite{DuenserGrassetBillinghurst2008}

Analyses qualitatives\\
  Observations, interviews formelles, classification des comportements utilisateurs \cite{DuenserGrassetBillinghurst2008}

Techniques d'évaluation d'utilisabilité\\
  Évaluations experts, évaluations heuristiques, analyse de tache, description à haute voix, méthode magicien d'Oz \cite{DuenserGrassetBillinghurst2008}

Évaluations informelles\\
  Observations utilisateurs, retours utilisateurs \cite{DuenserGrassetBillinghurst2008}


\subsection{Propositions d'expérimentations utilisateurs}
\subsubsection{Tâche de navigation}
Conjunction Search : l'espace est partagé en deux, avec un objet cible à gauche et une grille d'objets à droite. Le participant doit compter dans la grille le nombre d'objets similaire à la cible. Les participants doivent être le plus rapide et le plus précis possible. Cela a permis de déterminer la meilleur distance et la meilleure taille pour le contenu virtuel dans l'espace. L'effort perçu (7 points) et le temps d'essais ont été pris en mesure. Il y a eu 10 essais pour chaque condition et 10 participants. \cite{EnsFinneganIrani2014}\\
\cite{RashidNacentaQuigley2012}\\
Recherche d'un objet avec certaines propriétés. Ici, les participants devaient déterminer l'item avec le plus bas prix, parmi cinq items. 10 essais par 5 interaction possible (Handled: écran mobile seul ; Smarwatch: écran smartwatch seul ; HMD: HMD seul + entrée indirecte sur la SW ; BodyRef: contenu body-ref + entrée sur l'écran SW ; SWRef: contenu SW-ref + entrée sur l'écran SW). 23 participants. Mesure du temps de complétion de la tâche (TCT), du taux d'erreurs, charge mentale (subjectif). Handled et BodyRef étaient les plus rapides, pas de différence dans les erreurs. Handled demandait moins de charge. \cite{GrubertHeinischQuigleyEtAl2015}

\subsubsection{Tâche de sélection}
Les participants doivent toucher une cible (ici avec leur doigt). La cible peut se trouver dans cinq emplacements : haut, bas, gauche, droite ou centre. Cela a permis de tester la distance du contenu, et le reference frame. Il y a eu 5 essais pour chaque condition et 12 participants. Ont été mesurés le temps de complétion, les erreurs de pointage, et la fatigue perçue (échelle de Borg à 12 points). \cite{EnsFinneganIrani2014}\\
Deux fenêtres sont affichées, de direction (haut, bas, gauche, droite) et d'angle (le layout est courbe : 15°, 25°, 35°, 45°, 55°) variables. La première fenêtre contient un bouton de démarrage (placée à hauteur d'yeux du participant) et la deuxième une cible à déclencher après le démarrage. Chaque condition est répétée 10 fois, et il y a eu 8 participants. Ont été mesurés le temps de complétion, les erreurs de pointage, et la fatigue perçue (échelle de Borg à 12 points) pour le bras et pour la tête. \cite{EnsFinneganIrani2014}\\
Le long d'une bande à faire scroller, les participants doivent sélectionner une cible. 10 essais pour chacune des 5 interactions (Handled, Smartwatch, HMD, BodyRef, SWRef). BodyRef demandait à déplacer la sw vers la cible, pas scroller. Un bouton start était à toucher, puis la cible était à atteindre. 2 distances ont été testées (15 cm et 30 cm). 2 directions à tester. 23 participants. Mesure TCT, erreur, charge mentale et préférence utilisateur. BodyRef a été le plus rapide, mais génère beaucoup d'erreurs. SWRef génère peu d'erreurs. Handled le moins de charge. Frustration pour SW et SWRef.

\subsubsection{Tâche de classification}
Tri \cite{RobertsonCzerwinskiLarsonEtAl1998}, ou classification \cite{LiuChapuisBeaudouin-LafonEtAl2014}\\

\subsubsection{Tâche spatiale}
\cite{BurigatChittaro2011}
\cite{BergeSerranoPerelmanEtAl2014}

\subsubsection{Tâche de commutation d'affichages}
4 applications sur mobile : Question, My contacts, Calendar, Map. Étudie différentes façons de switcher entre des application. Pris des applications connues et habituelles. Il n'y a besoin d'aucune interaction dans les taches, mais simplement de switcher entre elles et de lire les indices qu'elles contiennent. Tache faite non pas en mouvement, mais faite quand utilisateur s'arrête pour chercher infos. Le but est que les utilisateurs répondent à la question posée. Designs factors : 6 questions et 4 types (=24 questions), et 3 conditions (ici 3 façons de switcher). Les mesures prisent ont été le temps de complétion (MT) et le taux d'erreurs (ER) + un questionnaire NASA TLX. 12 participants, qui ont pu essayer les tâches jusqu'à ce qu'ils se sentent prêts. \cite{CauchardLoechtefeldFraserEtAl2012}\\
La même tache que Cauchard (2012) a été reprise. Les fenêtres sont à 50 cm de l'utilisateur, de taille 22 cm, sur un layout courbe, séparée de 27,5°, toujours orientée vers l'utilisateur et fixées au monde (pas à l'utilisateur). Les fenêtres des applications à utiliser ont été placées aléatoirement dans une grille de 9 ou 16 emplacements. Pour ajuster la difficulté il y a avait 4 ou 5 applications. Enfin, deux techniques étaient présente, une directe (avec une assistance visuelle de la distance du doigt à la fenêtre) et une indirecte sur une tablette permettant de switcher entre les applications (toujours affichées sur le HMD). \cite{EnsFinneganIrani2014}\\
Les deux sont des tâches ecologiquement plus valides, car utilisant des applications familières et quotidiennes pour les utilisateurs. + discuter  des taches écologiques en réel avec toutes les problématiques que ça ouvre \cite{KoelleKranzMoeller2015} \cite{DenningDehlawiKohno2014}



<<<<<<< HEAD
Citer Alexandre Millette + ajout du meta 2 depuis pour la liste des casques existants
- Justification tracking tête dans condition 'indirect input' : 
    - Article SmoothMoves a "[...] confirms the idea that head motions can accurately track moving targets. In the head condition, the fidelity of the behavior [...] exceeded that of the eyes [...]. This suggests that head based input can act as a surrogate for eye-based input in many smooth pursuits input scenarios; it may even be preferred in terms of performance. However, data from the natural condition also clearly indicates that participant’s predilection was to track with the eyes; only when specifically instructed did they use clear, accurate and distinctive head movements."
- A survey of augmented reality (Billlinghurst, 2015) : https://fr.slideshare.net/marknb00/a-survey-of-augmented-reality
    - Caractéristiques RA (Azuma, 1997):
        - Combine des images réelles et virtuelles (les deux sont vus en même temps)
        - Interactif en temps réel
        - Le contenu virtuel est inscrit en 3D dans l'espace réel
    - [Rekimoto, 1995](https://www.sonycsl.co.jp/person/rekimoto/uist95/uist95.html)
    - Technologies (pour répondre aux carac d'Azuma) :
        - Affichage (Bimber \& Raskar, 2003)
            - Types :
                - Attaché à la tête : HMD, CAVE (projecteur)
                    - Video see-trough
                    - Optical see-trough
                - Attaché au corps : appareil tenu en main (ex: cellulaire)
                - Dans l'espace : écran ou projecteur sur des surfaces
        - Interaction
        - Tracking
            - Actif : GPS, Wifi/cell localisation
            - Passif
                - Capteurs internes : compas, accéléromètre, gyroscope
                - Vision par ordinateurs : marqueurs optiques, vision naturelle (SLAM, texture tracking, edge based tracking)


\section{À traiter}
Berard2009 - Did \"Minority Report\" Get It Wrong? Superiority of the Mouse over 3D Input Devices in a 3D Placement Task\\
« Interaction is not defined by an input device alone, but by the combination of a device and an interaction technique. In the example of 3D object rotation, the mouse is typically used with the virtual sphere technique while a free-space device is used with a direct mapping (either absolute or relative) Thus, each device must be matched with its most suitable interaction technique in making performance comparisons, rather than choosing a single interaction technique for all devices. »

Esteves2017 - SmoothMoves\\
Justifie de faire de l'IHM pour la RA : « The latest products, such as Microsoft HoloLens include powerful computers, high resolution displays and sophisticated tracking. While these technical achievements are impressive, there is less clarity about the best ways for users to interact with AR contents and interfaces. »\\
Liste des types de techniques d'interactions en IHM pour les HMD en RA : « There is an active community exploring viable modalities for head-mounted displays (HMDs) including on-headset touch mid-air hand input and the use of dedicated wearable peripherals such as gloves or belts Within this space, we argue that input from movements of the eyes and head are particularly practical and appealing: in such scenarios, hands remain free and all sensing can be integrated into the headset.\\
Argument du prix sur head-tracking vs eye-tracking : « Using head motions accords considerable practical benefits, primarily that the Inertial Motion Units (IMUs) needed to accurately track head movements are small, cheap, low power and already integrated into the majority of AR glasses and other wearables.\\
Principe et explication de pourquoi l'étudier (dans le HoloLens, le Cardboard) de la technique d'interaction du pointage avec le regard : « Ray-based pointing, in which users interact by projecting a ray from their head to intersect with a target of interest, is the most common and has been integrated into current head-mounted displays, such as the Google Cardboard and the Microsoft HoloLens. »\\
Explication du pointage avec le regard : « Gaze is the inseparable product of head movements plus eye movements. The relationship between these activities is sophisticated. At the most fundamental level, the Vestibulo-Ocular Reflex (VOR) continuously stabilizes gaze by adjusting (basically inverting) eye position in response to changes in head position sensed by the vestibular system »\\
Propriétés du du pointage avec le regard : « A number of properties make smooth pursuits movements useful as an input technique. First, they are innate. Users know how to visually track targets and can generate this kind of motion without training. Second, they are distinctive. Users are only able to generate smooth pursuits eye movements in the presence of visually moving targets. Third, they operate on movement not position. As such, they are relatively immune to changes in target size and robust to tracking errors – capturing changes in gaze is much simpler than accurately determining what a user is looking at. Fourth, they are operated hands-free. And fifth, they do not require users to memorize gestures. Several systems have been recently introduced to leverage these properties.\\
Les auteurs ont montré que les techniques d'interactions (en tout cas le suivi de cible) basées sur le mouvement de la tête (par exemple un curseur projeté depuis le centre de la tête sur des cibles dans la RA) est aussi efficace en terme de performances, et peu même être préféré, que les techniques d'interactions basées sur les mouvements des yeux et a les mêmes propriétés.

Berge2014 - Exploring smartphone-based interaction with overview+detail interfaces on 3D public displays\\
Les utilisateurs ont préféré les interactions Mid-Air Phone et Mid-Air Hand (avec ou sans entraînement) versus les interactions touchscreen dans une interface Overview+Detail d'exploration de contenu 3D : un écran faisait overview et un téléphone mobile de detail.\\
Nous on ne ce pose pas la question de l'entraînement : car on considère que c'est une IHM qui serait utilisée au quotidien ou professionnellement : donc c'est correct d'avoir un temps d'apprentissage.\\
Il y a trois de types de solutions/approches pour visualizer/afficher de larges espaces d'information : Overview+Detail, Focus+Context et Zooming. Nous n'étudions que Zooming, car c'est le plus courant sur les appareils mobiles. Or on compare les techniques d'interactions de notre solution à un mobile seul. Il aurait été intéressant cependant, dans une deuxième expérimentation, de comparer ces techniques de visualization sur notre solution versus sur un appareil mobile seul.

Toutes nos conditions de la VI technique sont des direct mapping (mappe directement la position de l'input au curseur).

Large information space visualization on mobile phone - Burigat2011 - On the effectiveness of Overview+Detail visualization on mobile devices\\
« Results of the experiment suggest that both direct manipulation of the overview and highlighting objects of interest in the overview have a positive effect on user performance in terms of the time to complete search tasks on mobile devices, but do not provide specific advantages in terms of recall of the spatial configuration of targets. »

3D navigation with a mobile phone - Peephole\\
Notre condition téléphone seul est en fait un static navigation peephole, et notre condition peephole est dynamic navigatino peephole.
« Handheld displays leave little space for the visualization and navigation of spatial layouts representing rich information spaces. The most common navigation method for handheld displays is static peephole navigation : The peephole is static and we move the spatial layout behind it (scrolling). A more natural method is dynamic peephole navigation: here, the spatial layout is static and we move the peephole across it. » Mehra2006

Huerst2010 - Dynamic versus static peephole navigation of VR panoramas on handheld devices\\
« We present a formal evaluation and usability studies comparing two interaction concepts. In the first one, the device is seen as a static peephole and the data is moved behind it via touch screen-based scrolling. In the second one, a mobile phone’s sensors are used to create a dynamic peephole that can be moved over static content. In the results of our formal analysis sensor-based dynamic peephole navigation performed twice as good in an orientation task, 75\% better in an object size discrimination task, and was preferred by 80\% of the users. Despite these advantages, additional usability studies indicate that if they are sitting, a majority of users resort to touch screen-based static peephole navigation when interacting. »

Mehra2006 - Navigating on handheld displays\\
« Subjects viewed a spatial layout containing two lines on a static display screen. Only a part of the screen—the peephole—was visible. Subjects had to discriminate line length by either moving a dynamic peephole across a static layout of the lines or by moving a dynamic layout behind a static peephole. »\\
« Discrimination thresholds for static peephole navigation were 50–75\% higher than for dynamic peephole navigation. Furthermore, static peephole navigation took 24\% more time than dynamic peephole navigation »