\chapter{Conception d'un visiocasque de RA à large champ de vision}
\label{ch:methodology}

\section{Motivation}
Pour concevoir un VESAD, nous avons besoin d'un visiocasque avec un champ de vision suffisamment large pour visualiser complètement l'écran étendu, le cas contraire limite l'intérêt de notre concept. Notre second sous-problème a donc été de développer un visiocasque de RA à large champ de vision.

Un téléphone est tenu en moyenne à une distance $D=\SI{34}{\cm}$ \citep{Bababekova2011} de la tête. Ainsi, si l'on souhaite étendre un téléphone à l'équivalent d'un écran {\NoAutoSpacing 16:9} de \SI{24}{\inch}, soit une taille de $(L,H)=\SI{53x30}{\cm}$, on peut calculer le champ de vision $(FoV_x,FoV_y)$ nécessaire pour chaque \oe il pour le visualiser complètement \reffigureETSp{Fov.png}, avec l'\autoref{eq:fov} : il doit être d'au moins \SI{76x48}{\degree}.

\begin{equation}
  \label{eq:fov}
  \left \{
  \begin{array}{r c c c c c l}
    FoV_x & = & 2 \arctan (\frac{L}{2 \times D}) & = & 2 \arctan (\frac{53}{2 \times 34}) & = & \ang{76}\\
    FoV_y & = & 2 \arctan (\frac{H}{2 \times D}) & = & 2 \arctan (\frac{30}{2 \times 34}) & = & \ang{48}
  \end{array}
  \right .
\end{equation}

\figureETS{Fov.png}{
  Représentation simplifiée du champ de vision (en anglais : \texten{Field of View}), qui peut être mesuré en degrés horizontalement ($FoV_x$), verticalement ($FoV_y$) ou en diagonale. Comme le décrit l'\autoref{eq:fov}, pour qu'un plan soit visible dans un champ de vision donné, il doit être placé à une certaine distance de la caméra ou de l'\oe il.
}

Nous avons d'abord voulu utiliser le Microsoft HoloLens, un visiocasque optique de RA avec une très courte latence, une excellente résolution et une très bonne documentation et un support natif sur le moteur de jeu Unity. Cependant, son champ de vision de \SI{30x17.5}{\degree} pour chaque \oe il \citep{Kreylos2015} est trop restreint, ne permettant de visualiser seulement l'équivalent d'un écran {\NoAutoSpacing 16:9} de \SI{7}{\inch}. Il n'existe en fait actuellement aucun visiocasque de RA sur le marché avec un grand champ de vision \citep[p. 25]{Millette2016}.


\section{Solution retenue}
\label{sec:prototype}

\subsection{Fonctionnement du visiocasque}
\label{subsec:prototype_operation}
Nous avons donc réalisé notre propre prototype, similaire à l'AR-Rift \citep{Steptoe2013} : c'est un visiocasque de RA vidéo de conception simple qui a fait ses preuves, utilisé par \cite{Steptoe2014} et \cite{Piumsomboon2014}. Il consiste à coller une caméra physique \emph{stéréoscopique} (avec deux objectifs capturant chacun une image pour un \oe il) à un visiocasque de RV.

\figureLayoutETS{ArRiftMarker}{%
  \parbox{0.72\textwidth}{% Allow line breaks
    \centering%
    \subfigureETS{ArRiftMarker_1.jpg}{Image non corrigée : l'image présente des distorsions importantes (les lignes droites sont courbées).}%
    \figurehspace%
    \subfigureETS{ArRiftMarker_2.jpg}{Image corrigée : les lignes droites le sont à nouveau.}%
    \\%
    \subfigureETS{ArRiftMarker_3.jpg}{Image corrigée à travers Unity : l'image est affichée en arrière-plan de l'environnement virtuel filmé par la caméra virtuelle (champ de vision en lignes blanches) pour l'\oe il gauche. Les caméras physique et virtuelle sont alignées pour faire coïncider environnements virtuels et réels.}%
    \figurehspace%
    \subfigureETS{ArRiftMarker_4.jpg}{Image corrigée et augmentée : un objet virtuel se trouve par dessus chacun des trois marqueurs.}%
  }%
}{
  Vue de l'image capturée par l'objectif gauche de l'Ovrvision Pro, puis corrigée et augmentée pour être affichée dans le visiocasque.
}

Notre visiocasque fonctionne sur le principe suivant :
\begin{enumerate}
  \item La caméra stéréoscopique physique filme l'environnement réel de l'utilisateur \reffigureETSp{ArRiftMarker_1.jpg}.
  \item Un algorithme \emph{corrige} les déformations des deux images de cette caméra \reffigureETSp{ArRiftMarker_2.jpg}.
  \item Une caméra stéréoscopique virtuelle va filmer des éléments virtuels ainsi que les deux images de la caméra physique en arrière-plan \reffigureETSp{ArRiftMarker_3.jpg}.
  \item Les deux images sont affichées dans le visiocasque, une pour chaque \oe il, affichant environnements virtuels et physiques superposés \reffigureETSp{ArRiftMarker_4.jpg}.
\end{enumerate}
\bigskip
Pour donner l'illusion que les éléments virtuels sont alignés avec l'environnement réel, nous devons donc \emph{aligner} la caméra virtuelle avec la caméra physique, dans un travail qui se fait en trois temps :
\begin{enumerate}
  \item \emph{Étalonner} (\texten{calibrate} en anglais) la caméra physique, c'est-à-dire mesurer ses \emph{paramètres intrinsèques} (ses propriétés, comme son champ de vision), et les \emph{distorsions} de ses objectifs (les déformations sur les images capturées). L'étalonnage est à faire seulement une seule fois par caméra.
  \item Au démarrage du visiocasque, aligner la caméra virtuelle en la configurant avec les paramètres intrinsèques de la caméra physique.
  \item Pour chaque image capturée par la caméra physique, appliquer le procédé de la \reffigureETS{ArRiftMarker}.
\end{enumerate}
\bigskip

La caméra virtuelle filmant ainsi du \emph{même point de vue} que la caméra physique, on peut donc placer les éléments virtuels comme s'ils faisaient partie de l'environnement réel. On utilise alors un algorithme de \emph{suivi de marqueurs} (\texten{fiducial marker tracking} en anglais) : ce sont des codes-barres en 2D imprimés \reffigureETSp{ArRiftMarker_2.jpg} et dont on peut déterminer très rapidement la position et l'orientation par rapport une caméra physique dont on connaît les paramètres intrinsèques \cite{Garrido-Jurado2014}. Les marqueurs étant détectés en temps réel sur chaque image, on place les éléments virtuels à ces mêmes positions et orientations par rapport à la caméra virtuelle \reffigureETSp{ArRiftMarker_3.jpg}, donnant donc l'illusion qu'ils font partie de l'environnement réel \reffigureETSp{ArRiftMarker_4.jpg}.

La \reffigureETS{ArRiftMarker} montre ce processus pour une caméra \emph{monoscopique} (avec un seul objectif). Pour une caméra stéréoscopique, on mesure en plus la différence de position et d'orientation entre les deux objectifs lors de l'étalonnage. On crée ensuite deux caméras monoscopiques virtuelles placées de la même manière que les deux objectifs de la caméra physique. Chaque caméra virtuelle va alors filmer le même environnement virtuel, d'un point de vue légèrement différent, mais avec seulement une image d'un des deux objectifs en arrière-plan.

\subsection{Choix techniques}
\label{subsec:technical_choices}
Nous avons tout d'abord choisi l'Ovrvision Pro comme caméra physique stéréoscopique (\url{http://ovrvision.com/}). Annoncée par son comme solution clé en main pour réaliser un visiocasque de RA, elle prévue pour fonctionner avec le visiocasque de RV Oculus DK2 que nous avions déjà à disposition dans notre laboratoire. Composée de deux objectifs \texten{fisheye} (en \oe il de poisson) d'une définition de \SI{960x950}{\px} et un champ de vision de \SI{100x98}{\degree}, ses caractéristiques correspondent bien à ceux de l'Oculus DK2 \autorefp{tab:visual_densities}. Elle est de plus intégrée avec les moteurs de jeu standards dans l'industrie : Unity et Unreal Engine. Son installation se fait simplement en la collant sur la face avant de l'Oculus DK2, comme le montre la \reffigureETS{ArRift_1.jpg}, et en le connectant par USB 3.0 au PC.

\figureETS[0.6]{ArRift_1.jpg}{
  Notre prototype de visiocasque de RA, basé sur le concept de l'AR-Rift de \cite{Steptoe2013} : il est composé du visiocasque de RV Oculus DK2, de la caméra stéréoscopique Ovrvision Pro et du capteur de reconnaissance des mains Leap Motion (sous la caméra).
}

Nous souhaitions ensuite rapidement prototyper du contenu sur notre visiocasque. Nous avons alors choisi le moteur de jeu Unity : gratuit (mais au code source propriétaire), il est le standard dans l'industrie du jeu-vidéo pour prototyper et supporte notre caméra et le Leap Motion. Il est de fait très avantageux d'investir un peu de temps dans l'apprentissage d'un moteur de jeu, car il prend complètement en charge le rendu 3D, la simulation de la physique, les entrées sur clavier, souris ou écran tactile, l'affichage dans les visiocasques de RV, un support réseau et propose de nombreuses fonctions mathématiques. Unity est simple plus simple à prendre en main que son concurrent l'Unreal Engine, par son GUI intuitif et l'utilisation de scripts C\#, un langage haut niveau efficace. Enfin, nous avions une expertise dans le laboratoire avec un projet de quatre mois sur le Microsoft HoloLens et la maîtrise de \cite{Millette2016}.

Après plusieurs essais de configuration et une lecture du code source de l'Ovrvision Pro (\url{https://github.com/Wizapply/OvrvisionPro/}), nous avons constaté que la bibliothèque fournie était non fonctionnelle : (1) les images affichées pour chaque \oe il dans le casque étaient décalées, rendant le visiocasque particulièrement inconfortable à utiliser et (2) un décalage important était présent entre le contenu virtuel et l'environnement réel. Nous avons donc besoin de ré-étalonner cette caméra et d'une bibliothèque de RA.

Nous avons donc réalisé la bibliothèque libre de RA par suivi de marqueur Aruco Unity (\url{https://github.com/NormandErwan/ArucoUnity/}) pour amener de la RA sur notre visiocasque. Utilisant la bibliothèque libre de vision par ordinateur OpenCV (\url{https://opencv.org/}), elle est la seule bibliothèque libre de RA par suivi de marqueurs sous Unity supportant plusieurs types de caméras : monoscopiques ou stéréoscopiques, avec des objectifs \texten{fisheye} ou \textquote{classiques}, dit \emph{rectilinéaires} (similaire à la vision humaine). Elle est également la seule à permettre l'étalonnage de tout ces types de caméras. On utilise en particulier les modules d'OpenCV suivants :
\begin{itemize}
  \item aruco pour suivre les marqueurs (\url{https://docs.opencv.org/master/d9/d6a/group__aruco.html}) ;
  \item calib3d pour étalonner et corriger les objectifs rectilinéaires (\url{https://docs.opencv.org/master/d9/d0c/group__calib3d.html}) ;
  \item ccalib pour étalonner et corriger les objectifs \texten{fisheye} (\url{https://docs.opencv.org/master/d3/ddc/group__ccalib.html}).
\end{itemize}


\subsection{Discussion et limites}
\label{subsec:solution_discusion}
Nous pourrions simplifier le visiocasque en utilisant une caméra monoscopique, et afficher l'image capturée aux deux yeux. Cependant, comme le souligne \cite{Bourke1999}, la vision stéréoscopique est l'un des principaux indices utilisés par le cerveau pour percevoir la profondeur : les yeux étant séparés horizontalement par une distance appelée écart pupillaire, chaque \oe il perçoit une image légèrement différente \reffigureETSp{Ovrvision} permettant au cerveau de percevoir en 3D. C'est pourquoi nous choisissons d'utiliser une caméra stéréoscopique.

\figureLayoutETS{Ovrvision}{%
  \subfigureETS[0.2]{Ovrvision_1.jpg}{Vue de l'objectif gauche.}%
  \figurehspace%
  \subfigureETS[0.2]{Ovrvision_2.jpg}{Vue de l'objectif droit.}%
}{
  Images corrigées de l'Ovrvision Pro : les deux objectifs sont décalés horizontalement d'environ \SI{60}{\mm} pour simuler un écart pupillaire humain.
}

Ensuite, si notre visiocasque est relativement simple à concevoir, son principal inconvénient est la faible densité visuelle de l'image affichée. La densité visuelle permet de mesurer la finesse d'affichage d'une image à partir de sa définition horizontale et du champ de vision : $Densite = \frac{d_x}{FoV_x}$ \citep{Boger2017}. Ainsi, sur les visiocasques de RV du marché et sur l'Ovrvision Pro, la caméra stéréoscopique que nous utilisons, elle est limitée à environ \SI{10}{\ppd} (pixels par degré) \autorefp{tab:visual_densities}. En comparaison, le Microsoft HoloLens a une densité visuelle d'environ \SI{42}{\ppd} quand la fovéa d'un \oe il humain est de \SIrange{60}{80}{\ppd} en moyenne \citep{Kistner2014}. Nous sommes donc face à une limite technologique : une meilleure qualité des images des caméras serait limitée par le visiocasque de RV, quelque qu'il soit. Une définition de 8K (\SI{7680x4320}{\px}), à champ de vision égal, par \oe il est alors nécessaire pour atteindre la même densité visuelle : $Densite_{8K} = 7680 / 110 = \SI{69.8}{\ppd}$.

\begin{tableETS}{tab:visual_densities}{Caractéristiques d'affichage de l'Ovrvision Pro et de visiocasques de RV et RA}
  \begin{tabular}{| C{3.5cm} | C{3.25cm} | C{3.25cm} | C{3cm} |}
    \hline \textbf{Nom} & \textbf{Définition\newline(pour chaque \oe il)} & \textbf{Champ de vision\newline(pour chaque \oe il)} & \textbf{Densité visuelle}\\
    \hline Ovrvision Pro & \SI{960x950}{\px} & \SI{100x98}{\degree} & \SI{9.6}{\ppd} \\
    \hline Oculus DK2 & \SI{960x1080}{\px} & \SI{94x105}{\degree} & \SI{10.2}{\ppd} \\
    \hline Oculus Rift & \SI{1080x1200}{\px} & \SI{94x93}{\degree} & \SI{11.5}{\ppd} \\
    \hline HTC Vive & \SI{1080x1200}{\px} & \SI{110x113}{\degree} & \SI{9.8}{\ppd} \\
    \hline Microsoft HoloLens & \SI{1268x720}{\px} & \SI{30x17.5}{\degree} & \SI{42.3}{\ppd} \\
    \hline
  \end{tabular}
\end{tableETS}

Une seconde limite évidente est la portabilité de notre visiocasque \reffigureETSp{ARRift_2.jpg} : il est volumineux et dépendant d'un PC demandant une bonne puissance de calcul. Cela reste un prototype de laboratoire qui convient à notre recherche, mais un visiocasque pour usage professionnel voire personnel devra dépasser ces problématiques. Correctement optimisé, un visiocasque de RA ne demande pas énormément de ressources : le Microsoft HoloLens est portable et a une très bonne autonomie, tout comme les nouvelles versions de 2018 des visiocasques de RV Oculus et Vive. De même, les objectifs de la caméra peuvent être miniaturisés et intégrés dans le visiocasque, comme le fait le HoloLens déjà.

\figureETS[0.6]{ARRift_2.jpg}{
  Notre prototype de visiocasque de RA porté par un utilisateur : il est assez volumineux et n'est pas portable. Un câble à l'arrière de la tête de l'utilisateur relie le visiocasque au PC. Des marqueurs infrarouges en bas du visiocasque sont illuminés.
}

En outre, notre visiocasque souffre également d'un flux vidéo peu fluide à 30 images par secondes (\texten{frames per second} ou FPS en anglais). Nous sommes limités artificiellement à cette vitesse par le visiocasque de RV Oculus que nous utilisons : en effet, il impose aux applications d'atteindre 60 FPS mais les brides dans le cas contraire. Dans notre cas, la capture des images de l'Ovrvision puis la correction des distorsions ainsi que le suivi de marqueurs sont des opérations trop lourdes en calculs pour le processeur. Les performances étaient plus faibles encore à l'origine ; nous décrivons dans la \autoref{subsec:aruco_unity_scripts} quelques optimisations que nous avons mis en place. 30 FPS nous semblant acceptable et par manque de temps, nous en sommes restés là.

\figureETS[0.6]{Mur-Artal2017.jpg}{
  Carte d'une pièce par localisation et cartographie simultanées (SLAM) et les différentes positions passées, en bleu, de la caméra. Dressée en temps réel, cette carte permet à la caméra de s'y situer et d'y intégrer de la RA.\\
  Tiré de \cite[Figure 1b]{Mur-Artal2017}.
}

Enfin, l'utilisation d'une solution de RA par suivi de marqueurs est bonne pour du prototypage, car elle est techniquement simple à mettre en \oe uvre, mais elle est inadéquate pour un produit industriel ou grand public. En effet, le placement de marqueurs sur les objets que l'on veut détecter, comme un téléphone, est une étape fastidieuse. C'est pourquoi le Microsoft HoloLens et les bibliothèques ARKit pour les téléphones iOS et ARCore pour les téléphones Android préfèrent utiliser des technologies de localisation et cartographie simultanées, ou \texten{Simultaneous Localisation And Mapping} (SLAM) en anglais. Un algorithme de SLAM permet à l'appareil de se situer en temps réel dans son environnement et de l'augmenter en dressant une carte de cette pièce \reffigureETSp{Mur-Artal2017.jpg}. Pourtant cette méthode n'est pas adaptée pour détecter de petits objets en mouvement, comme un téléphone, nous ne pouvions alors pas l'utiliser.

\figureETS[0.6]{Bradski2008_1.jpg}{
  Carte de profondeur d'une tasse sur une chaise, reconstituée par vision stéréoscopique.\\
  Adapté de \cite[Figure 12.17]{Bradski2008}.
}

\figureETS[0.6]{Steptoe2013.jpg}{
  Visiocasque de \citeauthor{Steptoe2013} : deux caméras sont collées sur un visiocasque de RV, et des marqueurs infrarouges sous forme de boules grises permettent de suivre la position et l'orientation du visiocasque et de sa main.\\
  Adapté de \cite{Steptoe2013}.
}

D'autres alternatives existent. Nous aurions pu par exemple nous appuyer sur la différence entre les deux images de la caméra pour faire de la vision stéréoscopique : de manière similaire à une méthode SLAM, on peut construire une carte de profondeur de ce qui est vu par la caméra \reffigureETSp{Bradski2008_1.jpg}. Cette méthode nous a cependant semblé trop imprécise. Une troisième alternative serait d'utiliser des marqueurs infrarouges, comme sur les visiocasques de RV. Une ou deux caméras infrarouges filment le visiocasque contenant de nombreux marqueurs insérés sous sa coque \reffigureETSp{ARRift_2.jpg} et, connaissant la structure 3D de ces marqueurs, on détermine à partir des images précisément la position et l'orientation du visiocasque. Des systèmes équivalents, dit de capture de mouvements (\texten{motion capture}), comme Vicon, MotionAnalysis ou OptiTrack sont excellents pour prototyper tout en conservant une précision inférieure au millimètre \reffigureETSp{Steptoe2013.jpg}. Étant malheureusement très chers, nous avons également écarté ces solutions.


\section{Réalisation de la bibliothèque de réalité augmentée Aruco Unity}
\label{sec:aruco_unity}

\subsection{Motivation}
\label{subsec:aruco_unity_motivation}
Unity est un moteur de jeux-vidéos ainsi qu'une excellente solution pour développer avec des visiocasques de RV. Cependant, nous avons besoin de fonctionnalités manquantes pour permettre de la RA avec notre visiocasque. La bibliothèque OpenCV répond parfaitement à nos besoins, mais étant écrite en C++, elle est incompatible avec Unity utilisant le C\#. Nous avons donc réalisé la bibliothèque Aruco Unity pour rendre disponible OpenCV dans Unity. Son utilisation est décrite dans la \autoref{sec:ar_hmd}.

La solution que nous avons retenue est d'utiliser le système de greffons (\texten{plugin} en anglais, \url{https://docs.unity3d.com/Manual/Plugins.html}) d'Unity pour appeler des bibliothèques externes en C\# ou en C. Cette technique, appelée \emph{liaison} (\texten{binding} en anglais) est classique dans les domaines des jeux-vidéos et des simulations physiques : elle permet de réutiliser du code existant sans avoir à le transposer dans un autre langage et surtout d'optimiser des portions de code critiques, le C++ étant conçu pour la performance.

Le code source d'OpenCV étant libre, une alternative aurait pu être de ré-écrire en C\# les fonctionnalités dont nous avions besoin. Le code source du module aruco nous semblait assez court pour cela, étant lui-même une ré-implémentation propre de la bibliothèque originale (\url{https://www.uco.es/investiga/grupos/ava/node/26}) proposée par \cite{Garrido-Jurado2014}. Une ré-écriture des fonctions d'étalonnage et de correction de caméra nous a semblé par contre trop risquée, n'ayant pas une bonne maîtrise de la théorie mis en \oe uvre. De plus, de nombreux projets de liaison existent vers d'autres langages comme Python (\url{https://docs.opencv.org/master/d6/d00/tutorial_py_root.html}), Javascript (\url{https://docs.opencv.org/master/d5/d10/tutorial_js_root.html}) ou Java (\url{https://opencv.org/platforms/android/}). L'écriture d'une liaison de C++ vers C\# semble donc plus sûre qu'une ré-écriture.

Enfin, quelques bibliothèques de RA par suivi de marqueurs sont déjà disponibles pour Unity, mais aucune ne nous a convenu :
\begin{itemize}
  \item Vuforia (\url{https://unity3d.com/fr/partners/vuforia}) ne supporte pas notre caméra et le code source est propriétaire donc non modifiable.
  \item ARToolKit (\url{https://github.com/artoolkit/arunity5}) ne supporte pas notre caméra non plus, mais malgré un code source libre, le projet est abandonné et il semble difficile d'y ajouter les fonctions d'étalonnage pour une caméra stéréoscopique avec objectifs \texten{fisheye}.
  \item OpenCV for Unity (\url{https://enoxsoftware.com/opencvforunity/}) propose des liaisons vers tous les modules d'OpenCV dont nous avons besoin, mais nous craignons de manquer de flexiblité avec son code source propriétaire si nous devons apporter des modifications dans le code source d'OpenCV.
\end{itemize}

\subsection{Architecture}
\label{subsec:aruco_unity_architecture}

\figureLayoutETS{ArucoUnityComponents}{%
  \begin{tikzpicture}
    \begin{umlcomponent}{ArucoUnity}
      \umlbasiccomponent[name=Liaison]{Liaison C-C\#}
      \umlbasiccomponent[x=6, name=Scripts]{Scripts C\#}
      \umlassemblyconnector[interface={Exposer OpenCV}, name=SL]{Scripts}{Liaison}
    \end{umlcomponent}
    \umlbasiccomponent[name=ArucoUnityPlugin, y=-5]{Greffon ArucoUnity}
    \umlassemblyconnector[interface={Appeler l'interface C du greffon}, name=LP, anchors=-90 and 90]{Liaison}{ArucoUnityPlugin}
    \umlbasiccomponent[x=6, y=-5]{OpenCV}
    \umlassemblyconnector[interface=Appeler]{ArucoUnityPlugin}{OpenCV}
  \end{tikzpicture}%
}{
  Diagramme des composants d'ArucoUnity. Le greffon enveloppe OpenCV dans une interface en C. Il est appelé par la liaison qui expose alors une interface orientée objet en C\# similaire à celle d'OpenCV. Les scripts permettent de travailler directement dans l'interface d'Unity avec les fonctionnalités d'OpenCV sans avoir à programmer.
}

Aruco Unity est composé de trois couches logicielles, s'appuyant sur OpenCV \reffigureETSp{ArucoUnityComponents}. Tout d'abord, nous avons créé une bibliothèque exposant les modules aruco, calib3d et ccalib d'OpenCV avec une interface en C (\url{https://github.com/NormandErwan/ArucoUnity/tree/master/ArucoUnityPlugin/}), que l'on peut appeler depuis C\#. Nous avons ensuite créé un projet Unity et placé la bibliothèque dans le dossier \code{Assets/Plugin} : elle est alors détectée comme un greffon par Unity \reffigureETSp{ArucoUnityPlugin.jpg}.

\figureETS{ArucoUnityPlugin.jpg}{%
  Le greffon d'ArucoUnity exposant les modules aruco, calib3d et ccalib d'OpenCV avec une interface en C dans le projet Unity d'Aruco Unity. Les autres modules présents sont des dépendances.
}

Nous avons ensuite écrit les classes de liaison dans ce projet (\url{https://github.com/NormandErwan/ArucoUnity/tree/master/Assets/ArucoUnity/Scripts/Plugin/})  : elles reproduisent en C\# l'interface orientée objet d'OpenCV en appelant en arrière-plan notre greffon. On peut alors utiliser en toutes les fonctionnalités que nous avons portées d'OpenCV dans Unity de manière similaire qu'en C++.

Enfin, nous avons écrit des scripts C\# pour Unity que nous utilisons dans la \autoref{sec:ar_hmd}. Nous avions d'abord écrit de simples scripts séquentiels d'étalonnage et de suivi de marqueur, mais le besoin s'est vite faite sentir de les rendre plus robustes et performants. De plus, le travail dans Unity se fait préférentiellement en deux temps : les développeurs programment les fonctionnalités dans les scripts, et les artistes les utilisent et configurent à volonté avec l'interface graphique d'Unity sans avoir à toucher au code source. Notre bibliothèque ayant demandé un certain effort de développement, faciliter son usage nous permet d'encourager sa réutilisation pour des équipes souhaitant travailler sur des problématiques similaires.

\subsection{Réalisation du greffon et de la liaison}
La première étape de réalisation a donc été le greffon pour Unity, pour rendre accessible les fonctions des modules aruco, calib3d et ccalib d'OpenCV. Il existe plusieurs techniques de liaison. Nous choisissons la technique la plus simple : P/Invoke (\texten{Platform Invocation Services}). Le principal défi dans son utilisation est la rigueur de la gestion de la mémoire : d'une part on gère manuellement la création et la destruction d'objets et d'autre part le greffon et les scripts doivent se partager cette mémoire.

\begin{listingETS}{cpp}{lst:aruco_unity_plugin_cpp}{Interface simplifiée en C autour de la classe Mat d'OpenCV dans le greffon d'ArucoUnity.}
  extern "C" {
    cv::Mat* au_cv_Mat_new1() {
      return new cv::Mat();
    }

    cv::Mat* au_cv_Mat_new2(int rows, int cols, int type) {
      return new cv::Mat(rows, cols, type);
    }
    
    void au_cv_Mat_delete(cv::Mat* mat) {
      delete mat;
    }

    int au_cv_Mat_total(cv::Mat* mat) {
      return mat->total();
    }
  }
\end{listingETS}

L'\autoref{lst:aruco_unity_plugin_cpp} montre un exemple simplifié de l'interface en C la classe \code[cpp]{cv::Mat}, qui permet de manipuler matrices et images dans OpenCV. On utilise en fait le patron de conception adaptateur : chaque constructeur, destructeur et fonction est enveloppé par une fonction C, que l'on nomme \code[cpp]{new} pour les constructeurs, \code[cpp]{delete} pour les destructeurs ou avec le nom de la fonction appelée. Le polymorphisme n'étant pas supporté en C, chaque surcharge est également enveloppée par une fonction : on les numérote alors avec un suffixe de $1$ à $n$. Le C ne comportant pas non plus d'espace de nom, on ajoute à chaque fonction le préfixe \code[cpp]{au_cv_} suivi du nom de la classe pour éviter des collisions de noms. Enfin, on entoure toutes les fonctions avec l'instruction \code[cpp]{extern "C"} pour indiquer au compilateur et à l'éditeur de liens que nous travaillons bien avec des fonctions en C.

Le passage de paramètres est la problématique principale avec cette technique. Certains types primitifs de C\# comme \code{int} ou \code{float} ont des équivalences en C++, on peut alors les passer directement en paramètres au greffon. Cependant, d'autres types plus complexes sont représentés différemment dans les deux langages, par exemple les classes, les chaînes de caractères \cite{Peterson2015}. Nous avons alors voulu faire simple : tous les objets OpenCV sont alloués avec le code C++, via les fonctions C pour la construction et la destruction que nous venons de décrire. Elles retournent alors un pointeur (qui n'est qu'un entier sur une adresse mémoire) au code C\#. Quand on souhaite exécuter une fonction sur un objet, il suffit de simplement y passer son pointeur comme paramètre. Sans rigueur, il y a plus de risques d'erreurs de segmentation ou de fuites de mémoire mais notre couche de liaison prend en charge cette responsabilité sans que l'utilisateur n'ait à s'en soucier.

\begin{listingETS}{cs}{lst:aruco_unity_plugin_cs}{Liaison simplifiée pour exposer en C\# la classe Mat d'OpenCV.}
  public class Mat
  {
    [DllImport("ArucoUnityPlugin")]
    static extern IntPtr au_cv_Mat_new1();

    [DllImport("ArucoUnityPlugin")]
    static extern IntPtr au_cv_Mat_new2(int rows, int cols, int type);

    [DllImport("ArucoUnityPlugin")]
    static extern void au_cv_Mat_delete(IntPtr mat);
    
    [DllImport("ArucoUnityPlugin")]
    static extern int au_cv_Mat_total(IntPtr mat);

    public IntPtr Ptr { get; }

    public Mat()
    {
      Ptr = au_cv_Mat_new1();
    }

    public Mat(int rows, int cols, int type)
    {
      Ptr = au_cv_Mat_new2(rows, cols, type);
    }

    ~Mat()
    {
      au_cv_Mat_delete(Ptr);
    }

    public int Total()
    {
      return au_cv_Mat_total(Ptr);
    }
  }
\end{listingETS}

L'\autoref{lst:aruco_unity_plugin_cs} montre la liaison associée à l'interface de l'\autoref{lst:aruco_unity_plugin_cpp}. Elle est composée de deux parties. Il y a d'abord les appels au greffon : on déclare des fonctions statiques avec la même signature (nom et paramètres) que celles du greffon et on indique au compilateur de les appeler dans le greffon avec l'attribut \code{DllImport}. Ensuite, on construit une interface orientée objet reproduisant celle d'OpenCV : on utilise le patron de conception façade pour masquer la liaison qui est complexe à utiliser et source d'erreurs de segmentations ou de fuites de mémoire si mal utilisée. La classe \code{Mat} reconstruite en C\# appelle donc automatiquement un des constructeurs C++ quand l'utilisateur désire en créer une instance, par exemple \code{var mat = new Mat()}. Le pointeur vers la classe C++ sous-jacente est stocké dans l'instance C\# correspondante. Nous avons placé toutes les classes de liaison dans l'espace de nom \code{ArucoUnity.Plugin} ; pour simplifier les extraits de code, nous ne les affichons pas.

Enfin, nous avons intégré le conteneur \code[cpp]{vector} de la bibliothèque standard de C++. Utilisé dans le module aruco, ce modèle (\texten{template} en anglais) impose un peu plus de travail. Nous devons en effet créer une interface et une classe de liaison pour chaque cas d'utilisation : par exemple, \code{VectorMat} ou \code{VectorVectorInt} qui permettent de manipuler respectivement les objets \code[cpp]{std::vector<cv::Mat>} et \code[cpp]{std::vector<vector<int>>} via des fonctions C comme \code{IntPtr au_std_vectorMat_new()}. Nous avons décidé cette fois de ne pas répliquer l'interface C++ en utilisant des génériques C\#, car chaque cas d'utilisation devant être codé, le générique aurait demandé un travail complexe non nécessaire.

\begin{listingETS}{cs}{lst:aruco_camera_cs}{Classe C\# ArucoCamera simplifée pour passer une image à OpenCV.}
  public class ArucoCamera : MonoBehaviour, IArucoCamera
  {
    public Mat Image;
    public Texture2D Texture;
    public byte[] ImageData;
    public Size Resolution;

    public event Action UndistortImages;
    public event Action ImageUpdated;

    // Exécuté au démarrage
    protected virtual void Start()
    {
      Image = new Mat(Resolution, Format.8UC3); // Format RVB : 3 octets par pixels (3x8=24 bits)
      Texture = new Texture2D(Resolution.x, Resolution.y, TextureFormat.RGB24);
      ImageData = new byte[3 * Images[cameraId].Total()];
      Image.DataByte = ImageData;
    }

    // Exécuté à chaque frame
    protected virtual void Update()
    {
      UpdateImages(); // Copie de l'image de la caméra dans ImageData

      UndistortImages(); // Correction de l'image
      ImageUpdated(); // Suivi des marqueurs

      // Mise à jour de l'image Unity
      Texture.LoadRawTextureData(image.DataIntPtr, 3 * Images[cameraId].Total());
      Texture.Apply();
    }

    protected abstract void UpdateImages();
  }
\end{listingETS}

\begin{listingETS}{cs}{lst:ovrvision_aruco_camera}{Classe C\# OvrvisionArucoCamera simplifiée.}
  public class OvrvisionArucoCamera : StereoArucoCamera
  {
    [DllImport("ovrvision"]
    static extern int ovOpen(int cameraId, float markerLength, int mode);
    ... // Autres liaisons

    protected override void Start()
    {
      Resolution = new Size(960, 950);
      ovOpen(cameraId: 0, markerLength: 1, mode: CameraMode.VR_960x950_60FPS);
      ovSetCamSyncMode(false);
      base.Start();
    }

    protected override void UpdateImages()
    {
      ovPreStoreCamData(ProcessingMode.Demosaic); // Retourner les images couleurs non corrigées
      ovGetCamImageRGB(ImageData[0], 0); // Copie image oeil gauche
      ovGetCamImageRGB(ImageData[1], 1); // Copie image oeil droit
    }
  }
\end{listingETS}

Le code C++ travaille par instants avec les images capturées par la caméra, cette fois allouées en mémoire dans le code C\#. On passe alors simplement un pointeur de l'image, ainsi que sa taille, aux fonctions d'OpenCV. Concrètement, on crée un objet \code{Mat} dans OpenCV et un objet \code{Texture2D} dans Unity pour manipuler l'image ainsi qu'un tableau partagé entre les deux objets pour représenter le contenu de l'image. On utilise le format le plus courant pour encoder l'image dans le tableau : chaque pixel est représenté en couleurs rouge, vert, bleu (RVB) avec un octet (8 bits) par couleur, soit un total de $3 \times 8 = 24\text{ bits}$ par pixel. On travaille donc avec une paire d'images de $2 \times 3 \times 960 \times 950 = \SI{5.472}{\mega\byte}$ dans le cas de l'Ovrvision, ce qui est assez conséquent pour les opérations de suivi de marqueurs et de corrections d'images. C'est pourquoi on partage la mémoire pour éviter des copies entre Unity et le greffon, coûteuses en temps de processeur.

Nous avons aussi besoin d'accéder à la caméra Ovrvision Pro. La caméra est fonctionnelle, les deux images retournées sont synchronisées mais, comme nous l'avons expliqué dans la \autoref{subsec:technical_choices}, son étalonnage et le suivis de marqueurs ne sont pas bons avec les scripts C\# fournis par le constructeur. Un greffon similaire au nôtre étant disponible, nous avons développé la classe \code{OvrvisionArucoCamera} de liaison \autorefp{lst:ovrvision_aruco_camera} pour pouvoir étalonner cette caméra avec Aruco Unity et y intégrer de la RA.

\figureLayoutETS{CoordinatesSystem}{%
  \subfigureETS[0.1]{CoordinatesSystemLeft.png}{Unity : système de coordonnées main gauche.}%
  \figurehspace%
  \subfigureETS[0.1]{CoordinatesSystemRight.png}{OpenCV : système de coordonnées main droite.}%
}{
  Illustrations des systèmes de coordonnées utilisés par Unity et OpenCV ; une inversion de l'axe des Y permet de passer de l'un à l'autre.
}

Nous avons également ajouté quelques fonctions de conversion, Unity et OpenCV utilisant des conventions et des classes différentes pour encoder les transformations d'objets dans la scène. Pour encoder la position, Unity utilise un système de coordonnées main gauche \reffigureETSp{CoordinatesSystemLeft.png} tandis qu'OpenCV utilise un système main droite \reffigureETSp{CoordinatesSystemRight.png}. On inverse alors l'axe des Y pour passer de l'un à l'autre avec la fonction \code{Vector3 Vec3d.ToPosition()}. En outre, OpenCV utilise des matrices de rotation ou des angles de rotations, selon les fonctions. OpenCV propose des fonctions de conversions, que nous avons retranscrites dans la liaison. On convertit les angles de rotations en quaternions d'Unity suivant l'algorithme de \cite{Baker1998} avec la fonction \code{Quaternion Vec3.ToRotation()}.


\subsection{Réalisation des scripts pour Unity}
\label{subsec:aruco_unity_scripts}

\figureLayoutETS{ArucoUnityArchitecture}{%
  \begin{tikzpicture}
    \begin{umlpackage}{Cameras}
      \umlsimpleclass[type=interface, x=0, y=0]{IHasArucoCameraParameters}

      \umlsimpleclass[type=interface, x=5]{IArucoCamera}

      \umlsimpleclass[type=interface, x=11]{IArucoCameraController}
      \umluniassoc{IArucoCameraController}{IArucoCamera}

      \umlsimpleclass[type=interface, x=5, y=-2]{IArucoCameraUndistortion}
      \umlVHVinherit[anchor1=85, arm2=-1.1]{IArucoCameraUndistortion}{IArucoCameraController}
      \umlVHVinherit[anchor1=95, arm2=-1.1]{IArucoCameraUndistortion}{IHasArucoCameraParameters}

      \umlsimpleclass[type=interface, x=10, y=-2]{IArucoCameraDisplay}
      \umlVHVinherit[arm2=-1.1]{IArucoCameraDisplay}{IArucoCameraController}
      \umluniassoc{IArucoCameraDisplay}{IArucoCameraUndistortion}
    \end{umlpackage}

    \begin{umlpackage}{Calibration}
      \umlsimpleclass[type=abstract, x=3, y=-4.5]{CalibrationFlags}
      
      \umlsimpleclass[type=abstract, x=7.75, y=-4.5]{ArucoCameraCalibration}
      \umlVHVimpl[anchor2=-150, arm2=-.3]{ArucoCameraCalibration}{IArucoCameraController}
      \umlVHVimpl[anchor2=-30, arm2=-.3]{ArucoCameraCalibration}{IHasArucoCameraParameters}
      \umluniassoc{ArucoCameraCalibration}{CalibrationFlags}
    \end{umlpackage}

    \begin{umlpackage}{Objects}
      \umlsimpleclass[type=abstract, x=10.5, y=-7]{ArucoObject}

      \umlsimpleclass[x=7.5, y=-8.5]{ArucoMarker}
      \umlVHVinherit{ArucoMarker}{ArucoObject}

      \umlsimpleclass[type=abstract, x=10.5, y=-8.5]{ArucoBoard}
      \umlVHVinherit{ArucoBoard}{ArucoObject}
      \umlHVHuniassoc[anchor1=0, arm1=2.5, anchor2=0]{ArucoCameraCalibration}{ArucoBoard}

      \begin{umlpackage}{Trackers}
        \umlsimpleclass[type=interface, x=5, y=-10, anchor=north]{IArucoObjectsController}
        \umlVHuniassoc{IArucoObjectsController}{ArucoObject}
        
        \umlsimpleclass[type=interface, x=10, y=-10, anchor=north]{IHasDetectorParameter}

        \umlsimpleclass[type=interface, x=10, y=-12, anchor=north]{IArucoObjectsTracker}
        \umlVHVinherit{IArucoObjectsTracker}{IArucoObjectsController}
        \umlVHVinherit{IArucoObjectsTracker}{IHasDetectorParameter}
        \umlHVinherit[anchor2=-17]{IArucoObjectsTracker}{IArucoCameraController}
      \end{umlpackage}

      \begin{umlpackage}{Displayers}
        \umlsimpleclass[type=abstract, x=0.2, y=-10, anchor=north]{ArucoObjectDisplayer}
        \umlVHuniassoc[anchor1=45]{ArucoObjectDisplayer}{ArucoObject}

        \umlsimpleclass[type=abstract, x=0.2, y=-12, anchor=north]{ArucoObjectCreator}
        \umlinherit{ArucoObjectCreator}{ArucoObjectDisplayer}
      \end{umlpackage}
    \end{umlpackage}
  \end{tikzpicture}%
}{Diagramme de classes simplifé de la couche de scripts C\# d'ArucoUnity. Seules les interfaces ou les classes abstraites de haut niveaux sont affichées : dans chaque cas, le patron de conception stratégie est appliqué pour supporter différents types de caméras. Elles font toutes parties de l'espace de nom \code{ArucoUnity}, non affiché ici.}

Ce travail avec les images des caméras et les conversions montre pourquoi nous avons besoin d'une couche supplémentaire de scripts C\# au-dessus de celle de la liaison pour faciliter l'usage d'OpenCV dans Unity. Les deux systèmes ne sont en effet pas conçus pour travailler ensemble : on masque ainsi cette complexité avec les scripts, formant une façade supplémentaire.

Avec ces scripts nous mettons aussi en place le fonctionnement de notre visiocasque décrit à la \autoref{subsec:prototype_operation} en suivant la procédure que nous décrivons à la \autoref{sec:ar_hmd}. Nous avons donc d'abord écrit premier script d'étalonnage et un second qui configurait au démarrage puis qui corrigeait à chaque \texten{frame} les images et plaçait les éléments virtuels. Nous y avons rapidement appliqué quelques principes de conception pour l'améliorer. Nous utilisons donc une seule responsabilité par classe \autorefp{ArucoUnityArchitecture} :
\begin{itemize}
  \item \code{IArucoCamera} récupère les images de la caméra physique.
  \item \code{IArucoCameraUndistortion} corrige les images capturées.
  \item \code{IArucoObjectsTracker} et \code{ArucoObject} détectent les marqueurs sur les images.
  \item \code{IArucoCameraDisplay} configure la caméra virtuelle et place les éléments virtuels.
  \item \code{ArucoCameraCalibration} étalonne la caméra physique.
  \item \code{CameraParametersController} gère les paramètres de l'étalonnage.
\end{itemize}
\bigskip

Pour chacune de ces interfaces, nous utilisons le patron de conception stratégie : nous créons une classe abstraite implémentant le procédé (nous les décrivons à la \autoref{sec:ar_hmd}) mais laissant les détails spécifiques à un type de caméra à des enfants de cette classe. Cela permet de multiples combinaisons entre les implémentations d'\code{IArucoCamera}, de \code{IArucoCameraDisplay} et de \code{IArucoCameraUndistortion}. Un développeur peut donc ajouter facilement le support dans Aruco Unity d'une nouvelle caméra, d'un nouveau type d'affichage ou la correction d'un nouveau type d'objectif photographique.

\figureLayoutETS{ArucoUnitySequence}{
  \begin{tikzpicture}
    \begin{umlseqdiag}
      \umlobject{ArucoCamera}
      \umlobject{ArucoCameraUndistortion}
      \umlobject{ArucoObjectsTracker}
      \umlobject{ArucoCameraDisplay}

      \begin{umlcallself}[op=UpdateImages()]{ArucoCamera}
        \begin{umlfragment}[type=par, inner xsep=3]
          \begin{umlcallself}[op={SwapImages()}]{ArucoCamera}
          \end{umlcallself}
          \umlfpart[thread]
          \begin{umlcallself}[op={GetImages()}]{ArucoCamera}
          \end{umlcallself}
        \end{umlfragment}
      \end{umlcallself}

      \begin{umlcall}[op={UndistortImages()}]{ArucoCamera}{ArucoCameraUndistortion}
        \begin{umlfragment}[type=par, inner xsep=3]
          \begin{umlcallself}[op={SwapImages()}]{ArucoCameraUndistortion}
          \end{umlcallself}
          \umlfpart[thread]
          \begin{umlcallself}[op={Undistort()}]{ArucoCameraUndistortion}
          \end{umlcallself}
        \end{umlfragment}
      \end{umlcall}

      \begin{umlcall}[op={ImagesUpdated()}]{ArucoCamera}{ArucoObjectsTracker}
        \begin{umlfragment}[type=par, inner xsep=3]
          \begin{umlcallself}[op={CopyImages()}]{ArucoObjectsTracker}
          \end{umlcallself}
          \begin{umlcall}[op={PlaceArucoObjects()}]{ArucoObjectsTracker}{ArucoCameraDisplay}
          \end{umlcall}
          \umlfpart[thread]
          \begin{umlcallself}[op={TrackArucoObjects()}]{ArucoObjectsTracker}
          \end{umlcallself}
        \end{umlfragment}
      \end{umlcall}
    \end{umlseqdiag}
  \end{tikzpicture}
}{Diagramme de séquence simplifé de chaque \texten{frame} de la couche de scripts C\# d'ArucoUnity.}

Enfin, nous améliorons les performances en externalisant certains calculs sur plusieurs fils d'exécution (\texten{thread} en anglais). Par défaut, les scripts tournent en effet sur le fil d'exécution principal d'Unity. L'ensemble des scripts doit donc s'exécuter rapidement pour avoir une application fluide, en moins de \SI{16}{\ms} pour atteindre 60 FPS par exemple. La capture des images, leur correction et le suivi des marqueurs sont les opérations les plus lentes. Nous créons donc un fil d'exécution pour chacune de ces trois calculs ci-dessous : chacun tourne alors de manière indépendante et ne bloque plus celui d'Unity \reffigureETSp{ArucoUnitySequence}. Si le fil d'exécution de la caméra a récupéré de nouvelles images, celui d'Unity va les copier dans les autres fils et recopie les images corrigées pour les afficher dans le visiocasque. Cette solution nous permet d'avoir d'excellentes performances avec une webcam où nous avions à l'origine peu de fluidité, mais n'est pas encore totalement satisfaisante avec l'Ovrvision \autorefp{subsec:solution_discusion}. Dans cette configuration, ce sont les multiples copies des images qui prennent trop de temps au processeur, mais nous en sommes resté là par manque de temps et la vitesse de 30 FPS avec notre visiocasque étant tout de même satisfaisante.


\section{Intégration de la réalité augmentée dans le visiocasque}
\label{sec:ar_hmd}

\subsection{Procédure}
\label{subsec:ar_hmd_procedure}
Les fonctionnalités d'OpenCV étant maintenant disponibles dans Unity \autorefp{sec:aruco_unity}, nous pouvons maintenant enfin mettre en place la RA dans notre visiocasque. Pour cela, nous devons tout d'abord comprendre le principe d'une caméra \autorefp{subsec:camera_theory}. Puis nous étudions le cas le plus simple de l'alignement d'une caméra physique monoscopique avec un objectif rectilinéaire \autorefp{subsec:pinhole_camera_calibration}. Nous itérons ensuite avec notre caméra stéréoscopique à objectifs \texten{fisheye} \autorefp{subsec:ovrvision_camera_calibration}. Les caméras physiques et virtuelles alignées nous pourrons alors utiliser les fonctions de suivi de marqueurs pour placer les éléments virtuels.

Aruco Unity permet d'étalonner une caméra facilement et directement dans Unity, sans avoir à programmer. Après l'étalonnage, notre bibliothèque de RA corrige la caméra physique et aligne la caméra virtuelle automatiquement sans que l'utilisateur n'ait à s'en soucier lors du suivi de marqueurs. Nous décrivons ainsi dans cette section l'utilisation des scripts C\# d'Aruco Unity, le code OpenCV qu'ils exécutent en arrière-plan ainsi que la théorie nécessaire ; le lecteur peut se référer à \cite[chap. 18-19]{Kaehler2017} pour une théorie et des procédures plus complètes.

Cette section a son équivalent en anglais sur plusieurs articles dans la documentation en ligne d'Aruco Unity (\url{https://normanderwan.github.io/ArucoUnity/}). Ils sont plus pratiques sans détailler la théorie pour permettre à nouvel utilisateur de rapidement construire son application de RA.

\subsection{Fonctionnement d'une caméra}
\label{subsec:camera_theory}

\subsubsection{Caméra virtuelle}
Une caméra est un dispositif qui capture une vue en 2D d'une scène en 3D. Cette vue est une projection en \emph{perspective} rectilinéaire. Le plus simple des dispositifs de projection en perspective est le \emph{sténopé} (\texten{pinhole} en anglais) : c'est une simple boite percée d'une petite ouverture, le \emph{centre de projection}, noté $O$, sur une de ses face, et d'un écran, \emph{le plan image}, sensible à la lumière sur la face opposée. La scène n'est donc visible que par un unique point de vue : ainsi, chaque point sur le plan image ne provenant que d'un seul rayon de lumière à travers le centre de projection, une image nette et inversée de la scène va se former \reffigureETSp{Pinhole.png}.

\figureETS[1]{Pinhole.png}{
  Un sténopé : une petite ouverture sur une face de la boite laisse entrer la lumière, projettant une scène 3D (par exemple les points $P_1$ et $P_2$) en une image inversée sur la face opposée à l'ouverture (les projections respectives sont les points $p_1$ et $p_2$).
}

L'image formée sur le plan image étant toujours nette, on peut modifier le champ de vision du sténopé en déplaçant le plan image le long de l'\emph{axe optique}, la droite perpendiculaire au plan image et passant par le centre de projection, la distance entre le plan image et le centre de projection étant la \emph{longueur focale} $f$. On calcule alors le champ de vision du sténopé avec l'\autoref{eq:fov} avec cette distance $f$, habituellement donnée en millimètres par le constructeur de la caméra. Ainsi, à taille équivalente du plan image, une longueur focale plus courte donne lieu à un plus grand champ de vision.

Pour simplifier nos équations, on peut modifier le modèle du sténopé tout en le gardant valide mathématiquement. On crée un nouveau plan image symétrique au véritable plan image par rapport au centre de projection, placé donc à la même distance $f$ du centre optique. La vue générée sur ce plan image virtuel reste la même mais est non inversée : les rayons de la scène sont toujours projeté vers le centre de projection mais interceptés par ce nouveau plan image \reffigureETSp{PerspectiveProjection.png}. Le repère est placé sur le centre de projection.

\figureETS[1]{PerspectiveProjection.png}{
  Projection en perspective rectilinéaire : un point $P=(X,Y,Z)$ va être projeté sur le plan image situé à une distance $z=f$ du centre de projection $O$ en un point $p=(x,y,f)$. Une caméra virtuelle est basée sur ce principe et on peut également simplifier le sténopé à ce modèle.
}

De manière simplifiée, c'est également le fonctionnement d'une caméra virtuelle monoscopique. En suivant l'\autoref{eq:virtual_camera_projection}, on projette chaque point $P=(X,Y,Z)$ de la scène en un point $p=(x,y,z)$ sur le plan image. Les unités sont arbitraires ; Unity utilise par exemple des mètres. En capturant un nombre suffisamment élevé d'images par secondes (au moins 15 FPS) avec cette caméra, on a l'illusion de voir une vidéo.

\begin{equation}
  \label{eq:virtual_camera_projection}
  x = f \frac{X}{Z},\qquad y = f \frac{Y}{Z},\qquad z = f
\end{equation}

\subsubsection{Caméra physique}
Le sténopé est un modèle de caméra également suffisant pour décrire une caméra physique monoscopique. On y ajoute alors un capteur numérique placé sur le plan image pour enregistrer les images qui se forment sur le plan image. Cet ajout introduit de nouveaux paramètres que l'on va mesurer lors de l'étalonnage de la caméra \autorefp{subsec:pinhole_camera_calibration}. Comme nous travaillons seulement avec les images capturées dans ce procédé, les mesures sont donc faites en pixels et non plus en millimètres. De plus, nous déplaçons le repère dans le coin en bas à gauche du plan image.

Premièrement, le centre du capteur utilisé ne coïncide pas totalement avec l'axe optique : cela requerrait une précision trop importante et non nécessaire lors de la fabrication de la caméra. L'axe optique va donc intersecter le plan image non pas en son centre, mais sur un point décalé appelé \emph{centre optique} et noté $c$. On mesure en pixels le décalage $(c_x,c_y)$ sur l'image capturée.

En outre, la longueur focale mesurée de la caméra peut être différente sur les deux axes $X$ et $Y$ du plan image. On note $f_x$ et $f_y$ les deux longueurs focales mesurées. Cela est dû a certains capteurs, de mauvaise qualité, produisant des pixels rectangulaires et non carrés. On pourrait également les calculer en connaissant la taille physique $(L,H)$ et la définition $(d_x,d_y)$ du capteur et la longueur focale physique de la caméra avec l'\autoref{eq:focal_lengths}, mais on n'a pas toujours accès avec fiabilité à toutes ces informations\footnote{Les propriétés d'un appareil photo sont bien décrites sur internet mais c'est souvent moins le cas de la caméra d'un téléphone par exemple.}.

\begin{equation}
  \label{eq:focal_lengths}
  f_x = f \frac{L}{d_x},\qquad f_y = f \frac{H}{d_y}
\end{equation}

Ces nouveaux paramètres font partie de la \emph{matrice intrinsèque} de la caméra, notée $K$. Elle permet de transformer un point $P$ de la scène est transformé en un point $p'$ \autorefp{eq:projection}, qu'on projette ensuite sur le plan image en divisant ses coordonnées par $Z$ comme dans l'\autoref{eq:virtual_camera_projection}.

\begin{equation}
  \label{eq:projection}
  \begin{pmatrix}
    x'\\
    y'\\
    z'
  \end{pmatrix}
  =
  \underbrace{
    \begin{pmatrix}
      f_x & 0 & c_x\\
      0 & f_y & c_y\\
      0 & 0 & 1
    \end{pmatrix}
  }_\text{K}
  \begin{pmatrix}
    X\\
    Y\\
    Z
  \end{pmatrix}
\end{equation}

On résume donc la projection d'un point $P=(X,Y,Z)$ de la scène par une caméra physique monoscopique en un point $p=(x,y)$ sur l'image en suivant l'\autoref{eq:physical_camera_projection}.

\begin{equation}
  \label{eq:physical_camera_projection}
  x = f_x \frac{X}{Z} + c_x,\qquad y = f_y \frac{Y}{Z} + c_x
\end{equation}

Enfin, une caméra physique monoscopique requiert également un objectif photographique. En effet, l'ouverture infiniment petite du sténopé ne laissant passer que trop peu de lumière demande infiniment de temps pour qu'une image visible se forme. C'est un problème pour une caméra si l'on souhaite capturer plusieurs images par secondes. La solution est de faire une ouverture plus grande pour laisser passer plus de lumière et de placer donc un objectif photographique pour continuer à concentrer la lumière sur le plan image en une seule image nette. L'inconvénient est que les lentilles utilisées introduisent nécessairement des distorsions en barillet dans l'image \reffigureETSp{Distorsion}. De plus, le capteur et l'objectif ne sont pas forcément bien montés parallèles, entraînant une distorsion tangente supplémentaire, le plan image ne coïncidant pas totalement avec le capteur. D'autres types de distorsions existent mais peuvent être négligés \citep[p. 377]{Bradski2008}. L'ensemble des \emph{paramètres de distorsion} qui sont mesurés est noté $D$. Pour plus de détails sur ces paramètres, nous redirigeons le lecteur vers \cite[p. 375]{Bradski2008}.

\figureLayoutETS{Distorsion}{%
  \subfigureETS[0.15]{Distorsion_1.png}{Aucune distorsion.}%
  \figurehspace[10]%
  \subfigureETS[0.15]{Distorsion_2.png}{Distorsion en barillet, la grille est vue bombée vers l'extérieur.}%
  \figurehspace[10]%
  \subfigureETS[0.15]{Distorsion_3.png}{Distorsion en coussinet, la grille est vue écrasée vers l'intérieur.}%
}{
  Illustrations d'une grille vue à travers un système optique avec différents types de distorsion.
}

Ces distorsions éloignent le fonctionnement de la caméra du modèle idéal du sténopé. C'est pourquoi il est important de corriger les déformations que cela produit sur les images, comme si elles avaient été capturées par un sténopé avec les mêmes paramètres intrinsèques et donc aucune distorsions, pour conserver un bon alignement avec la caméra virtuelle.

\subsection{Étalonnage d'une caméra}
\label{subsec:pinhole_camera_calibration}
Étalonner une caméra monoscopique rectilinéaire consiste donc à déterminer sa matrice intrinsèque $K$ et ses paramètres de distorsion $D$. Nous avons décrit les équations qui permettent de projetter un point $P$ de la scène en un point $p$ sur le plan image. Connaissant les coordonnées de ces deux points, nous pouvons donc écrire les vecteurs de translation $t = (t_x, t_y, t_z)$ et rotation $r = (r_x, r_y, r_z)$ permettant de transformer un point $P$ de la scène en un point $p$ sur le plan image. En connaissant assez de couple de points $(P_i, p_i)$, on peut alors poser un ensemble d'équations pour résoudre $K$. Ainsi, les déviations des points $p_i$ du modèle de sténopé ainsi calculé permettent de déterminer $D$.

\figureETS{Bradski2008_2.jpg}{
  Une caméra physique monoscopique est étalonné avec OpenCV en capturant plusieurs images d'un échiquier imprimé sur une planche rigide capturé dans différentes positions et angles de vues.\\
  Tiré de \cite[Figure 11-9]{Bradski2008}.
}

Concrètement, on utilise une image d'un échiquier qu'on capture avec la caméra dans plusieurs positions et angles de vues \reffigureETSp{Bradski2008_2.jpg}, les points utilisés étant les intersections entre les carrés noirs et blancs. On connaît par avance les coordonnées relatives des points $P_i$ entre eux (il suffit de le mesurer sur l'échiquier) et ces intersections sont détectées facilement sur l'image capturée par la caméra. La première étape de l'étalonnage consiste donc à capturer avec la caméra quelques images d'un échiquier dont on connaît la configuration.

\figureETS[1]{CharucoBoardCreation.jpg}{
  Création d'un échiquier d'étalonnage avec Aruco Unity : à gauche les scripts de configuration, à droite l'échiquier vu dans la scène Unity. Les marqueurs, décrits dans la \autoref{sec:aruco_unity}, dans les carrés blancs permettent l'amélioration de la détection de l'échiquier.
}

On crée alors notre échiquier d'étalonnage \reffigureETSp{CharucoBoardCreation.jpg} :
\begin{itemize}
  \item On crée un projet Unity configuré avec Aruco Unity \autorefp{sec:aruco_unity}.
  \item Dans ce projet, on ajoute un nouvel objet vide.
  \item On ajoute à l'objet le script \code{ArucoCharucoBoard}, qui représente l'échiquier, et on le configure :
  \begin{itemize}
    \item la taille des carrés en pixels avec le champ \code{SquareSideLength} ;
    \item le nombre de carrés avec \code{SquaresNumberX} et \code{SquaresNumberY} ;
    \item la taille des marqueurs en pixels avec \code{MarkerSideLength} ;
    \item la marge en pixels entre les marqueurs et les carrés blancs qui les contiennent avec \code{MarginLength}.
  \end{itemize}
  \item On ajoute à l'objet le script \code{ArucoObjectCreator}, qui permet de créer l'image de l'échiquier, et on fait pointer \code{ArucoObject} vers \code{ArucoCharucoBoard}.
  \item On démarre la scène Unity : l'image de l'échiquier s'affiche et est enregistrée sur le disque dur dans le dossier du projet (le chemin est indiqué par \code{OutputFolder}).
  \item On imprime l'image que l'on colle fermement sur une surface rigide pour qu'elle reste la plus plate possible.
\end{itemize}
\bigskip

\figureLayoutETS{PinholeCameraCalibration}{%
  \subfigureETS[0.4]{PinholeCameraCalibration_1.jpg}{Sélection de la première webcam disponible (identifiant \code{0} dans le champ \code{WebcamId}).}%
  \figurehspace%
  \subfigureETS[0.4]{PinholeCameraCalibration_2.jpg}{Scripts d'étalonnage.}%
}{
  Configuration de la scène Unity \code{CalibrateCamera.unity} pour étalonner une caméra monoscopique rectilinéaire.
}

On prépare ensuite la scène de calibration \reffigureETSp{PinholeCameraCalibration}:
\begin{enumerate}
  \item On ouvre la scène Unity \code{Assets/ArucoUnity/Scenes/CalibrateCamera.unity} déjà préparée pour l'étalonnage d'une caméra monoscopique rectilinéaire.
  \item Sur le script \code{ArucoWebcam}, dans l'objet du même nom, on choisit la caméra à étalonner en renseignant son identifiant numérique dans le champ \code{WebcamId}.
  \item On crée à nouveau un objet représentant notre échiquier imprimé, en y configurant cette fois la taille des carrés en mètres, permettant à Aruco Unity de construire automatiquement les positions relatives à l'échiquer des points $P_i$.
  \item Sur le script \code{PinholeCameraCalibration}, dans l'objet du même nom, on fait pointer le champ \code{CalibrationBoard} vers notre objet d'échiquier.
  \item On peut ajuster les paramètres de calibration (\texten{calibration flags} en anglais) du module calib3d avec le script \code{PinholeCameraCalibrationFlags} et les paramètres de détection du module aruco avec le script \code{DectectorParametersController}, ces deux scripts ayant par défaut les valeurs recommandées par les documentations de leurs modules respectifs.
\end{enumerate}

\figureETS[0.75]{PinholeCameraCalibration_3.jpg}{%
  Scène \code{CalibrateCamera.unity} jouée pour étalonner une webcam. Les marqueurs de l'échiquier sont surlignés pour indiquer s'il est correctement détecté.
}

On peut maintenant débuter l'étalonnage en jouant la scène \reffigureETSp{PinholeCameraCalibration_3.jpg}. Le flux vidéo de la caméra est affiché alors dans Unity ainsi qu'une IHM : le bouton \code{Add Image} permet de capturer l'image courante dans une liste, le bouton \code{Reset} permet de vider cette liste et le bouton \code{Calibrate} permet d'étalonner la caméra en utilisant cette liste d'images. Il est important de garder la caméra dans une position fixe durant l'étalonnage et de varier seulement la position et l'orientation de l'échiquier entre les images. On désactive aussi la mise au point automatique de la caméra, ce paramètre impactant en effet la matrice intrinsèque	$K$ de la caméra et les distorsions de l'objectif. Enfin, l'échiquier est surligné dans le flux vidéo pour aider à le positionner.

\begin{listingETS}{cs}{lst:aruco_camera_calibration}{Classe C\# ArucoCameraCalibration simplifiée.}
  public abstract class ArucoCameraCalibration : ArucoCameraController, IArucoCameraCalibration, IHasCameraParameters
  {
    public Board Board; // L'échiquier
    public CameraParameters CameraParameters;
    public double Error;
    public var ObjectPoints = new VectorVectorPoint3f(); // Points P
    public var ImagePoints = new VectorVectorPoint2f(); // Points p

    // (1) Détection des points P et p
    protected void AddImages()
    {
      VectorInt ids; // Identifiants des marqueurs
      VectorVectorPoint2f corners; // Coins des marqueurs
      DetectMarkers(ArucoCamera.Image, Board.Dictionary, out corners, out ids, DetectorParameters, out rejectedCandidateCorners);
      
      VectorPoint3f P;
      VectorPoint2f p;
      GetBoardObjectAndImagePoints(Board, corners, ids, out P, out p);
      ObjectPoints.PushBack(P);
      ImagePoints.PushBack(p);
    }

    public abstract void Calibrate(); // (2) Étalonnage
  }
\end{listingETS}

\begin{listingETS}{cs}{lst:pinhole_camera_calibration}{Classe C\# PinholeCameraCalibration simplifiée.}
  public class PinholeCameraCalibration : ArucoCameraCalibration
  {
    public PinholeCameraCalibrationFlags Flags;

    public virtual void Calibrate()
    {
      VectorMat t, r; // Position et rotation de l'échiquier à chaque image
      Error = CalibrateCamera(ObjectPoints, ImagePoints, ArucoCamera.Resolution, CameraParameters.K, CameraParameters.D, out r, out t, Flags);
      CameraParameters.Save();
    }
  }
\end{listingETS}

Nos scripts d'étalonnages sont simples : (1) on détecte l'échiquier dans et on extrait les points $P_i$ et $p_i$ pour chaque image de la liste \autorefp{lst:aruco_camera_calibration}, (2) on calcule la matrice intrinsèque $K$ et les paramètres de distorsion $D$. Un script spécifique par type de caméra implémente cette seconde étape, par exemple l'\autoref{lst:pinhole_camera_calibration} pour une caméra monoscopique rectilinéaire.

Une fois l'étalonnage effectué, un score mesurant sa qualité est alors affiché. Il correspond à la somme de l'erreur de reprojection pour chaque image de la liste : c'est la distance en pixels entre les points $p_i$ observés et la projection simulée des points $P_i$ sur le plan image avec le modèle de sténopé mesuré par l'étalonnage. Ce score doit être le plus proche possible de zéro si l'on souhaite un bon alignement des deux caméras : un décalage trop important dans l'alignement des éléments vrituels sur l'image de la caméra physique briserait l'illusion de la RA. Enfin, le résultat de l'étalonnage est enregistré dans un fichier XML dans le dossier \code{Assets/ArucoUnity/CameraParameters/} que l'on pourra utiliser à volonté dans n'importe quelle application de RA utilisant cette caméra. L'\hyperref[appendix:ovrvision_camera_parameters]{Annexe~\ref{appendix:ovrvision_camera_parameters}} présente le fichier de l'étalonnage de l'Ovrvision Pro.

\subsection{Correction des image de la caméra}
\label{subsec:pinhole_camera_undistortion}

\figureETS[1]{TrackMarkersCamera.jpg}{%
  Configuration de la correction d'une caméra monoscopique rectilinéaire (\code{PinholeCameraUndistortion}) et de l'alignement d'une caméra virtuelle (\code{MonoArucoCameraDisplay}) pour permettre de la RA.
}

\begin{listingETS}{cs}{lst:aruco_camera_undistortion}{Classe C\# ArucoCameraUndistortion simplifiée.}
  public class ArucoCameraUndistortion : ArucoCameraController, IArucoCameraUndistortion, IHasCameraParameters
  {
    public CameraParameters CameraParameters;
    public Mat Knew;
    public float Fx, Fy;
    public Mat noD = new Mat(); // Aucune distortions
    protected Mat rectMapX, rectMapY;

    protected void Start()
    {
      InitializeUndistortion();
      Fx = (float)Knew.AtDouble(0,0), Fy = (float)Knew.AtDouble(1,1);
      ArucoCamera.UndistortImages += Undistort;
    }

    protected abstract void InitializeUndistortion();

    protected void Undistort() // (3) Correction des images
    {
      Remap(ArucoCamera.Image, ArucoCamera.Image, rectMapX, rectMapY, InterpolationFlags.Linear);
    }
  }
\end{listingETS}

\begin{listingETS}{cs}{lst:pinhole_camera_undistortion}{Classe C\# PinholeCameraUndistortion simplifiée.}
  public class PinholeCameraUndistortion : ArucoCameraUndistortion
  {
    public float Alpha;

    protected override void InitializeUndistortion()
    {
      // (1) Matrice intrinsèque de la caméra corrigée
      Knew = GetOptimalCameraMatrix(CameraParameters.K, CameraParameters.D, ArucoCamera.Resolution, Alpha);

      // (2) Fonction de mappage de correction
      var noR = new Mat(); // Aucune rotation de la caméra corrigée
      InitUndistortRectifyMap(CameraParameters.K, CameraParameters.D, noR, newK, ArucoCamera.Resolution, Type.CV_16SC2, out rectMapX, out rectMapY);
    }
  }
\end{listingETS}

Une fois la caméra étalonnée, nous pouvons y intégrer la RA. Nous devons donc tout d'abord corriger les images de la caméra physique. Cette opération se fait en deux temps : on calcule d'abord une fonction de mappage $m$ entre les pixels de l'image corrigée $I_c$ et ceux l'image originale $I$, selon l'\autoref{eq:undistortion}, que l'on pourra ensuite appliquer sur chacune des images.

\begin{equation}
  \label{eq:undistortion}
  I_c(x,y) = I(m_x(x,y), m_y(x,y))
\end{equation}

Cette opération est intéressante, car elle nous permet donc de choisir une autre matrice intrinsèque, comme si les images corrigées avaient été capturées par une autre caméra avec un autre plan image. Cela nous permet de replacer le centre optique au centre de l'image soit $c_x = \sfrac{d_x}{2}$ et $c_y = \sfrac{d_y}{2}$. Nous pourrions également changer la position et l'orientation de cette caméra corrigée de cette manière.

Aruco Unity effectue automatiquement la correction des images, lors du suivi de marqueurs, avec le script \code{ArucoCameraUndistortion}. On le met en place facilement \reffigureETSp{TrackMarkersCamera.jpg} :
\begin{enumerate}
  \item On ouvre la scène \code{Assets/ArucoUnity/Scenes/TrackMarkers.unity} préparée pour le suivi de marqueurs avec une caméra monoscopique rectilinéaire.
  \item Dans l'objet \code{ArucoWebcam}, sur le script \code{ArucoCameraParametersController}, on renseigne le nom du fichier d'étalonnage dans le champ \code{CameraParametersFilename}.
  \item Le script \code{ArucoCameraUndistortion} est déjà configuré pour utiliser la caméra physique et le fichier d'étalonnage.
\end{enumerate}
\bigskip

Dans les scripts de correction, on calcule tout d'abord (1) la nouvelle matrice intrinsèque et (2) la fonction de mappage \autorefp{lst:pinhole_camera_undistortion} puis (3) on l'applique sur chaque image capturée par la caméra \autorefp{lst:aruco_camera_undistortion}.

Cependant, lors de l'application de la fonction de mappage, il y a une ambiguïté à résoudre. La fonction $m$ retourne des nombres décimaux alors que les coordonnées des pixels d'une image sont des entiers : il n'existe donc pas toujours une correspondance exacte entre les pixels des deux images. On choisit alors une méthode d'interpolation pour résoudre ce problème : l'interpolation bilinéaire donne de bons résultats sans trop augmenter le temps de calcul. En outre, certains pixels de l'image corrigée peuvent n'avoir aucune correspondance : on les colore en noir dans ce cas. Avec le paramètre $\alpha$ \reffigureETSp{PinholeCameraUndistortionAlpha.png}, l'utilisateur peut décider de n'afficher que les pixels de l'image corrigée ayant une correspondance ($\alpha = 0$) ou de conserver tous les pixels de l'image originale, introduisant de nombreux pixels noirs ($\alpha = 1$). Nous conseillons toutefois ce dernier cas pour conserver les valeurs de longueur focale non-corrigées et éviter un effet de \textquote{zoom} non voulu.

\figureETS[0.4]{PinholeCameraUndistortionAlpha.png}{%
  Image corrigée avec en surimpression la zone sélectionnée suivant la valeur du paramètre $\alpha$ : seulement les pixels valides dans l'image corrigée avec $\alpha=0$ ou tous les pixels transformés de l'image originale $\alpha=0$.
}

\subsection{Alignement de la caméra virtuelle}
\label{subsec:pinhole_camera_display}

\begin{listingETS}{cs}{lst:aruco_camera_display}{Classe C\# ArucoCameraDisplay simplifiée.}
  public abstract class ArucoCameraDisplay : ArucoCameraController, IArucoCameraDisplay
  {
    public Camera Camera; // Caméra virtuelle d'Unity
    public ArucoCameraUndistortion Undistortion;
    protected float cameraFov;
    protected const int bgDistance = 1000;

    protected override void Start()
    {
      // (1) Configuration de la caméra virtuelle
      Camera = new GameObject().AddComponent<Camera>();
      Camera.transform.SetParent(this.transform.parent);
      Camera.transform.localPosition = Vector3.zero;
      Camera.transform.localRotation = Quaternion.identity;
      Camera.fieldOfView = cameraFov;

      // (2) Configuration de l'arrière-plan
      var bg = GameObject.CreatePrimitive(PrimitiveType.Quad);
      bg.transform.SetParent(this.transform.parent);
      bg.transform.localPosition = new Vector3(0, 0, bgDistance);
      bg.transform.LookAt(camera.transform);
      bg.transform.localScale = new Vector3(
        ArucoCamera.Resolution.Width / Undistortion.Fx * bgDistance,
        ArucoCamera.Resolution.Height / Undistortion.Fy * bgDistance, 1);
      bg.material.mainTexture = ArucoCamera.Texture;
    }

    public void PlaceArucoObject(ArucoObject obj, Vec3d pos, Vec3d rot)
    {
      obj.gameObject.SetActive(true);
      obj.transform.SetParent(Camera.transform);
      obj.transform.localPosition = new Vector3(pos);
      obj.transform.localRotation = new Quaternion(rot);
    }
  }
\end{listingETS}

\begin{listingETS}{cs}{lst:mono_aruco_camera_display}{Classe C\# MonoArucoCameraDisplay simplifiée.}
  public class MonoArucoCameraDisplay : ArucoCameraDisplay
  {
    protected override void Start()
    {
      cameraFov = 2f * Mathf.Atan(0.5f * ArucoCamera.Resolution.Height / fy) * Mathf.Rad2Deg;
      base.Start();
    }
  }
\end{listingETS}

Nous pouvons maintenant configurer la caméra virtuelle en l'alignant avec la caméra physique corrigée. La matrice intrinsèque associée avec les images corrigées pouvant être modifiée par la correction, l'alignement doit donc se faire après.

Comme pour la correction, Aruco Unity aligne automatiquement la caméra virtuelle lors du suivi de marqueurs. En reprenant la scène de la \autoref{subsec:pinhole_camera_undistortion}, on voit le script déjà configuré pour utiliser la caméra physique \reffigureETSp{TrackMarkersCamera.jpg}.

On crée alors dans les scripts : (1) une caméra virtuelle dans Unity, placée à l'origine, avec un champ de vision calculé \autorefp{eq:fov} depuis la matrice intrinsèque corrigée, puis (2) l'arrière-plan sous forme d'un rectangle face à la caméra virtuelle, le long de son axe optique (nous avons corrigé le centre optique) à une distance arbitraire lontaine pour éviter une intersection avec les éléments virtuels \autorefp{lst:aruco_camera_display}. Enfin, on calcule la taille de ce rectangle en utilisant les triangles semblables : $d_x' = \text{distance} \times \sfrac{d_x}{f_x}$ \reffigureETSp{Fov.png}.

\subsection{Réalité augmentée par suivi de marqueur}
\label{subsec:pinhole_camera_tracker}

\figureETS[1]{TrackMarkersMarker.jpg}{%
  Configuration d'un marqueur mesurant \SI{5.4}{\cm} avec un modèle de théière comme élément enfant.
}

\figureETS[1]{TrackMarkersTracker.jpg}{%
  Configuration du suivi de marqueurs avec une liste de trois objets : deux échiquiers et du marqueur de la \reffigureETS{TrackMarkersMarker.jpg}. La caméra associée est surlignée en jaune.
}

\figureLayoutETS{TrackMarkersDemo}{%
  \subfigureETS[0.15]{TrackMarkersDemo_1.jpg}{ }%
  \figurehspace%
  \subfigureETS[0.15]{TrackMarkersDemo_2.jpg}{ }%
  \figurehspace%
  \subfigureETS[0.15]{TrackMarkersDemo_3.jpg}{ }%
}{
  Suivi du marqueur configuré à la \reffigureETS{TrackMarkersMarker.jpg} dans plusieurs angles de vues.
}

Nous pouvons utiliser le suivi de marqueur du module aruco pour savoir où placer nos éléments virtuels. Aruco Unity permet un suivi de marqueurs facile, à configurer directement dans l'éditeur d'Unity :
\begin{enumerate}
  \item On crée tout d'abord des marqueurs comme nous l'avons vu dans la \autoref{subsec:pinhole_camera_calibration} qu'on imprime et place dans l'environnement physique.
  \item On reprends la scène configurée aux sections \ref{subsec:pinhole_camera_undistortion} et \ref{subsec:pinhole_camera_display}.
  \item On y crée à nouveau tous marqueurs suivis en y les configurants avec les valeurs en mètres de ceux imprimés, et on y ajoute les éléments que l'on souhaite afficher en RA comme enfants de ces marqueurs \reffigureETSp{TrackMarkersMarker.jpg}.
  \item Dans le script \code{ArucoObjectsTracker} sur l'objet du même nom, on dépose les marqueurs virtuels dans la liste \code{ArucoObjects} \reffigureETSp{TrackMarkersTracker.jpg}.
  \item En jouant la scène, les marqueurs virtuels sont placés aux même positions et orientations que les marqueurs physiques détectés \reffigureETSp{TrackMarkersDemo}.
\end{enumerate}

\begin{listingETS}{cs}{lst:aruco_camera_tracker}{Classe C\# ArucoObjectsTracker simplifiée pour le suivi des marqueurs seuls.}
  public class ArucoObjectsTracker : ArucoCameraController, IArucoObjectsTracker
  {
    public Dictionary<int, ArucoMarker> Markers; // Format: <id, marker>
    public ArucoCameraDisplay Display;

    private List<Aruco.Dictionary> dictionaries;
    private int length = 1; // Valeur fictive pour l'estimation

    protected void Start()
    {
      dictionaries = Markers.Select(m => m.Value.Dictionary).ToList();
      ArucoCamera.ImagesUpdated += TrackArucoObjects;
    }

    protected void TrackArucoObjects()
    {
      // (1) Masquer les éléments
      foreach (var marker in Markers.Values) { marker.gameObject.SetActive(false); }

      foreach (var dic in dictionaries)
      {
        // (2) Détection des marqueurs physiques sur l'image
        VectorVectorPoint2f corners; // Points p
        VectorInt ids; // Identifiants des marqueurs
        DetectMarkers(ArucoCamera.Image, dic, out corners, out ids);
        
        // (3) Estimation de la positions des marqueurs détectés
        VectorVec3d pos, rot;
        EstimatePoseSingleMarkers(corners, length, Display.Undistort.Knew, Display.Undistort.noD, rot, pos);

        // (4) Placement des marqueurs virtuels détectés
        for (i = 0; i < ids.Size(); i++)
        {
          if (markers.TryGetValue(ids[i], out marker))
          {
            var correctedPos = pos[i] * marker.Length / length;
            Display.PlaceArucoObject(marker, correctedPos, rot[i]);
          }
        }
      }
    }
  }
\end{listingETS}

On montre dans l'\autoref{lst:aruco_camera_tracker} l'utilisation du module aruco pour faire le suivi des marqueurs seuls. Le suivi des échiquiers fonctionne sur le même principe, avec des fonctions spécifiques de détection et d'estimation de position. Il fonctionne en quatre étapes : (1) on cache tous les marqueurs virtuels, (2) on détecte les marqueurs physiques sur l'image (extraction des points $p$ comme durant l'étalonnage à la \autoref{subsec:pinhole_camera_calibration}), (3) on détermine la position et l'orientation des marqueurs physiques détectés par rapport à la caméra physique et enfin (4) on place les marqueurs virtuels configurés dans ces mêmes positions et orientations par rapport à la caméra virtuelle.

\figureETS[0.6]{Garrido-Jurado2014.jpg}{%
  Procédé de détection des marqueurs du module aruco, utilisé avec un dictionnaire de marqueurs 5x5.\\
  Tiré de \cite[Figure 5]{Garrido-Jurado2014}.
}

Comme les fonctions aruco détectent tous les marqueurs physiques sur l'image, nous devons faire une correspondance avec les marqueurs virtuels configurés. Le procédé de détection est simple et rapide à effectuer \reffigureETSp{Garrido-Jurado2014.jpg} : (a) l'image est d'abord (b) segmentée pour permettre de (c) détecter les contours puis (d) d'extraire les polygones comme marqueurs potentiels. La (e) perspective est ensuite supprimée par reprojection en utilisant la matrice intrinsèque et (f) le code du marqueur est extrait. Chaque marqueur étant composé d'une grille de taille $n \times n$ de cellules noires ou blanches, entourée d'une bordure noire, il peut être facilement identifié par le code formé par les cellules. Ces identifiants sont connus d'avance, car la génération des marqueurs se fait avec un dictionnaire que l'on fournit à l'algorithme de détection, par exemple un dictionnaire de 50 marqueurs 4x4 dans la \reffigureETS{TrackMarkersMarker.jpg} ou 5x5 dans la \reffigureETS{Garrido-Jurado2014.jpg}. On fait alors la correspondance entre les marqueurs détectés et ceux configurés en utilisant ces identifiants.

\figureETS[0.6]{Bradski2008_3.jpg}{%
  Un échiquier projeté sur le plan image d'une caméra. Les lignes construisant la projection montrent que de multiples échiquiers pourraient produite le même résultat sur le plan image.\\
  Tiré de \cite[Figure 11-11]{Bradski2008}.
}

Enfin, la position des marqueurs détectés est estimée avec une méthode de résolution au problème \texten{perspective-n-points}, en utilisant la fonction \code{solvePnP} d'OpenCV (\url{https://github.com/opencv/opencv/blob/master/modules/calib3d/src/solvepnp.cpp}). La projection perspective faisant \textquote{perdre} une dimension, il existe de multiples solutions à la position d'un marqueur détecté : cela peut être l'image d'un petit marqueur proche de la caméra comme celle d'un grand marqueur loin de la caméra \reffigureETSp{Bradski2008_3.jpg}. C'est pourquoi on doit fournir à l'algorithme la taille du marqueur, donc la longueur d'un côté, car c'est un carré. La fonction d'estimation de pose travaille en une fois avec tous les marqueurs détectés, en ne prenant qu'une seule longueur de côté. Comme l'on souhaite utiliser des marqueurs de tailles différentes, on fournit une longueur fictive pour ensuite corriger les positions calculées avec la vraie longueur des différents marqueurs \autorefp[(4)]{lst:aruco_camera_tracker}. Ce \texten{hack} (\textquote{triche}) nous évite de nombreux appels inutiles et coûteux à cette fonction.


\subsection{Alignement avec une caméra stéréoscopique avec objectifs \texten{fisheye}}
\label{subsec:ovrvision_camera_calibration}

\subsubsection{Description}

\figureETS[0.45]{Mei2007.jpg}{%
  Illustration du modèle de \cite{Mei2007} : un point $X$ de la scène est d'abord projeté en un point $X_s$ sur une sphère puis projeté à nouveau en perspective rectilinéaire en un point $p$ sur le plan image.\\
  Tiré de \cite[Figure 4]{Mei2007}%
}

\figureETS[1]{StereoProjection.png}{%
  Modèle d'une caméra stéréoscopique rectilinéaire idéale : les deux plans images sont coplanaires, distants horizontalement, et les deux couples objectif-plan image ont les mêmes paramètres intrinsèques ($f_g = f_d$). Les objectifs pourraient également être placés verticalement.
}

Les objectifs utilisés sur l'Ovrvision Pro étant \texten{fisheye}, le modèle du sténopé ne s'y applique pas. En effet, contrairement à la perspective de l'\oe il humain ou des objectifs rectilinéaires, ces objectifs utilisent une distorsion en barillet volontairement importante pour capter un très grand champ de vision jusqu'à \ang{180}.

Nous utilisons alors le modèle de \cite{Mei2007} utilisable pour les objectifs \texten{fisheye} et implémenté par le module ccalib d'OpenCV. La caméra y est modélisée sur le principe suivant : la scène est d'abord projetée sur une sphère, puis une projection perspective rectilinéaire de la sphère est faite sur le plan image \reffigureETSp{Mei2007.jpg}. Comme pour une caméra avec un objectif rectilinéaire, nous corrigeons ces distorsions : cela nous permet de reprojetter les images capturées en perspective rectilinéaire pour pouvoir les aligner avec la caméra virtuelle.

\figureETS[0.5]{Bradski2008_4.jpg}{%
  L'étalonnage d'une caméra stéréoscopique mesure deux paramètres supplémentaire : les vecteurs de translation $T$ et de rotation $R$ du plan droit ($\Pi_l$) par rapport au plan image gauche ($\Pi_r$).\\
  Tiré de \cite[Figure 12.9]{Bradski2008}.
}

En outre, l'Ovrvision étant une caméra stéréoscopique nous avons besoin de déterminer sa géométrie. Sur une version idéale, les caméras gauche et droite auraient les mêmes paramètres intrinsèques et leurs plans images respectifs seraient coplanaires (donc leurs axes optiques parallèles) simplement séparés horizontalement de quelques centimètres \reffigureETSp{StereoProjection.png}. Or nous avons vu que l'utilisation d'un objectif entraîne des distorsions radiales et tangentielles. À l'étalonnage, les caméras gauches et droites ont donc des paramètres intrinsèques légèrement différents et, leurs plans images n'étant pas coplanaires, on mesure deux nouveaux paramètres : les vecteurs de translation $T$ et de rotation $R$ du plan image droit par rapport au plan image gauche \reffigureETSp{Bradski2008_4.jpg}.

En plus de la correction de la caméra, nous devons la \emph{rectifier} pour rendre à nouveau coplanaires les deux plans images. La correction étant faite en premier, le type d'objectif n'a pas d'impact sur la rectification. Comme pour les distorsions, nous n'avons pas besoin de comprendre la procédure de rectification ; pour plus de détails, nous redirigeons le lecteur vers \cite[p. 419]{Bradski2008}.

\subsubsection{Application dans Aruco Unity}

\figureLayoutETS{OvrvisionCalibration}{%
  \subfigureETS[0.4]{OvrvisionCalibration_1.jpg}{Scripts pour la caméra.}%
  \figurehspace%
  \subfigureETS[0.4]{OvrvisionCalibration_2.jpg}{Scripts pour l'étalonnage.}%
}{
  Configuration de l'étalonnage dans Unity de la caméra Ovrvision.
}

\figureETS[0.75]{OvrvisionCalibration_3.jpg}{%
  Étalonnage de la caméra Ovrvision.
}

\figureETS[1]{OvrvisionTracking_1.jpg}{%
  Configuration de la correction et la rectification de la caméra Ovrvision et de l'alignement d'une caméra virtuelle pour intégrer la RA dans notre visiocasque.
}

\figureETS[1]{OvrvisionTracking_2.jpg}{%
  Vues des yeux gauche et droit dans notre visiocasque lors du suivi d'un marqueur.
}

\begin{listingETS}{cs}{lst:stereo_omnidir_camera_undistortion}{Classe C\# StereoOmnidirCameraUndistortion simplifiée.}
  public class StereoOmnidirCameraUndistortion : ArucoCameraUndistortion
  {
    public float[] Fovs;

    protected override void InitializeUndistortion()
    {
      // (1) Matrices intrinsèques pour la correction
      var newKs = new Mat[ArucoCamera.Images.Length]
      for (int i = 0; i < newKs.Length; i++)
      {
        float f = ArucoCamera.Resolutions[i].Height / (2f * Mathf.Tan(0.5f * Fovs[i] * Mathf.Deg2Rad));
        newKs[i] = new Mat(3, 3, Type.CV_64F, new double[9] {
          f, 0, ArucoCamera.Resolution[i].Width / 2,
          0, f, ArucoCamera.Resolution[i].Height / 2,
          0, 0, 1});
      }

      // (2) Rotations pour la rectification
      var R = new Mat[ArucoCamera.Images.Length];
      Omnidir.StereoRectify(CameraParameters.R, CameraParameters.T, out R[0], out R[1]);

      // (3) Fonction de mappage
      for (int i = 0; i < newKs.Length; i++)
      {
        Omnidir.InitUndistortRectifyMap(CameraParameters.K[i], CameraParameters.D[i], CameraParameters.Xi[i], R[i], newK[i], ArucoCamera.Resolutions[i], Type.CV_16SC2, out rectMapX[i], out rectMapY[i], Omnidir.Rectifify.Perspective);
      }
    }
  }
\end{listingETS}

\begin{listingETS}{cs}{lst:stereo_vr_aruco_camera_display}{Classe C\# StereoVRArucoCameraDisplay simplifiée.}
  public class StereoVRArucoCameraDisplay : ArucoCameraDisplay
  {
    protected override void Start()
    {
      // (1) Configuration des deux caméras virtuelles
      cameraFov[0] = Undistort.Fovs[0], cameraFov[1] = Undistort.Fovs[1];
      base.Start();

      // (2) Chaque caméra rend pour un oeil dans le visiocasque
      Cameras[0].stereoTargetEye = StereoTargetEyeMask.Left;
      Cameras[1].stereoTargetEye = StereoTargetEyeMask.Right;

      // (3) Placement des deux caméras
      Cameras[1].transform.localPosition += Undistort.CameraParameters.T;
    }
  }
\end{listingETS}

Tous les procédés avec les caméras rectilinéaires que nous venons de décrire restent les mêmes \autorefp{subsec:ar_hmd_procedure}. En revanche, nous utilisons quelques algorithmes différents, en créant simplement de nouveaux scripts spécifique d'étalonnage de correction t d'alignement de caméra. Comme la correction nous permet de travailler avec une caméra corrigée rectilinéaire, le script suivi de marqueurs reste inchangé. Nous avons cependant mis à jour tous les scripts vu ci-dessus pour qu'ils supportent des caméras capturant de multiples images, par exemple en remplaçant \code{Mat ArucoCamera.Image} par \code{Mat[] ArucoCamera.Image} initialisé au démarrage de l'application : tous les algorithmes que nous avons décrits sont alors exécutés pour chaque image capturée.

Pour l'étalonnage, la procédure à suivre est la même que dans la \autoref{subsec:pinhole_camera_calibration} à la différence que nous travaillons avec les objets \code{OvrvisionArucoCamera} pour la caméra et \code{StereoOmnidirCameraCalibration} pour l'étalonnage que nous glissons dans la scène depuis le dossier \code{Assets/ArucoUnity/Prefabs/} et nous désactivons \code{PinholeCameraCalibration} \reffigureETSp{OvrvisionCalibration}. Ce nouveau script d'étalonnage que nous avons crée est très similaire à \autoref{lst:pinhole_camera_calibration} : nous y avons seulement remplacé la fonction d'étalonnage par \code{Cv.Omnidir.StereoCalibrate}. Cette fonction étalonne chaque caméra individuellement, puis calcule la position et l'orientation relative des deux plans images, et retourne tous ces paramètres que l'on enregistre dans un fichier d'étalonnage.

On met finalement en place la RA dans notre visiocasque. Comme pour l'étalonnage, la procédure est la même que dans les sous-sections \ref{subsec:pinhole_camera_undistortion} et \ref{subsec:pinhole_camera_display}, mais on utilise des objets différents : on désactive donc les objets de correction et d'affichage pour la webcam et on glisse dans la scène ceux pour une caméra stéréoscopique \texten{fisheye} ainsi que \code{OvrvisionArucoCamera} \reffigureETSp{OvrvisionTracking_1.jpg}. Contrairement à la correction des caméras rectilinéaires, les fonctions du module omnidir nous permettent de choisir directement le champ de vision des caméras gauches et droites corrigées. On les définit toutes les deux à un champ de vision vertical de \SI{105}{\deg}, comme le visiocasque de RV que nous nous utilisons. Les marqueurs et leur suivi une fois configurés, notre visiocasque fonctionne comme nous l'avons décrit à la \autoref{subsec:prototype_operation} \reffigureETSp{OvrvisionTracking_2.jpg} !

Nous effectuons la correction et la rectification dans le même temps \autorefp{lst:stereo_omnidir_camera_undistortion} : nous (1) créons les matrices intrinsèques pour les caméras corrigées rectilinéaires à partir des champs de visions choisis, puis on calcule (2) la rotation des deux plans images pour les rectifier, enfin (3) la fonction de mappage pour chaque caméra. Ces fonctions sont toujours appliquées sur toutes les images capturées dans le script parent \autorefp{lst:aruco_camera_undistortion}.

Pour la configuration des caméras virtuelles, nous devons tout d'abord activer la RV dans Unity avec l'option \code{Virtual Reality Supported} dans le menu \code{Edit/Project Settings/Player/XR Settings}. Nous (1) créons et configurons ensuite deux caméras virtuelles, une pour chaque \oe il avec les champs de visions choisis. Puis nous (2) indiquons à Unity pour quel \oe il chaque caméra rend. Enfin, on (3) décale la caméra droite suivant le vecteur $T$.