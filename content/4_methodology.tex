\chapter{Conception d'un visiocasque de RA à large champs de vision}
\label{ch:methodology}

\section{Motivations et procédure}
Pour exploiter notre concept détaillé au \autoref{ch:concept}, nous avions besoin d'un visiocasque à large champs de vision supérieur à \ang{90} horizontalement \reffigureETSp{AngleOfView.png}. En effet, un plus petit champs de vision aurait limité l'intérêt d'étendre l'écran tactile du téléphone : l'écran étendu n'aurait pas été visible dans sa totalité. Cependant, il n'existe actuellement aucun visiocasque de RA sur le marché avec un aussi grand champs de vision \citep{Millette2016}. Le second sous-problème de ce travail de recherche a donc été de développer un visiocasque de RA à large champs de vision.

\figureETS{AngleOfView.png}{
  Illustration du champs de vision d'une caméra, qui peut être mesuré en degrés horizontalement, verticalement ou en diagonale.\\
  Tiré de \cite{DWC2006}
}

Un visiocasque de RA vidéo étant plus simple à concevoir qu'un casque optique, nous avons donc décidé de reproduire l'AR-Rift \citep{Steptoe2013}. C'est un prototype de conception simple qui a fait ses preuves notamment utilisé par \cite{Steptoe2014} et \cite{Piumsomboon2014}.
Cependant, si dans sa série d'articles en ligne, \citeauthor{Steptoe2013} détaille correctement la procédure générale de conception de l'AR-Rift, la sélection des caméras, leur montage et le principe d'alignement de la caméra physique avec la caméra virtuelle, il ne détaille pas la procédure de calibration et rectification de la caméra physique, ni ne propose pas de solution accessible de détection des objets réels. Nous souhaitons ici détailler un peu plus ces étapes.

La procédure suivie a été la suivante :
\begin{enumerate}
  \item sélection d'une caméra stéréo, d'un visiocasque de RV et d'un moteur 3D ;
  \item calibration et rectification de la caméra stéréo, et alignement des contenus réels et virtuels ;
  \item réalisation de la librairie de réalité augmentée ArucoUnity
\end{enumerate}
La figure \reffigureETS{ARRift.jpg} montre le visiocasque conçu.

\figureETS{ARRift.jpg}{
  Notre prototype de visiocasque de RA, basé sur le concept de l'AR-Rift de \cite{Steptoe2013} : il est composé du visiocasque de RV Oculus DK2, de la caméra stéréoscopique Ovrvision et du capteur de reconnaissance des mains Leap Motion.
}


\section{Choix techniques}
\subsection{Choix matériels}
- Principe : AR video see-through avec un casque VR diffusant des caméras stereo fisheye
  - Avantages/inconvénients par rapport à l'optical see through : « While optical see-through AR is an attractive ideal, in practice it is very difficult to achieve accurate registration with optical see-through AR systems. (Registration is the alignment of the virtual objects shown in the display and their real-world referent). Most of these limitations come from the properties of the display itself, not the calibration procedures. » : \url{https://www.artoolkit.org/documentation/doku.php?id=8_Advanced_Topics:config_optical_see-through}
- Choix techniques
  - Casque de VR (permet tracking)
  - Caméras : même fov (donc fisheye), même résolution + stéréo. Choix ovrvision
  - Leap Motion
  - Téléphone Android

On pourrait choisir d'utiliser simplement une seule caméra et afficher son image pour chaque oeil. Cependant, comme le rappelle \citep{Bourke1999}, la plupart des personnes utilise la disparité binoculaire pour voir en profondeur. En effet, chaque oeil perçoit une image légèrement différente de l'autre, car ils sont séparés horizontalement par une distance appelé écart pupillaire. On a donc tout intérêt à utiliser une caméra stéréo.

\subsection{Choix logiciels}
- Unity : Unity3d utilisé pour gestion du oculus et 3d top. Plus simple à prendre en main que Unreal, mais plus rigide (framework boîte noire, difficile de sortir sentiers battus)
- OpenCV
  - ArUco
    - Citer Mobile Marker-based Augmented Reality as an Intuitive Instruction Manual HANNAH REUTERDAHL
    - Pour la liste des markers et frameworks : dire ce qui est compatible Unity ou c++ mais aucun ne permet de travailler avec caméra - fisheye : d'où aruco unity
  - Calibration
- Unity networking
- Leap Motion

Quand choix plateforme, citer Billinghurst 2015 pour dire qu'il y a eu pas mal d'efforts avec ARToolKit, Studierstud, osgArt ou GoblinXNA : mais vieillots. On a besoin du support de la VR, et soit Unity soit Unreal. Choix de Unity car facile et déjà utilisé dans labo et très bon pour la VR.
On pouvait utiliser le nouveau ARToolKit, ou Vuforia. Mais besoin d'avoir le support de caméra stéréo. Or ces libs sont faites pour des webcams de pc ou de téléphone.


\section{Calibration, rectification et alignement des caméras}
Un visiocasque de RA par vidéo fonctionne sur le principe suivant :
\begin{enumerate}
  \item une caméra filme l'environnement réel de l'utilisateur ;
  \item un logiciel de rendu 3D va ajouter du contenu virtuel par dessus l'image de cette caméra ;
  \item l'image
\end{enumerate}

Ainsi, pour que l

Comme le rappelle \cite[p. ]{Billinghurst2015}


- Voir AdrianKaehler2017 - Learning OpenCV 3
  - Chapitre 11 + OpenCV3.3.0-Calib3dModule + notes 2016-11-28 pour le modèle pinhole de caméra + calibration
  - Chapitre 19 pour la calibration stereo + LeapMotionAlignmentCameraAR2015 pour expliquer pourquoi on applique aux caméras virtuelles l'ICD et non l'IPD
  - OpenCV3.3.0-CcalibModule (car module fisheye buggé et citer le papier qui dit que le modèle de caméra omnidir s'appliquer aussi aux caméras fisheye) pour la calibration fisheye
- Voir BuJo p.150 + AR-Rift PArt 5 pour les équations de configuration de la caméra virtuelle et du placement du background pour qu'il soit aligné avec le contenu 3D filmé par la caméra virtuelle
- Conseils/notes calibrations :
  - Utiliser une board la plus plate possible (attention à l'humidité de l'air qui est absorbée par le papier)
  - Utiliser une bonne lumière pour que la board soit bien détectée et sans reflets
  - Désactiver l'autofocus de la caméra : une calibration se fait pour une focale fixe (les distorsions restent les même mais pas la camera matrix)
  - La caméra ou la board doit rester fixe pendant la calibration
  - Prendre des dizaines de captures remplissant uniformément l'espace de capture de la caméra en variant les angles de capture
  - L'objectif est d'avoir une erreur de reprojection inférieure à 1 pixel
  - OpenCV est système main droite dans son système de coordonnées (\url{http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT9/img4.gif}) alors qu'Unity est système main gauche : il suffit d'inverser l'axe des Y (\url{https://answers.unity.com/storage/temp/8053-spaces.jpg}) : faire un petit graphe comme la 2e image
  - OpenCV encode sa rotation dans un vecteur dont les coordonnées normalisées donnent l'axe et sa norme l'angle autour de cet axe, alors qu'Unity utilise des quaternions. Adapté ce calcul (\url{http://www.euclideanspace.com/maths/geometry/rotations/conversions/angleToQuaternion/}) pour passer du premier au second : \url{https://github.com/enormand/aruco-unity/blob/master/src/aruco_unity_package/Assets/ArucoUnity/Scripts/Plugin/Cv/Vec3d.cs}. Voir BuJo 2017-11-07.

Pinhole camera model (physical camera)\\
\url{https://www.scratchapixel.com/lessons/3d-basic-rendering/3d-viewing-pinhole-camera/how-pinhole-camera-works-part-1}\\
\url{https://www.scratchapixel.com/lessons/3d-basic-rendering/3d-viewing-pinhole-camera/how-pinhole-camera-works-part-2}

Perspective projection (virtual camera)\\
Perspective projection matrix (virtual camera frustum) = projection transform (camera matrix) + Normalized Device Coordinates (NDC) matrix (orthogonal projection) : \url{http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/}\\
Perspective projection matrix = simpler pinhole camera model : \url{http://ksimek.github.io/2013/08/13/intrinsic/}\\
« They all require a precise understanding of how the pixels in a 2D image relate to the 3D world they represent. In other words, they all hinge on a strong camera model. »\\

Undistortion and rectification of pinhole camera model\\
Voir BuJo

Perspective projection rectification from fisheye image in the unified projection model used in omnidirectional camera calibration (ccalib OpenCV's module)\\
Omnidir camera model : \url{http://rpg.ifi.uzh.ch/docs/omnidirectional_camera.pdf}\\
determine Knew \url{https://medium.com/@kennethjiang/calibrate-fisheye-lens-using-opencv-part-2-13990f1b157f}\\
explications : \url{http://www.bobatkins.com/photography/technical/field_of_view.html}\\
fisheye projections : \url{https://wiki.panotools.org/Fisheye_Projection}\\
various lens projections : \url{http://michel.thoby.free.fr/Fisheye_history_short/Projections/Various_lens_projection.html}\\
ccalib article : \url{https://hal.archives-ouvertes.fr/file/index/docid/767674/filename/omni_calib.pdf}\\

Stereo calibration


pb de la faible résolution de : $960 / 94 \approx$\SI{10.2}{\ppd} sur le dk2 et $960 / 100 \approx$\SI{9.6}{\ppd} sur l'ovrvision\footnote{Par comparaison, le HoloLens a une résolution de $\approx$\SI{44}{\ppd} et la fovera d'\oe il humain \SI{60}{\ppd}.}

\section{Réalisation de la bibliothèque de réalité augmentée ArucoUnity}
Camera Models and Fundamental Concepts Used in Geometric Computer Vision
Camera model : équation pour expliquer comment la caméra projette le monde 3d en une image
Pinhole p.41
Calib p.103

- Voir 2016-01-25 pour l'archi :
  - Parler du principe pour dialoguer avec un plugin C++ : couche en C, gestion des pointeurs en faisant une API C\# qui encapsule ces appels à la couche C : plugin C++ OpenCV <-> couche en C <-> couche C\# reproduisant la couche C++ 
  - couche Unity avec des gameobjects et components au dessus de la couche C\#
- Aussi parler de la mémoire partagée entre Unity et OpenCv sur les images :
  - calcul de taille : 2 cameras * 950 px * 960 px * 3 bytes (RGB) = 5,47 MB
  - lecture du buffer dans sens différents (voir note 2017-04-11)
  - Threads et ordonnancement + copies des buffers images (c'était plus rapide de faire des copies que de faire attendre l'affichage avant le nouveau detect : voir note 2017-05-10) -> il n'y a pas d'attente/blocages entre les threads hormis sur les copies de buffer
- La doc qui a été faite, la petite PR pour rendre compatible les modues aruco et ccalib, le package Unity, les forks et ajouts sur internet, le package pour ovrvision (preuve que c'est extensible (citer les termes du cours MGL843))
- Utiliser des boards de 2 markers minimum pour la détection : beaucoup plus robuste qu'utiliser des markers seuls


\section{Communication réseau entre le téléphone et le visiocasque}
- Réalisation de la bibliothèque DevicesSyncUnity basée sur Unity Unet