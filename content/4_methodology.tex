\chapter{Conception d'un visiocasque de RA à large champs de vision}
\label{ch:methodology}

\section{Motivation}
Pour concevoir un VESAD, nous avons besoin d'un visiocasque avec champs de vision suffisament large pour visualiser complètement l'écran étendu, le cas contraire limite l'intérêt de notre concept. Le second sous-problème de ce travail de recherche a donc été de développer un visiocasque de RA à large champs de vision.

Un téléphone est tenu en moyenne à une distance $D=\SI{34}{\cm}$ \citep{Bababekova2011} de la tête. Ainsi, si l'on souhaite étendre un téléphone à l'équivalent d'un écran {\NoAutoSpacing 16:9} de \SI{24}{\inch}, soit une taille de $(L,H)=\SI{53x30}{\cm}$, on peut calculer le champs de vision $(FoV_x,FoV_y)$ nécessaire pour chaque \oe il pour le visualiser complètement \reffigureETSp{Fov.png}, avec l'\autoref{eq:fov} : il doit être d'au moins \SI{76x48}{\degree}.

\begin{equation}
  \label{eq:fov}
  \left \{
  \begin{array}{r c c c c c l}
    FoV_x & = & 2 \arctan (\frac{L}{2 \times D}) & = & 2 \arctan (\frac{53}{2 \times 34}) & = & \ang{76}\\
    FoV_y & = & 2 \arctan (\frac{H}{2 \times D}) & = & 2 \arctan (\frac{30}{2 \times 34}) & = & \ang{48}
  \end{array}
  \right .
\end{equation}

\figureETS{Fov.png}{
  Représentation simplifiée du champs de vision (en anglais : \texten{Field of View}), qui peut être mesuré en degrés horizontalement ($FoV_x$), verticalement ($FoV_y$) ou en diagonale. Comme le décrit l'\autoref{eq:fov}, pour qu'un plan soit visible dans un champs de vision donné, il doit être placé à une certaine distance de la caméra ou de l'\oe il.
}

Nous avons d'abord voulu utiliser le Microsoft HoloLens, un visiocasque optique de RA avec une très courte latence, une excellente résolution et une très bonne documentation et un support natif sur le moteur de jeu Unity. Cependant, son champs de vision de \SI{30x17.5}{\degree} pour chaque \oe il \citep{Kreylos2015} est trop restreint, ne permettant de visualiser seulement l'équivalent d'un écran {\NoAutoSpacing 16:9} de \SI{7}{\inch}. Il n'existe en fait actuellement aucun visiocasque de RA sur le marché avec un grand champs de vision \citep[p. 25]{Millette2016}.


\section{Solution retenue}
\subsection{Présentation du visiocasque}
\label{subsec:prototype}
Nous avons donc réalisé notre propre prototype de visiocasque \reffigureETSp{ArRift_1.jpg} similaire à l'AR-Rift \citep{Steptoe2013} : c'est un visiocasque de RA vidéo de conception simple qui a fait ses preuves, utilisé par \cite{Steptoe2014} et \cite{Piumsomboon2014}. Il consiste à coller une caméra \emph{stéréoscopique} (avec deux objectifs capturant deux images à la fois, une pour chaque \oe il) à un visiocasque de RV. Il fonctionne sur le principe suivant :
\begin{enumerate}
  \item La caméra stéréoscopique filme l'environnement réel de l'utilisateur \reffigureETSp{ArRiftMarker_1.jpg}.
  \item Un logiciel va corriger les distorsions des deux images de cette caméra (rectification) \reffigureETSp{ArRiftMarker_2.jpg}.
  \item Un logiciel de rendu 3D va ajouter du contenu virtuel sur les deux images \reffigureETSp{ArRiftMarker_3.jpg}.
  \item Les deux images sont affichées dans le visiocasque, une pour chaque \oe il \reffigureETSp{ArRiftMarker_4.jpg}.
\end{enumerate}

\figureETS[0.6]{ArRift_1.jpg}{
  Notre prototype de visiocasque de RA, basé sur le concept de l'AR-Rift de \cite{Steptoe2013} : il est composé du visiocasque de RV Oculus DK2, de la caméra stéréoscopique Ovrvision Pro et du capteur de reconnaissance des mains Leap Motion (sous la caméra).
}

La condition pour que l'illusion que le contenu virtuel soit aligné avec l'environnement réel, la caméra virtuelle du moteur 3D doit être similaire à et alignée avec la caméra physique utilisée. Autrement dit, la caméra virtuelles et et la caméra physique doivent avoir le même champs de vision (\emph{paramètres intrinsèques}) et la même position et rotation dans l'espace (\emph{paramètres extrinsèques}). Aussi, nous devons rectifier les distorsions des images de la caméra physique, c'est-à-dire une corriger cette déformation de l'image visible en particulier sur les lignes droites qui sont courbées.

Concrètement, la caméra virtuelle filme le contenu 3D et en arrière-plan l'image capturée par la caméra physique. Comme les deux caméras virtuelles et physiques ont les mêmes paramètres intrinsèques et extrinsèques, les environnement réels et virtuels sont filmés du même point de vue et apparaissent donc comme une seule scène dans le visiocasque. La \reffigureETS{ArRiftMarker} montre ce processus pour une caméra monoscopique. Pour une caméra stéréoscopique, il suffit de créer deux caméras virtuelles et de les aligner chacune avec un des objectif de la caméra physique : les deux caméras virtuelles vont filmer le même contenu 3D mais chacune l'image capturée d'un seul des deux objectifs en arrière-plan.

\figureLayoutETS{ArRiftMarker}{%
  \parbox{0.72\textwidth}{%
    \centering%
    \subfigureETS{ArRiftMarker_1.jpg}{Image non rectifiée : l'image présente des distorsions importantes (les lignes droites sont courbées).}%
    \figurehspace%
    \subfigureETS{ArRiftMarker_2.jpg}{Image rectifiée : les lignes droites le sont à nouveau.}%
    \\%
    \subfigureETS{ArRiftMarker_3.jpg}{Image rectifiée à travers Unity : l'image est affichée en arrière-plan du contenu 3D filmé par la caméra virtuelle (champs de vision en lignes blanches) pour l'\oe il gauche. La caméra physique et la caméra virtuelle sont alignées pour faire coïncider contenus virtuels et réels.}%
    \figurehspace%
    \subfigureETS{ArRiftMarker_4.jpg}{Image rectifiée et augmentée : un objet virtuel se trouve par dessus chacun des trois marqueurs.}%
  }%
}{
  Vue de l'image capturée par l'objectif gauche de l'Ovrvision Pro, puis rectifiée et augmentée pour être affichée dans le visiocasque.
}


\subsection{Choix techniques}
\label{subsec:technical_choices}
Nous avons tout d'abord choisis l'Ovrvision Pro comme caméra physique stéréoscopique (\url{http://ovrvision.com/}). Annoncée par son comme solution clé en main pour réaliser un visiocasque de RA, elle prévue pour fonctionner avec le visiocasque de VR Oculus DK2 que nous avions déjà à disposition dans notre laboratoire. Composée de deux objectifs \texten{fisheye} (en \oe il de poisson) d'une définition de \SI{960x950}{\px} et un champs de vision de \SI{100x98}{\degree}, ses caractéristiques correspondent  bien à ceux de l'Oculus DK2 \autorefp{tab:visual_densities}. Elle est de plus intégrée avec les moteurs de jeu standards dans l'industrie : Unity et Unreal Engine. Son installation se fait simplement en la collant sur la face avant de l'Oculus DK2, comme le montre la \reffigureETS{ArRift_1.jpg}, et en le connectant par USB 3.0 au PC.

Pour la localisation des mains, nous avons utilisé un Leap Motion : c'est un dispositif peu dispendieux, conçus pour concevoir des IHMs avec une main virtuelle pour les visiocasques de RV et intégré avec les moteurs de jeu Unity et Unreal Egine. Il se fixe également sur la face avant du visiocasque et se connecte simplement au PC par USB 3.0 \reffigureETSp[, sous la caméra]{ArRift_1.jpg}.

Pour faire tourner notre visiocasque de RA, nous avons utilisé un ordinateur de bureau roulant sous Windows 10, avec un processeur Intel Core i5 7400 (\SI[product-units = single]{4x3.0}{\GHz}), \SI{8}{\giga\byte} DDR4 de mémoire vive, une carte graphique NVIDIA GeForce GTX 1060 de \SI{6}{\giga\byte}. Pour le téléphone, nous avons utilisé un Xiaomi Redmi Note 4 : roulant sous Android 7, il est récent et léger, à faible prix et possède une bonne puissance de calcul ainsi qu'écran \SI{1920x1080}{\px} de \SI{5.5}{\inch}.

Nous souhaitions ensuite prototyper le plus rapidement possible sur notre visiocasque. Nous avons alors choisis le moteur de jeu Unity : gratuit (mais au code source propriétaire), il est le standard dans l'industrie du jeu-vidéo pour prototyper et supporte notre caméra et le Leap Motion. Il est de fait très avantageux d'investir un peu de temps dans l'apprentissage d'un moteur de jeu, car il prends complètement en charge le rendu 3D, la simulation de la physique, les entrées sur clavier, souris ou écran tactile, l'affichage dans les visiocasques de RV, un support réseau et propose de nombreuses fonctions mathématiques. Unity est simple plus simple à prendre en main que son concurrent l'Unreal Engine, par son GUI intuitif et l'utilisation du C\#, un langage très haut niveau, pour programmer son jeu. Enfin, nous avions une expertise dans le laboratoire avec un projet de quatre mois sur le Microsoft HoloLens et la maîtrise de \cite{Millette2016}.

Après plusieurs essais de configuration et une lecture du code source de l'Ovrvision Pro (\url{https://github.com/Wizapply/OvrvisionPro/}), nous avons constaté que la bibliothèque fournie était non fonctionnelle : (1) les images affichées pour chaque \oe il dans le casque étaient décalées, rendant le visiocasque particulièrement inconfortable à utiliser et (2) un décalage était présent entre le contenu virtuel et l'environment réel. Nous avions donc besoin de ré-étalonner cette caméra \autorefp{sec:calibration} et d'une bibliothèque de RA \autorefp{sec:aruco_unity}.

Quelques bibliothèque de RA sont disponibles sur Unity, telles que Vuforia (\url{https://unity3d.com/fr/partners/vuforia}), ARToolKit (\url{https://github.com/artoolkit/arunity5}) ou OpenCV for Unity (\url{https://enoxsoftware.com/opencvforunity/}). Malgré tout, aucune ne nous a convenu : les deux premières sont conçues pour être utilisées avec des webcams monoscopiques donc ne supportaient pas notre caméra, tandis que la troisième risquait de nous donner peu de flexibilité avec son code source propriétaire. C'est pourquoi nous avons donc réalisé la bibliothèque Aruco Unity (\url{https://github.com/NormandErwan/aruco-unity}) : basée sur la bibliothèque libre de vision par ordinateur OpenCV (\url{https://opencv.org/}), elle est la seule bibliothèque libre, sous licence BSD-3-Clause, supportant l'étalonnage et la RA sur les webcams, monoscopiques ou stéréoscopiques, et l'Ovrvision Pro sous Unity.


\subsection{Procédure}
Pour realiser notre visiocasque, nous avons suivis la procédure suivante, basée sur celle de \cite{Steptoe2013} :
\begin{enumerate}
  \item Sélection d'une caméra physique stéréoscopique, d'un visiocasque de RV et d'un moteur 3D \autorefp{subsec:technical_choices}.
  \item Étalonnage et rectification de la caméra physique \autorefp{sec:calibration}.
  \item Réalisation de la librairie de réalité augmentée Aruco Unity pour aligner la caméra virtuelle avec la caméra physique \autorefp{sec:aruco_unity}.
  \item Intégration du Leap Motion pour le suivi de la main \autorefp{sec:hand_tracking}.
  \item Conception de techniques d'interactions sur l'écran tactile et avec une main virtuelle \autorefp{sec:interaction_techniques}.
  \item Synchronisation entre le visiocasque et le téléphone \autorefp{sec:synchronization}.
\end{enumerate}

Si \cite{Steptoe2013} détaille correctement la première étape de la procédure ci-dessus, il n'expose que le principe de la troisième étape sans solution accessible pour la localisation d'objets dans l'espace réel et n'explique pas correctement la seconde étape. Nous souhaitons donc détailler ici un peu plus ces étapes deux et trois. Les étapes suivantes sont spécifique à notre projet de VESAD.


\subsection{Discussion et limites}
Nous pourrions simplifier le visiocasque en utilisant une caméra \emph{monoscopique} (avec un seul objectif capturant une seule image à la fois), et afficher l'image capturée aux deux yeux. Cependant, comme le souligne \cite{Bourke1999}, la vision stéréoscopique est l'un des principal indice utilisé par le cerveau pour percevoir la profondeur : les yeux étant séparés horizontalement par une distance appelée écart pupillaire, chaque \oe il perçoit une image légèrement différente \reffigureETSp{Ovrvision} permettant au cerveau de percevoir en 3D. C'est pourquoi nous choisissons d'utiliser une caméra stéréoscopique.

\figureLayoutETS{Ovrvision}{%
  \subfigureETS[0.2]{Ovrvision_1.jpg}{Vue de l'objectif gauche.}%
  \figurehspace%
  \subfigureETS[0.2]{Ovrvision_2.jpg}{Vue de l'objectif droit.}%
}{
  Images capturées par l'Ovrvision Pro et rectifiées : les deux objectifs sont décalés horizontalement d'environ \SI{60}{\mm} pour simuler un écart pupillaire humain.
}

Ensuite, si notre visiocasque est relativement simple à concevoir, son principal inconvénient est la faible densité visuelle de l'image. Elle permet de mesurer la finesse de l'image affichée à partir de la définition horizontale $l$ et le champs de vision et se calcule ainsi : $Densite = \frac{l}{FoV_x}$ \citep{Boger2017}. Ainsi, sur les visiocasques de RV du marché et sur l'Ovrvision Pro, la caméra stéréoscopique que nous utilisons, elle est limitée à environ \SI{10}{\ppd} (pixels par degré) \autorefp{tab:visual_densities}. En comparaison, le Microsoft HoloLens a une densité visuelle d'environ \SI{42}{\ppd} et la fovéa d'un \oe il humain de \SIrange{60}{80}{\ppd} en moyenne \citep{Kistner2014}.

Nous sommes donc face à une limite technologique : une meilleure qualité des images des caméras serait limitée par le visiocasque de RV, quelque qu'il soit. Une définition de 8K (\SI{7680x4320}{\px}) par \oe il serait alors nécessaire sur ces visiocasques et notre caméra pour atteindre la densité visuelle de la fovéa, à champs de vision égal : $Densite_{8K} = 7680 / 110 = \SI{69.8}{\ppd}$.

\begin{tableETS}{tab:visual_densities}{Caractéristiques d'affichage de l'Ovrvision Pro et de visiocasques de RV et RA}
  \begin{tabular}{| C{3.5cm} | C{3.25cm} | C{3.25cm} | C{3cm} |}
    \hline
    \textbf{Nom} & \textbf{Définition\newline(pour chaque \oe il)} & \textbf{Champs de vision\newline(pour chaque \oe il)} & \textbf{Densité visuelle}\\
    \hline
    Ovrvision Pro & \SI{960x950}{\px} & \SI{100x98}{\degree} & \SI{9.6}{\ppd}\\
    \hline
    Oculus DK2 & \SI{960x1080}{\px} & \SI{94x105}{\degree} & \SI{10.2}{\ppd}\\
    \hline
    Oculus Rift & \SI{1080x1200}{\px} & \SI{94x93}{\degree} & \SI{11.5}{\ppd}\\
    \hline
    HTC Vive & \SI{1080x1200}{\px} & \SI{110x113}{\degree} & \SI{9.8}{\ppd}\\
    \hline
    Microsoft HoloLens & \SI{1268x720}{\px} & \SI{30x17.5}{\degree} & \SI{42.3}{\ppd}\\
    \hline
  \end{tabular}
\end{tableETS}

Une seconde limite évidente est la portabilité de notre visiocasque \reffigureETSp{ARRift_2.jpg} : il est en effet volumineux et dépendant d'un PC demandant une bonne puissance de calcul. Cela reste un prototype de laboratoire qui convient à notre recherche, mais un visiocasque pour usage  profesionnel voire personnel devra dépasser ces problématiques. Correctement optimisé, un visiocasque de RA ne demande pas énormément de ressources : le Microsoft HoloLens est portable et a une très bonne autonomie. De même, il est à parier que les futures visiocasques de RV Oculus et Vive le seront aussi. De même, les objectifs de la caméra peuvent être miniaturisés et intégrés dans le visiocasque, comme le fait le HoloLens déjà.

\figureETS[0.6]{ARRift_2.jpg}{
  Notre prototype de visiocasque de RA porté par un utilisateur : il est assez volumineux et n'est pas portable. On peut voir le câble reliant le reliant au PC à l'arrière de la tête de l'utilisateur.
}


\section{Étalonnage et rectification de la caméra stéréoscopique}
\label{sec:calibration}
Pour aligner la caméra virtuelle avec la caméra physique, nous devons tout d'abord rendre identiques leur paramètres intrinsèques. Pour cela nous devons comprendre le principe d'une caméra pour ensuite pouvoir étalonner notre caméra stéréoscopique, c'est-à-dire mesurer ses paramètres intrinsèques et ses distortions. Nous pourrons ainsi corriger les distortions et appliquer les paramètres intrinsèques à la caméra virtuelle stéréoscopique. Nous survolons ici seulement la théorie pour résoudre notre problème de rectification et d'alignement ; le lecteur peut se référer à \cite[chapitres 18 et 19]{Kaehler2017} pour dérouler la procédure complète.

\subsection{Fonctionnement d'une caméra}
\subsubsection{Sténopé et caméra virtuelle}
Une caméra est un dispositif qui capture une vue en 2D (deux dimensions) d'une scène en 3D. Cette vue est une projection en \emph{perspective}, similaire à la vision humaine. Le plus simple des dispositifs de projection en perspective est le \emph{sténopé} (\texten{pinhole} en anglais) : c'est une simple boite percée d'une petite ouverture, le \emph{centre de projection}, sur une de ses face, et d'un écran, \emph{le plan image}, sensible à la lumière sur la face opposée. La scène n'est donc visible que par un unique point de vue : ainsi, chaque point sur le plan image ne provenant que d'un seul rayon de lumière à travers le centre de projection, une image nette et inversée de la scène va se former \reffigureETSp{Pinhole.png}.

\figureETS[0.9]{Pinhole.png}{
  Un sténopé : une petite ouverture sur une face de la boite laisse entrer la lumière, projettant une scène 3D (par exemple les points $P_1$ et $P_2$) en une image inversée sur la face opposée à l'ouverture (les projections respectives sont les points $p_1$ et $p_2$).
}

L'image formée sur le plan image étant toujours nette, on peut modifier le champs de vision du sténopé en déplaçant le plan image le long de l'axe optique : il s'agit de la droite perpendiculaire au plan image et passant par le centre de projection. On calcule le champs de vision avec l'\autoref{eq:fov}, la distance étant la \emph{longueur focale} $f$, c'est-à-dire la distance entre le plan image et le centre de projection. Elle est habituellement mesurée en millimètres. Ainsi, à taille équivalent du plan image, une longueur focale plus courte donne lieu à un plus grand champs de vision.

Pour simplifier les équations, on peut modifier le modèle du sténopé. Il suffit de créer un nouveau plan image symétrique au véritable plan image par rapport au centre de projection, placé donc à la même distance $f$ du centre optique. La vue générée sur ce plan image virtuel reste la même mais est non inversée : les rayons de la scène sont toujours projeté vers le centre de projection mais interceptés par ce nouveau plan image \reffigureETSp{PerspectiveProjection.png}.

\figureETS[0.9]{PerspectiveProjection.png}{
  Une projection en perspective : un point $P=(X,Y,Z)$ va être projeté sur le plan image situé à une distance $z=f$ du centre de projection en un point $p=(x,y,f)$. Une caméra virtuelle est basée sur ce principe et on peut également simplifier le sténopé à ce modèle.
}

De manière simplifiée, c'est également le fonctionnement d'une caméra virtuelle monoscopique. En suivant l'\autoref{eq:virtual_camera_projection}, on projete chaque point $P=(X,Y,Z)$ de la scène en un point $p=(x,y,z)$ sur le plan image. Les unités sont arbitraires ; Unity utilise par exemple des mètres.

\begin{equation}
  \label{eq:virtual_camera_projection}
  x = f \frac{X}{Z},\qquad y = f \frac{Y}{Z},\qquad z = f
\end{equation}

\subsubsection{Caméra physique}
Le sténopé est un modèle de caméra également suffisant pour décrire une caméra physique monoscopique. Mais elle recquiert un capteur si l'on veut enregistrer l'image capturée. Il est placé sur le plan image, là où l'image est nette.

Cependant, le centre du capteur ne coïncide pas totalement avec l'axe optique : cela requerrait une précision trop importante et non nécessaire lors du montage du capteur. Nous pouvons simplement mesurer le décalage sur le plan image de l'intersection de l'axe optique avec le plan image, appelé \emph{centre optique} avec les paramètres $c_x$ et $c_y$. On les mesure en pixels.

En outre, la longueur focale mesurée peut être différente sur les deux axes du plan image, avec les paramètres $f_x$ et $f_y$, aussi mesurés en pixels. Cela est dû a certains capteurs, de mauvais qualité, produisant des pixels rectangulaires et non carrés. On peut donc les calculer en connaissant la taille physique $(L,H)$ et la définition $(l,h)$ du capteur avec l'\autoref{eq:focal_lengths}.

\begin{equation}
  \label{eq:focal_lengths}
  f_x = f \frac{L}{l},\qquad f_y = f \frac{H}{h}
\end{equation}

Ces nouveaux paramètres font partie de la ce qu'on appelle la \emph{matrice intrinsèque} de la caméra, notée $K$. Elle permet de transformer un point $P$ de la scène est transformé en un point $p'$ dans l'espace caméra \autorefp{eq:projection}. Il suffit ensuite de le projeter sur le plan image en divisant ses coordonnées par $Z$ comme dans l'\autoref{eq:virtual_camera_projection}.

\begin{equation}
  \label{eq:projection}
  \begin{pmatrix}
    x'\\
    y'\\
    z'
  \end{pmatrix}
  =
  \underbrace{
    \begin{pmatrix}
      f_x & 0 & c_x\\
      0 & f_y & c_y\\
      0 & 0 & 1
    \end{pmatrix}
  }_\text{K}
  \begin{pmatrix}
    X\\
    Y\\
    Z
  \end{pmatrix}
\end{equation}

On résume la projection d'un point $P=(X,Y,Z)$ de la scène par une caméra physique monoscopique en un point $p=(x,y,z)$ sur le plan image en suivant l'\autoref{eq:physical_camera_projection}.

\begin{equation}
  \label{eq:physical_camera_projection}
  x = f_x \frac{X}{Z} + c_x,\qquad y = f_y \frac{Y}{Z} + c_x,\qquad z = f
\end{equation}

Enfin, une caméra physique monoscopique recquiert également un objectif photographique. En effet, le problème du sténopé est qu'il faut des heures pour qu'une image visible se forme, l'ouverture ne laissant passer que très peu de lumière. La solution est de faire une ouverture plus grande pour laisser passer plus de lumière et de placer donc un objectif photographique pour continuer à concentrer la lumière sur le plan image en une seule image nette. L'inconvénient est que ce système optique introduit des distorsions en barillet dans l'image \reffigureETSp{Distorsion}. De plus, le capteur et l'objectif ne sont pas forcément bien montés parallèles, entraînant une distorsion tangente supplémentaire, le plan image ne coïncidant pas totalement avec le capteur. D'autres distorsions existent mais sont minimes et souvent non mesurées \citep[p.377]{Bradski2008}.

\figureLayoutETS{Distorsion}{%
  \subfigureETS[0.15]{Distorsion_1.png}{Aucune distorsion.}%
  \figurehspace[10]%
  \subfigureETS[0.15]{Distorsion_2.png}{Distorsion en barillet, la grille est vue bombée vers l'extérieur.}%
  \figurehspace[10]%
  \subfigureETS[0.15]{Distorsion_3.png}{Distorsion en coussinet, la grille est vue écrasée vers l'intérieur.}%
}{
  Illustrations d'une grille vue à travers un système optique avec différents types de distorsion.
}

Ces distorsions éloignent le fonctionnement de la caméra du modèle du sténopé que nous venons de décrire. C'est pourquoi il est important de les corriger en rectifiant l'image produite par la caméra pour conserver un bon alignement avec la caméra virtuelle.

\subsubsection{Caméra physique avec un objectif \texten{fisheye}}
Les objectifs utilisés sur l'Ovrvision Pro étant \texten{fisheye}, le modèle du sténopé ne s'y applique pas. En effet, contrairement à la perspective de l'\oe il humain et des objectifs classiques, dit \emph{rectilinéaires}, ces objectifs utilisent un autre type de projection perspective avec une distorsion en barillet volontairement importante pour capter un très grand champs de vision jusqu'à \ang{180} \reffigureETSp{JPRoche2004_1.jpg}. Nous avons besoin donc de rectifier ces distortions en reprojetant les images de notre caméra en perspective rectilinéaire, c'est-à-dire comme si elles avaient été capturées par un sténopé, pour pouvoir aligner la caméra virtuelle \reffigureETSp{JPRoche2004_2.jpg}.

\figureLayoutETS{JPRoche2004}{%
  \subfigureETS[0.2]{JPRoche2004_1.jpg}{Photographie par un objectif \texten{fisheye} : les distorsions sont très importantes dans l'image, les lignes droites sont vues courbes.}%
  \figurehspace%
  \subfigureETS[0.2]{JPRoche2004_2.jpg}{Rectification de la photographie en une reprojection en perspective rectilinéaire : les lignes droites le sont à nouveau.}%
}{
  Photographie par un objectif \texten{fisheye} et rectification de la photographie.\\
  Adapté de \cite{JPRoche2004}.
}

\cite{Mei2007} proposent un modèle utilisable pour les objectifs \texten{fisheye}. La caméra est modélisée sur le principe suivant : la scène est d'abord projetée sur une sphère, puis une projection perspective rectilinéaire de la sphère est faite sur le plan image \reffigureETSp{Mei2007.jpg}. On peut alors reprojeter cette image lors de sa rectification en faisant l'opération inverse de la projection et ainsi simuler que l'image ait été capturée par un objectif rectilinéaire \reffigureETSp{PerspectiveProjection.png}.

\figureETS[0.5]{Mei2007.jpg}{%
  Illustration du modèle de \cite{Mei2007} : un point $X$ de la scène est d'abord projeté en un point $X_s$ sur une sphère puis projeté à nouveau en perspective rectilinéaire en un point $p$ sur le plan image.\\
  Tiré de \cite{Mei2007}%
}

\subsection{Étalonnage d'une caméra}
Nous étudions d'abord l'étalonnage d'une caméra physique monoscopique avec un objectif rectilinéaire, c'est-à-dire suivant le modèle du sténopé, car c'est le cas le plus simple. Nous itérons ensuite sur ce procédé en utilisant une caméra stéréoscopique, puis avec des objectifs \texten{fisheye}.

Aruco Unity permet d'étalonner facilement ces différents types de caméra. Nous décrivons dans cette sous-section les actions à faire pour cela dans Unity ; en arrière-plan, Aruco Unity exécute des fonctions de modules de la bibliothèque OpenCV : (1) aruco (\url{https://docs.opencv.org/master/d9/d6a/group__aruco.html}), implémentant \cite{Garrido-Jurado2014}, (2) calib3d pour les objectifs rectilinéaires (\url{https://docs.opencv.org/maste0r/d9/d0c/group__calib3d.html}) et (3) ccalib pour les objectifs \texten{fisheye} (\url{https://docs.opencv.org/master/d3/ddc/group__ccalib.html}), basé sur le modèle de \cite{Mei2007} et son implémentation par \cite{Li2013}. Nous avions d'abord essayé les fonctions d'étalonnage pour les objectifs \texten{fisheye} du module calib3d, mais nous n'avons pas pu les faire fonctionner : c'est pourquoi nous nous sommes tournés vers le module ccalib et le modèle de \cite{Mei2007}.

Cette sous-section a son équivalent en anglais dans la documentation en ligne (\url{https://github.com/NormandErwan/ArucoUnity/wiki/2.-Camera-Calibration}). La version en ligne est plus pratique pour permettre à nouvel utilisateur de rapidement étalonner sa caméra sans s'attarder sur la théorie sous-jacente.

\subsubsection{Caméra monoscopique avec un objectif rectilinéaire}
Étalonner une caméra monoscopique rectilinéaire consiste donc à déterminer sa matrice intrinsèque $K$ et ses paramètres de distorsion. Ce procédé fonctionne sur un principe simple. Nous avons décrit les équations qui permettent de projeter un point $P$ de la scène en un point $p$ sur le plan image. Connaissant les coordonnées de ces deux points, nous pouvons donc écrire les vecteurs de translation $T = (t_x, t_y, t_z)$ et rotation $R = (\theta_x, \theta_y, \theta_z)$ permettant de transformer un point $P$ de la scène en un point $p$ sur le plan image. En connaissant assez de couple de points $(P_i, p_i)$, on peut alors poser un ensemble d'équations pour résoudre la matrice intrinsèque $K$ de la caméra. Les déviations des points $p_i$ du modèle de sténopé ainsi calculé permettent de déterminer les paramètres de distorsion de la caméra.

Concrètement, on utilise une image d'un échiquier qu'on capture avec la caméra dans plusieurs positions et angles de vues \reffigureETSp{Bradski2008_1.jpg}, les points utilisés étant les intersections entre les carrés noirs et blancs. On connait par avance les coordonnées relatives des points $P_i$ entre eux (il suffit de le mesurer sur l'échiquier) et ces intersections sont très facile à détecter sur l'image capturée par la caméra. La première étape de l'étalonnage consiste donc à capturer avec la caméra quelques images d'un échiquier dont on connait la configuration.

\figureETS{Bradski2008_1.jpg}{
  Une caméra physique monoscopique est étalonné avec OpenCV en capturant plusieurs images d'un échiquier imprimé sur une planche rigide capturé dans différentes positions et angles de vues.\\
  Tiré de \cite[p.382]{Bradski2008}.
}

On crée alors notre échiquier d'étalonnage \reffigureETSp{CharucoBoardCreation.jpg} :
\begin{itemize}
  \item on crée un projet Unity configuré avec Aruco Unity \autorefp{sec:aruco_unity} ;
  \item dans ce projet, on ajoute un nouvel objet vide ;
  \item on ajoute à l'objet le script \code{ArucoCharucoBoard.cs}, qui représente l'échiquier, et on le configure :
  \begin{itemize}
    \item la taille des carrés en pixels avec le champs \code{SquareSideLength} ;
    \item le nombre de carrés avec \code{SquaresNumberX} et \code{SquaresNumberY} ;
    \item la taille des marqueurs avec \code{MarkerSideLength} ;
    \item la marge entre les marqueurs et les carrés blancs qui les contiennent avec \code{MarginLength} ;
  \end{itemize}
  \item on ajoute à l'objet le script \code{ArucoObjectCreator.cs}, qui permet de créer l'image de l'échiquier, et on fait pointer \code{ArucoObject} vers \code{ArucoCharucoBoard.cs} ;
  \item on démarre la scène Unity : l'image de l'échiquier s'affiche et est enregistrée sur le disque dur dans le dossier du projet (le chemin est indiqué par \code{OutputFolder}) ;
  \item on imprime l'image et on la colle fermement sur une surface rigide pour qu'elle reste le plus plate possible.
\end{itemize}  

\figureETS[0.9]{CharucoBoardCreation.jpg}{
  Création d'un échiquier d'étalonnage avec Aruco Unity : à gauche les scripts de configuration, à droite l'échiquier vu dans la scène Unity. Les marqueurs, décrits dans la \autoref{sec:aruco_unity}, dans les carrés blancs permettent l'amélioration de la détection de l'échiquier.
}

On prépare ensuite la scène de calibration \reffigureETSp{PinholeCameraCalibration}:
\begin{enumerate}
  \item on ouvre la scène Unity \code{Assets/ArucoUnity/Scenes/Calibrate.unity} déjà préparée pour l'étalonnage d'une caméra monoscopique rectilinéaire ;
  \item sur le script \code{WebcamArucoCamera.cs}, dans l'objet du même nom, on choisit la caméra à étalonner en renseignant son identifiant numérique dans le champs \code{WebcamId} ;
  \item on crée à nouveau un objet représentant notre échiquier imprimé, en y configurant cette fois la taille des carrés en mètres, permettant à Aruco Unity de construire automatiquement les positions relatives à l'échiquer des points $P_i$ ;
  \item sur le script \code{PinholeCameraCalibration.cs}, dans l'objet du même nom, on fait pointer le champs \code{CalibrationBoard} vers notre objet d'échiquier ;
  \item on peut ajuster les paramètres de calibration (\texten{calibration flags} en anglais) du module calib3d avec \code{PinholeCameraCalibrationFlags.cs} et les paramètres de détection du module aruco avec \code{DectectorParametersController.cs}, ces deux scripts ayant par défaut les valeurs recommandées par les documentations de leurs modules respectifs.
\end{enumerate}

\figureLayoutETS{PinholeCameraCalibration}{%
  \subfigureETS[0.4]{PinholeCameraCalibration_1.jpg}{Sélection de la première caméra disponible (identifiant \code{0} dans le champs \code{WebcamId}).}%
  \figurehspace%
  \subfigureETS[0.4]{PinholeCameraCalibration_2.jpg}{Lien vers l'échiquier utilisé (champs \code{CalibrationBoard}).}%
}{
  Configuration de la scène Unity \code{Calibrate} pour étalonner une caméra monoscopique rectilinéaire.
}

On peut maintenant débuter l'étalonnage en jouant la scène \reffigureETSp{PinholeCameraCalibration_3.jpg}. Le flux vidéo de la caméra est affiché alors dans Unity ainsi qu'une IHM : le bouton \code{Add Image} permet de capturer l'image courante dans une liste, le bouton \code{Reset} permet de vider cette liste et le bouton \code{Calibrate} permet d'étalonner la caméra en utilisant cette liste d'images. Il est important de garder la caméra dans une position fixe durant l'étalonnage et de varier seulement la position et l'orientation de l'échiquier entre les images. On désactive aussi la mise au point automatique de la caméra, ce paramètre impactant en effet la matrice intrinsèque	$K$ de la caméra et les distorsions de l'objectif. Enfin, les contours de l'échiquier sont surlignés en rouge dans le flux vidéo pour aider à le positioner.

%\figureETS[0.75]{PinholeCameraCalibration_3.jpg}{
%}

Une fois l'étalonnage effectué, un score mesurant sa qualité est alors affiché. Il correspond à la somme de l'erreur de reprojection pour chaque image : c'est la distance en pixels entre les points $p_i$ observés et la projection simulée des points $P_i$ sur le plan image avec le modèle de sténopé mesuré par l'étalonnage. Ce score doit être le plus proche possible de zéro si l'on souhaite un bon alignement des deux caméras : un décalage trop important entre l'image de la caméra physique et le contenu virtuel briserait l'illusion de la RA.

Le code C++ d'étalonnage \ref{lst:pinhole_calibration} exécuté en arrière-plan est assez simple : on crée l'échiquier décrit, on le détecte dans chaque image de la liste, enfin on calcule la matrice intrinsèque $K$ et les paramètres de distorsion. Ce code tourne dans un fil d'exécution (\texten{thread} en anglais) séparé de celui d'Unity : pouvant en effet demander de lourds calculs, cela nous permet de garder l'IHM d'Unity fluide.

\begin{listingETS}{C++}{lst:pinhole_calibration}{Code simplifié d'étalonnage d'Aruco Unity pour une caméra monoscopique avec un objectif rectilinéaire.}
  cv::Ptr<aruco::CharucoBoard> board = ... // create charuco board
  cv::Size imgSize = ... // camera image size
  std::vector<std::vector<cv::Point2f>> allCharucoCorners;
  std::vector<std::vector<int>> allCharucoIds;
  // Detect charuco board from several viewpoints and fill allCharucoCorners and allCharucoIds
  ...
  ...
  // After capturing in several viewpoints, start calibration
  cv::Mat cameraMatrix, distCoeffs;
  std::vector<cv::Mat> rvecs, tvecs;
  int calibrationFlags = ... // Set calibration flags (same than in calibrateCamera() function)
  double repError = cv::aruco::calibrateCameraCharuco(allCharucoCorners, allCharucoIds, board, imgSize, cameraMatrix, distCoeffs, rvecs, tvecs, calibrationFlags);
\end{listingETS}


\subsubsection{Caméra avec un objectif \texten{fisheye}}
Nous devons donc utiliser un modèle de caméra plus adapté pour décrire les objectifs de notre caméra. Le module \texten{calib3d} d'OpenCV propose un ensemble de fonctions pour étalonner une caméra monoscopique \texten{fisheye} (\url{https://docs.opencv.org/master/db/d58/group__calib3d__fisheye.html}) mais nous n'avons pas pu le faire fonctionner correctement. Nous avons alors décidé d'utiliser un second module de fonctions d'étalonnage d'OpenCV, \texten{ccalib} (\url{https://docs.opencv.org/master/d3/ddc/group__ccalib.html}), implémentant le modèle et son étalonnage décrit par \cite{Mei2007}.

Ce modèle décrit en réalité une autre catégorie d'objectifs : les objectifs catadioptriques. Ils utilisent un miroir pour capter la scène avec un très grand champs de vision jusqu'à \ang{360}, ce miroir étant ensuite filmé par la caméra \reffigureETSp{Mei2007_1.jpg}. C'est donc une portion de sphère qui est projetté sur le plan image, entraînant également une très grande distorsion de la scène \reffigureETSp{Mei2007_2.jpg}. \citeauthor{Mei2007} démontrent que ce modèle s'applique également aux caméras \texten{fisheye}.

\subsubsection{Caméra stéréoscopique}
Cependant, l'image produite par une caméra \texten{fisheye} présente des distorsions trop importantes pour pouvoir appliquer le modèle du sténopé \reffigureETSp{OvrvisionCalibration}. Nous avons décidé d'utiliser le modèle de projection

\figureLayoutETS{OvrvisionCalibration}{%
  \subfigureETS{OvrvisionCalibration_1.jpg}{Vue de l'objectif gauche.}%
  \figurehspace%
  \subfigureETS{OvrvisionCalibration_2.jpg}{Vue de l'objectif droit.}%
}{
  Vues non rectifiées d'une planche de calibration à travers l'Ovrvision Pro. L'importante distorsion des images se remarque aux lignes parallèles de la planche qui sont courbées. L'étalonnage de la caméra mesure entre autres cette distorsion.
}


%- determine Knew https://medium.com/@kennethjiang/calibrate-fisheye-lens-using-opencv-part-2-13990f1b157f

\subsection{Rectification de la caméra stéréoscopique}
% La rectification des images va les reprojeter dans une projection en perspective.

% - AdrianKaehler2017 -  Chapitre 19 pour l'étalonnage stereo + LeapMotionAlignmentCameraAR2015 pour expliquer pourquoi on applique aux caméras virtuelles l'ICD et non l'IPD


\subsection{Alignement de la caméra virtuelle}
%- Voir BuJo p.150 + AR-Rift PArt 5 pour les équations de configuration de la caméra virtuelle et du placement du background pour qu'il soit aligné avec le contenu 3D filmé par la caméra virtuelle


\section{Réalisation de la bibliothèque de réalité augmentée Aruco Unity}
\label{sec:aruco_unity}

Notre caméra étant étalonnée, rectifiée, alignée avec la caméra virtuelle et affichée dans le visiocasque, nous pouvons maintenant insérer du contenu 3D à l'image.

Camera Models and Fundamental Concepts Used in Geometric Computer Vision
Camera model : équation pour expliquer comment la caméra projette le monde 3d en une image
Pinhole p.41
Calib p.103

- Voir 2016-01-25 pour l'archi :
  - Parler du principe pour dialoguer avec un plugin C++ : couche en C, gestion des pointeurs en faisant une API C\# qui encapsule ces appels à la couche C : plugin C++ OpenCV <-> couche en C <-> couche C\# reproduisant la couche C++ 
  - couche Unity avec des gameobjects et components au dessus de la couche C\#
- Aussi parler de la mémoire partagée entre Unity et OpenCv sur les images :
  - calcul de taille : 2 cameras * 950 px * 960 px * 3 bytes (RGB) = 5,47 MB
  - lecture du buffer dans sens différents (voir note 2017-04-11)
  - Threads et ordonnancement + copies des buffers images (c'était plus rapide de faire des copies que de faire attendre l'affichage avant le nouveau detect : voir note 2017-05-10) -> il n'y a pas d'attente/blocages entre les threads hormis sur les copies de buffer
- La doc qui a été faite, la petite PR pour rendre compatible les modues aruco et ccalib, le package Unity, les forks et ajouts sur internet, le package pour ovrvision (preuve que c'est extensible (citer les termes du cours MGL843))
- Utiliser des boards de 2 markers minimum pour la détection : beaucoup plus robuste qu'utiliser des markers seuls

%- OpenCV encode sa rotation dans un vecteur dont les coordonnées normalisées donnent l'axe et sa norme l'angle autour de cet axe, alors qu'Unity utilise des quaternions. Adapté ce calcul (\url{http://www.euclideanspace.com/maths/geometry/rotations/conversions/angleToQuaternion/}) pour passer du premier au second : \url{https://github.com/enormand/aruco-unity/blob/master/src/aruco_unity_package/Assets/ArucoUnity/Scripts/Plugin/Cv/Vec3d.cs}. Voir BuJo 2017-11-07.
%- OpenCV est système main droite dans son système de coordonnées (\url{http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT9/img4.gif}) alors qu'Unity est système main gauche : il suffit d'inverser l'axe des Y (\url{https://answers.unity.com/storage/temp/8053-spaces.jpg}) : faire un petit graphe comme la 2e image

\section{Intégration du Leap Motion pour le suivi de la main}
\label{sec:hand_tracking}


\section{Conception de techniques d'interactions pour le VESAD}
\label{sec:interaction_techniques}
L'affichage du VESAD étant fonctionnel, nous avons 

Aussi justification for why we won’t be doing experimental comparison with ray cast (INPUT: RayCast): because Leap Motion and HoloLens require your hand to be visible, not pulled back, so ray cast selection wouldn’t help the user avoid fatigue

\subsection{Techniques d'interactions utilisant l'écran tactile}
User-Defined Gestures for Surface Computing \url{https://www.microsoft.com/en-us/research/wp-content/uploads/2009/04/SurfaceGestures_CHI2009.pdf}
Référence industrie (Android) : \url{https://material.io/guidelines/patterns/gestures.html}

\subsection{Techniques d'interactions utilisant une main virtuelle}
Leap in:
User-Defined Gestures for Augmented Reality \url{https://hal.inria.fr/hal-01501749/document} : on garde quelque chose de simple, pointer avec son doigt. on a pas fait de pinch, mais touch avec un doigt : pour rester proche touch sur écran tactile, car taxonomie RA, car rester simple
Pourquoi on teste l'interaction directe et pas mid-air interaction : c'est lent Vulture: a mid-air word-gesture keyboard \url{https://dl.acm.org/citation.cfm?id=2556964}
Grasp-Shell vs Gesture-Speech: A comparison of direct and indirect natural interaction
techniques in Augmented Reality \url{https://ir.canterbury.ac.nz/bitstream/handle/10092/11090/12652683_paper138-cr.pdf?sequence=1}
Lee2013 : Augmented Reality systems exploit the cognitive benefits of co-locating 3D visualizations with direct input in a real environment, using optical combiners [8, 6, 5]. This makes it possible to enable unencumbered 3D input to directly interact with situated 3D graphics in mid-air [5, 9]. -> 5 ref HoloDesk : defense au mid-air pour dire que naturel colocate display et input, interagir directement avec des affichages 3D en l'air (comme sur un écran tactile)

zoom centré sur le téléphone et non sur la grille : The focus point is generally coincident with the center of the view, more rarely with the cursor position. \cite{Guiard2004}


\section{Synchronisation entre le visiocasque et le téléphone}
\label{sec:synchronization}
- Réalisation de la bibliothèque DevicesSyncUnity basée sur Unity Unet