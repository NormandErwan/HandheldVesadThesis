\chapter{Revue de littérature}
\label{ch:litterature}

\section{Historique et concepts de la RA}
C'est \cite{Sutherland1968} qui conçoit le premier visiocasque de RA \reffigureETSp{Sutherland1968.jpg} : ce prototype permet déjà de visualiser du contenu 3D affiché dans l'espace réel de l'utilisateur, donnant l'illusion que le contenu virtuel fait réellement partie de la pièce. Par la suite, la recherche académique en RA se développe lentement : les applications développées sont surtout à visées militaires et gouvernementales \citep{VanKrevelen2010}. Il faut alors attendre les années 1990, avec la miniaturisation des PCs, pour que le domaine de recherche s'établisse enfin. Plusieurs conférences dédiées à la RA sont notamment créées, fusionnées aujourd'hui sous le nom de International Symposium on Mixed and Augmented Reality (ISMAR), une conférence d'importance pour la recherche et l'industrie en RA \citep{Azuma2001}.

\figureETS[0.6]{Sutherland1968.jpg}{
  Photos du visiocasque de RA de Sutherland.\\
  Tiré de \cite{Sutherland1968}
}

\cite{Milgram1994} donne un premier cadre théorique au domaine, en proposant une échelle ordonnée nommée \texten{Reality-Virtuality Continuum} \reffigureETSp{Milgram1994.png}. \citeauthor{Milgram1994} y oppose deux extrèmes : les environnements réels et les environnements virtuels. Les IHMs habituelles sur ordinateurs et téléphones, ou encore \texten{graphical user interface} (GUI), font partie de la première catégorie, tandis qu'un visiocasque de RV, qui immerge totalement son utilisateur dans un monde virtuel, de la seconde. Entre ces deux extrêmes, les environnements de Réalité Mixte (RM), comme la RA, vont mélanger éléments réels et virtuels. La force de cette représentation est qu'il n'existe pas de catégories séparées entre réel, RA et RV mais que la RA peut se trouver d'un extrème à un autre le long de cette échelle.

Le second enseignement de l'échelle de \citeauthor{Milgram1994} est que RA et RV sont techniquement très proches : les dispositifs de localisation, d'affichage et de géneration de contenu sont les mêmes \citep{Billinghurst2015}. Cependant, ces deux technologies n'ont pas les mêmes attentes. En effet, pour que l'immersion en RV fonctionne, il est nécessaire d'avoir un large champs de vision ; par exemple, le champs de vision du visiocasque de RV HTC Vive est de \SI{100x113}{\degree} (horizontalement $\times$ verticalement) pour les deux yeux \citep{Kreylos2016}, ce qui est proche du champs de vision humain qui est de \SI{200x135}{\degree} pour les deux yeux (\url{https://biology.stackexchange.com/a/28140}). La RA va en revanche demander tout d'abord une très grande précision et rapidité dans la localisation de l'utilisateur et des objets à augmenter pour donner le sentiment de « présence » du virtuel dans l'environnement réel.

\figureETS{Milgram1994.png}{
  L'échelle du \texten{Reality-Virtuality Continuum} de Milgram.\\
  Tiré de \citet[p. 3]{Milgram1994}
}

\cite{Rekimoto1995} apportent un second cadre théorique à la RA \reffigureETSp{Rekimoto1995.png}. Ils montrent que les GUI sont coupées des interactions avec l'environment réel, tandis que la RV isole l'utilisateur dans des IHMs totalement virtuelles. Deux approches à ces extrèmes sont la RA et l'informatique ubiquitaire (en anglais: \texten{ubiquitous computing}) : cette dernière permet à l'utilisateur d'interagir avec des ordinateur intégrés dans l'environnement réel, par exemple avec téléphones intelligents et des objets connectés, tandis que la RA fusionne réel et virtuel en un seul environnement pour l'utilisateur. Ainsi, avec une IHM bien faite, la RA s'intègre naturellement à l'environment réel et devient invisible à l'utilisation.

\figureETS{Rekimoto1995.png}{
  Comparaison de quatre styles d'IHMs : (a) les GUI, coupées de l'environment réel, (b) les IHMs en VR, isolant l'utilisateur dans un environment virtuel, (c) l'informatique ubiquitaire, faite d'ordinateurs faisant partie intégrante de l'environment réel et (d) les IHMs en RA faisant interface entre l'utilisateur et l'environment réel.\\
  Tiré de \cite{Rekimoto1995}
}

Une première définition formelle de la RA est par la suite proposée par \cite{Azuma1997} dans le premier état de l'art du domaine. Ainsi, la RA :
\begin{enumerate}
  \item combine des éléments réels et virtuels ;
  \item est interactive en temps réel ;
  \item aligne les éléments virtuels avec les éléments réels.
\end{enumerate}
Ce sont les trois conditions techniques à respecter en RA et qui vont permettre le \emph{sentiment de la présence du virtuel} dans l'environnement réel, c'est-à-dire à notre cerveau de voir virtuel et réel comme un seul et même environment. Cette définition a le mérite d'être assez générale pour s'appliquer tout autant à la RA visuelle, qu'au RA auditives ou haptiques.

Enfin, \cite{Buxton1998} repris par \cite{Bimber2005} catégorisent les différents dispositifs d'affichage en RA \reffigureETSp{Bimber2005.jpg}. On retient essentiellement (nous excluons volontairement les projecteurs) :
\begin{itemize}
  \item Le \texten{cave automatic virtual environment} (CAVE) : c'est un environnment immersif sous forme de cube de \SI{3}{\m^{3}}, chaque face comportant un écran \reffigureETSp{CAVE.jpg}. Les capteurs portés par l'utilisateur permettent au CAVE de suivre son mouvement et ainsi recalculer son champs de vision en temps réel. C'est un dispositif coûteux et encombrant, mais très utile pour prototyper des concepts.
  \item Les affichages fixes : la RA est affichée à travers un écran fixé dans l'environment \reffigureETSp{Lee2013.jpg}. Ils sont utiles pour des démonstrations pour plusieurs personnes à la fois.
  \item Les appareils mobiles : identique à un affichage fixe mais l'écran est tenu en main, comme une \emph{fenêtre sur le contenu} en RA \reffigureETSp{MobileAR.jpg}. Ils sont populaires mais limités en taille et en puissance \citep{Huang2013}.
  \item Les \texten{head-mounted display} (HMD) ou visiocasques en français : ils sont portés sur la tête et projetent les images virtuelles directement aux yeux de l'utilisateur. Ils ont l'avantage de laisser les mains libres. On distingue deux technologies :
  \begin{itemize}
    \item Les visiocasques vidéo : des caméras placées devant le casque filment l'environment de l'utilisateur, les images capturées sont ensuite combinées avec le contenu virtuel puis affichées sur un écran \reffigureETSp{ARRift.jpg}.
    \item Les visiocasques optique : le contenu virtuel est projeté sur un écran transparent par un système de mirroirs \reffigureETSp{HoloLens.jpg}.
  \end{itemize}
  \item Les lentilles : elles sont identiques aux visiocasques mais posées directement sur les yeux. Peu utilisées, elles sont encore au stade de prototype mais une fois maîtrisées, elles pourront être à l'avenir le dispotif de RA idéal \citep{VanKrevelen2010}.
\end{itemize}

Tout ces dispositifs ont été explorés dans la recherche, mais ce sont essentiellement des prototypes sur appareils mobiles qui ont été développés : en effet, à partir des années 2000, les téléphones intelligents ont eu des caméras intégrées d'une qualité suffisante, tous les capteurs nécessaire et assez de puissance de calcul pour rendre la RA possible \citep{Huang2013}. Cependant, avec l'arrivée du Microsoft HoloLens en 2016 et bientôt du Magic Leap et du Meta en 2018, les visiocasques vont pouvoir être à leur tour plus utilisés dans des recherches en RA ; c'est pourquoi nous souhaitons étudier la combinaisons des visiocasques avec des téléphones intelligents, ces deux appareils  étant présents dans nos quotidiens personnels et professionnels.

\figureETS{Bimber2005.jpg}{
  Les différents dispositifs d'affichages en RA.\\
  Tiré de \citet[p. 72]{Bimber2005}
}

\figureETS[0.7]{CAVE.jpg}{
  Un CAVE : un cube immersif d'écrans à taille humaine réagissant aux déplacement de l'utilisateur. Ici, l'utilisateur tape la main de son propre hologramme.\\
  Tiré de \cite{Kreylos2012}
}

\figureETS{Lee2013.jpg}{
  Photos de SpaceTop : l'utilisateur peut interagir avec du contenu 3D à travers un écran transparent.\\
  Tiré de \cite{Lee2013}
}

\figureETS[0.5]{MobileAR.jpg}{
  Une application sur tablette affichant un tableau de musée en RA.\\
  Tiré de \cite{KWC2012}
}

Dans leur état de l'art \cite{Azuma2001} identifient les trois obstables à dépasser pour que la RA puisse être utilisable par le grand public : (1) les limites techniques, (2) les limites des IHMs et (3) les problème d'acception sociale. Cependant, si des concepts et cadres théoriques pour la RA existent depuis plusieurs années, la recherche s'est malgré tout majoritairement consacrée aux limites techniques de la RA, comme le souligne \cite{Zhou2008}, \cite{VanKrevelen2010} et \cite{Billinghurst2015}, dans leurs états de l'art respectifs. Ils indiquent également que trop peu de travaux ont été consacrés aux IHM et à l'expérience utilisateur en RA : \textquote{there is a need to develop interface metaphors and interaction techniques specific to [augmented reality]} \citep{Billinghurst2015}.


\section{Conception et évaluation d'IHMs de RA}
\subsection{IHMs en RA}
\cite{Billinghurst2005} a exploré le premier des principes de conception d'IHM en RA. indiquait : 

\cite{VanKrevelen2010} rappellent qu'aucun paradigme d'IHM pertinent n'a encore été trouvé pour la RA. En effet, la métaphore du bureau WIMP (pour \texten{windows}, \texten{icons}, \texten{menus} et \texten{pointing device}) utilisé par les IHM des systèmes d'exploitations des ordinateurs ne fonctionne pas en RA, tout comme les dispositifs d'entrées en 2D tels que la souris : ils sont fait pour fonctionner sous la contrainte d'un plan 2D et restreignent alors l'expérience de la RA. \citep{VanKrevelen2010} A l'inverse, les dispositifs d'interactions dit naturels, sans aucune contraintes dans l'espace, c'est-à-dire avec six degrés de libertés (6 DoF), sont, en réalité, difficiles à manipuler avec dans un environnement virtuel. \cite{ChanKaoChenEtAl2010} ont en particulier montré que sans retour tactile il est difficile pour une personne d'estimer sans erreur la profondeur. Un paradigme d'IHM différent et approprié pour la RA, est donc à déterminer.

Discuter des IHM sur mobile pendant les années 2000 et 2010 (information Browsers dans la classification de Billinghurst2015)

VanDam1997 - Post Wimp user interfaces : changement IHM, difficulté conception novice vs expert, citer ihm-intention, consistent look n feel

Billinghurst2015, p. 165 : il y a maintenant plein de méthodes d'interactions mais encore besoin d'effort de conception
There have been a number of AR interface types developed since the 1960’s, including:\\
1) Information Browsers: Interfaces for showing AR information on the real world\\
2) 3D User Interfaces: Using 3D interaction techniques to manipulate content in space\\
3) Tangible User Interfaces: Using real objects to interact with AR virtual content\\
4) Natural User Interfaces: Using natural body input such as free hand gestures\\
5) Multimodal Interfaces: Using combined speech and gesture input\\

Billinghurst2015, p.179 : par contre clairement besoin de recherche dans les interfaces\\
+ étapes construction d'IHM pour la RA (le WIMP est passé par là) :\\
1) Prototype Demonstration\\
2) Adoption of Interaction techniques from other interface metaphors\\
3) Development of new interface metaphors appropriate to the medium\\
4) Development of formal theoretical models for modelling user in teractions\\
La plupart des IHM en RA ne vont pas au dela des étapes 1 et 2. Vs VR qui en est à l'étape 3 (Go-Go, controlleurs, gaze)\\
MacIntyre points out that AR design is driven by the need to define and fuse the relationship between entities in the physical world and virtual world [MacIntyre, 2002]

there are three components that must be designed in an AR application (see Figure 8.1): (1) the real physical objects, (2) the virtual elements to be displayed, and (3) the interaction metaphor that links the real and virtual elements together.\\
1 et 2 : affordances visuelles pour faire comprendre comment peuvent être manipulés. La technique d'interaction lie 1 et 2

\subsection{Interfaces Utilisateur Intangibles (IUT)}
Billinghurst2015, p.169 : interface tangibles (Tangible User Interface (TUI))\\
Kato et al. [2000] proposed the concept of Tangible AR (TAR). TAR uses Tangible UI as input interaction metaphor while using AR for visualising virtual information overlaid on the physical object used for interaction. the interaction space and display space are seamlessly merged together\\
The basic goal of designing a Tangible AR interface is to map physical objects (input) with virtual objects (output) using an appropriate interaction metaphor.\\
Space multiplexed (ex la souris qui se déplace sur bureau) vs time multiplexed

Les objets physiques ont l'avantage d'êtres familiers, facile à utiliser et de présenter des contraintes physiques sur lesquelles l'IHM peut s'appuyer \citep{ZhouDuhBillinghurst2008}. La voie de recherche de la RA tangible a été initiée par \cite{FeinerMacIntyreHauptEtAl1993} qui ont proposé un prototype accrochant des fenêtres virtuelles aux objets~; \citeauthor{FeinerMacIntyreHauptEtAl1993} parlaient alors, à juste titre, de \emph{design spatial}. Cependant, \citeauthor{ZhouDuhBillinghurst2008} soulignent que l'utilisation de telle IHM pose le défi de faire comprendre à l'utilisateur les commandes possibles avec les objets physiques et les conséquences de ces actions.

Lee2011 : Graphical Menus Using a Mobile Phone for Wearable AR Systems\\
White2009 : shake menus

\subsection{Interactions en 3D}
Si les interactions sur des surfaces tactiles comme les téléphones intelligents sont bien maîtrisées, les interactions dans les environnements en 3D sont moins connues.

Berard2009 : c'est quoi une technique d'interaction : « « Interaction is not defined by an input device alone, but by the combination of a device and an interaction technique. In the example of 3D object rotation, the mouse is typically used with the virtual sphere technique while a free-space device is used with a direct mapping (either absolute or relative) Thus, each device must be matched with its most suitable interaction technique in making performance comparisons, rather than choosing a single interaction technique for all devices. » »

Bowman2004 : summarizes various types of 3D interactions into three categories: (1) navigation, (2) selection, and (3)
manipulation

Argelaguet2013 : pb des interactions 3D, sélection tâche + métaphores main et gaze, subjectivité vs performance, design technique, taxonomie technique,  (voir cahier gris)\\
Métaphore main virtuelle : cependant, cette technique amène un autre problème quand elle est utilisée en RA. En effet, en RA, le contenu virtuel est « imprimé par dessus » le contenu réel et peut donc masquer le contenu réel. Par exemple, je peux vouloir toucher un objet 3D en l'air avec ma main, mais cette dernière sera toujours masqué par l'objet même si ma main semble en avant de l'objet. Dès lors, il faut utiliser une technique d'occlusion, c'est-à-dire masquer l'objet 3D quand un objet réel, comme la main de l'utilisateur, se trouve devant \reffigureETSp{Piumsomboon2014_1}.

\figureLayoutETS{Argelaguet2013}{%
  \subfigureETS[0.2]{Argelaguet2013_1.jpg}{Main virtuelle}%
  \figurehspace%
  \subfigureETS[0.2]{Argelaguet2013_2.jpg}{Pointeur virtuel}%
}{
  Différentes techniques de sélection.\\
  Adapté de \cite{Argelaguet2013}.
}

Bowman2001 : Principal problème est qu'il n'y a aucun retour tactile « touching a menu item floating in space is much more difficult than selecting a menu item on the desktop, not only because the task has become 3-D, but also because the impor- tant constraint of the physical desk on which the mouse rest is missing. »\\
Chan2010 - Touching the void : mid air touch in intangible displays. Naturle car simplifie la manip d'objet : le display et l'interactions sont combinés (de la même manière qu'on manipule des objets réels). Expérience d'acquisition d'objets : les personnes évaluent mal la profondeur de leur doigt (donc quand elles ont touché la cible), car pb double vision : vise le doigt et donc cible est floue. Conclusion : il faut utiliser des retours visuels pour guider l'utilisateur. Deux types de feedbacks : continu pour situer sa main, discret pour confirmer une action.

Berard2009 : input en 2D est plus performant

Piumsomboon2013 : fait une taxonomie des gestes mid-air pour l'AR, sur le modèle de Wobbrock2009 -> à évaluer sur des interfaces

\cite{Piumsomboon2014} ont comparé différentes techniques de sélection et de manipulation d'objets 3D avec un visiocasque de RA : des interactions avec la main et des commandes vocales \reffigureETSp{Piumsomboon2014_2}. Une des première leçon de leur travail est sur l'occlusion des mains avec le contenu virtuel : dans une étude pilote, ils ont remarqué que les utilisateurs n'avaient pas de préférence entre l'occlusion de la main avec le contenu 3D ou ajouter de la transparence à ce contenu virtuel \reffigureETSp{Piumsomboon2014_1}. L'occlusion de la main virtuelle étant un problème difficile à résoudre et lourd en calcul, la transparence sur le contenu virtuel en RA est une solution simple à mettre en oeuvre, comme a pu le faire \cite{Lee2013} avec SpaceTop.

\figureLayoutETS{Piumsomboon2014_1}{%
  \subfigureETS[0.15]{Piumsomboon2014_1.jpg}{Main virtuelle faisant occlusion avec le contenu 3D.}%
  \figurehspace%
  \subfigureETS[0.15]{Piumsomboon2014_2.jpg}{Contenu 3D transparent laissant la main virtuelle visible.}%
}{
  Différentes techniques d'occlusion de la main.\\
  Tiré de \cite{Piumsomboon2014}.
}

Dans une seconde expérience, \citeauthor{Piumsomboon2014} ont également trouvé que les participants préféraient utiliser et étaient plus performants avec les interactions manuelles plutôt que vocales sur les tâches de manipulation et de rotation d'objets. Les participants ont par contre préféré les commandes vocales pour modifier la taille des objets 3D, sans qu'il n'y ait de différence de performance avec les interactions manuelles. Les auteurs suggèrent donc de combiner les deux types d'interactions dans les IHMs de RA. Une limite du travail de \citeauthor{Piumsomboon2014} cependant est de n'avoir pas étudié de techniques de navigation. De plus, les interactions étaient conçues et étudiées pour manipuler des objets en 3D : une IHM de RA peut demander d'interagir avec des données plus abstraites, comme le sont nos interfaces graphiques sur ordinateur et téléphone actuellement. Les résultats de cette étude sont donc intéressant pour des métier manipulant de la 3D, mais il serait intéressant de savoir qu'elles IHM seraient adaptée pour un usage personnel au quotidien de la RA.

\figureLayoutETS{Piumsomboon2014_2}{%
  \subfigureETS{Piumsomboon2014_3.jpg}{Configuration expérimentale.}%
  \figurehspace%
  \subfigureETS{Piumsomboon2014_4.jpg}{Utilisateur saisissant un objet virtuel (technique de main virtuelle).}%
  \figurehspace%
  \subfigureETS{Piumsomboon2014_5.jpg}{Vue de l'utilisateur saisissant l'objet virtuel.}%
}{
  Photos de Grasp-Shell.\\
  Adapté de \cite{Piumsomboon2014}.
}

\subsection{Évaluation d'IHM en RA}
Swan2005, Duenser2008 (p.203) : evaluation en RA\\
(1) Objective measurements : performance (temps, erreurs, efficacité), surtout temps de complétion et taux d'erreur, ou encore score, position, nombre d'actions, mouvements\\
(2) Subjective measurements : engagament, retour utilisateur, questionnaires, notes utilisateurs, retours utilisateurs\\
(3) Qualitative analysis : avis d'experts, observations, interviews formelles, classification des comportements utilisateurs\\
(4) Usability evaluation techniques : évaluations experts, évaluations heuristiques, analyse de tache, description à haute voix, méthode magicien d'Oz\\
(5) Informal evaluations : observations utilisateurs, retours utilisateurs

taches plus écologiques (proches des usages réels du quotidien)


\section{Espaces de travail en RA}
La séparation entre les styles d'IHMs présentées \cite{Rekimoto1995} \reffigureETSp{Rekimoto1995.png} est artificielle : comme le souligne très justement \cite{Billinghurst2015}, l'échelle de \cite{Milgram1994} nous montre que la RA devrait s'intégrer avec l'informatique ubiquitaire, les GUI et la RV. Par exemple, \cite{Heun2016} propose l'application RA mobile \texten{Reality Editor} pour interagir de façon naturelle avec les objets intelligents autour de soi \reffigureETSp{Heun2016.jpg}, tandis que le prototype \texten{SpaceTop} de \cite{Lee2013} permet de combiner un GUI sur ordinateur avec des interactions 3D de manière unifiée \reffigureETSp{Lee2013.jpg}. Un VESAD est donc une IHM combinant RA et un GUI sur un téléphone intelligent.

\figureETS[0.6]{Heun2016.jpg}{
  Photo du \texten{Reality Editor}, un navigateur RA pour interagir avec les objets ubiquitaires autour de soi. Ici l'utilisateur paye un stationnement via le navigateur en pointant son téléphone vers le parc-mètre. La communication entre le téléphone et l'objet intelligent passant par internet est rendue visible grâce à la RA.\\
  Tiré de \cite{Heun2016}.
}

\cite{KoelleKranzMoeller2015} rappellent que l'expérimentation de Google Glass a permis de montrer, malgré les problèmes de vie privée et d'acceptation sociale qu'elle a posée, qu'il était intéressant de combiner un visiocasque avec un téléphone intelligent : le système proposait à l'utilisateur d'interagir avec son téléphone via le visiocasque par des interactions multimodales à la voix ou avec des gestes en l'air lus et décodés par la caméra du visiocasque.\\
Une limite du Google Glass est que son IHM est affichée seulement sur un plan virtuel à une distance fixe des yeux. Ce n'est alors pas totalement de la RA au sens de la définition de \cite{AzumaBaillotBehringerEtAl2001}, car les éléments virtuels ne sont pas alignés avec les éléments réels. De plus, \cite{SerranoEnsYangEtAl2015} rappellent que ce plan peut faire occlusion avec l'environnement réel, et gêner la vue de l'utilisateur. En outre, les mains étant libres d'utiliser un autre système tel qu'un ordinateur de bureau ou téléphone intelligent, \cite{SerranoEnsYangEtAl2015b} proposent de faire travailler en synergie un système de RA d'un visiocasque avec les autres systèmes informatiques de l'utilisateur.

\cite{Ens2014} partent de ce même constat sur les IHM des visiocasques actuels : un visiocasque peut afficher du contenu en 3D dans l'environnement, et pourrait permettre d'exploiter l'espace de la vision périphérique de l'utilisateur. Ainsi, plusieurs fenêtres pourraient être affichées en même temps à des endroits différents dans l'espace de la vision d'un utilisateur.\\
Pour explorer les capacités des visiocasques, \citeauthor{Ens2014} réalisent un prototype de RA affichant de multiples fenêtres dans les airs \reffigureETSp{Ens2014.jpg}. Un utilisateur peut alors interagir directement avec les fenêtres par le toucher (c'est donc une \emph{interaction directe}). Ils explorent alors plusieurs facteurs de conception du prototype, au travers de quatre expériences : la taille d'affichage des fenêtres virtuelles, leur distance d'affichage, leur angle de placement par rapport à l'utilisateur, et enfin leur référentiel de placement des fenêtres virtuelle (fixées au corps ou fixées au monde). Les performances (temps et taux d'erreurs), la fatigue ainsi que les retours des utilisateurs étaient mesurés.\\ 
Leurs résultats ont montrés, entre autres, qu'un affichage courbe est important pour que toutes les fenêtres soient affichées à la même distance de l'utilisateur~; ce qui confirme les résultats de \cite{ShuppBallYostEtAl2006} pour les affichages virtuels. En outre, leur conception a montré qu'elle permettait un travail 40\% plus rapide qu'avec une fenêtre simple sur une tache multi-applications : en effet, leur prototype permet d'afficher plusieurs fenêtres simultanément et ainsi réduit le changement de fenêtre, donc d'application pour du multi-tâches, à un simple mouvement de tête. Ce résultat montre également que l'interaction directe est compatible avec du contenu virtuel sous forme de fenêtres. Enfin, le référentiel sur le corps avait de plus grands taux d'erreurs que le référentiel sur le monde, car l'action de sélection entraînait des perturbations involontaires des fenêtres par rapport à l'utilisateur.\\ 
Ce travail présente cependant quelques limites. Tout d'abord, leur IHM requiert des interactions directes, ce qui est à l'origine d'erreurs de sélection et de fatigue du bras des utilisateurs : il serait alors intéressant de savoir si un référentiel sur le corps permettrait d'être précis avec une interaction indirecte. En outre, le champ de vision était très limité : 30° à l'horizontal et 40° à la verticale, ce qui biaise les mesures de performances Enfin, le prototype a été réalisé dans un CAVE, c'est-à-dire des projections de l'image sur des écrans entourant l'utilisateur. Ainsi, il faudrait reproduire les expérimentations avec un visiocasque et différentes techniques d'interactions. On peut également se demander si l'utilisation de fenêtres est une représentation adaptée pour la RA, aucune contrainte d'écrans physiques n'étant présente, ou s'il existe une meilleure représentation de l'information virtuelle dans un espace de RA.

\figureETS{Ens2014.jpg}{
  À gauche, photo du Personal Cockpit. À droite, illustration du positionnement idéal de fenêtres virtuelles.\\
  Tiré de \cite{Ens2014}.
}

Dans un article suivant, \cite{Serrano2015} généralisent le travail de conception réalisé avec MultiFi \citep{Grubert2015}. Ils proposent pour cela Gluey : une IHM qui utilise le visiocasque comme affichage pour unifier les entrées et sorties de tous les appareils, qu'ils soient mobiles ou de bureau. \citeauthor{Serrano2015} s'appuient pour cela sur les propriétés de l'affichage permanent aux yeux de l'utilisateur du visiocasque et de sa connaissance de sa position dans les environnements réels et virtuels. Ainsi, c'est un médium idéal de transmission de l'information et de redirection des entrées entre les appareils \reffigureETSp{Serrano2015.jpg}.
Un utilisateur peut donc interagir sur un appareil, et voir ses actions s'exécuter sur un autre appareil qu'il regarde. Il est intéressant que le système permette que n'importe quel appareil puisse être utilisé de manière transparente et flexible comme un dispositif d'entrée (souris, téléphone, tablette). Le système permet également de transmettre des données, par exemple pour copier des données ou les imprimer.\\
\citeauthor{Serrano2015} présentent avec ce travail un ensemble de pistes de conception pour un tel système. Leur idée était de pouvoir créer des interactions invisibles entre les appareils, pour faciliter les taches demandant d'utiliser plusieurs appareils. Pour cela, l'IHM doit permettre de : 
\begin{itemize}
  \item rediriger les entrées entre les appareils
  \item migrer du contenu entre les appareils
  \item tous les appareils doivent être compatibles
  \item d'enregistrer de nouveaux appareils
  \item de tenir un modèle spatial du système
  \item les retours du système doit toujours être visibles (ici par le visiocasque)
  \item le système doit être mobile
\end{itemize}. Ainsi, MultiFi \citep{Grubert2015} ne satisfait qu'aux critères 1, 5, 6 et 7.\\
Une limite toutefois de l'article relève dans la faiblesse de son évaluation, informelle, qui a seulement pu montrer que le concept et la preuve de concept réalisée étaient enthousiasmantes et intéressantes aux yeux des participants. Une évaluation formelle devrait être reconduite avec un prototype léger et totalement mobile. Le visiocasque présentant en outre des limites de champs de vision et de résolution : si ces caractéristiques vont s'améliorer dans le futur, nous pensons que leur impact sur la navigation et les performances des interactions des utilisateurs n'est pas encore bien connue.

\figureETS{Serrano2015.jpg}{
  Illustration du concept de Gluey.\\
  Tiré de \cite{Serrano2015}.
}

\cite{Serrano2015a} ont par la suite complété leur travail, en proposant Desktop-Gluey : ce concept étend les possibilités de Gluey en permettant à l'utilisateur d'étendre les écrans physiques de ses appareils par des fenêtres virtuelles. Les fenêtres peuvent être arrangés dans l'espace autour des écrans physique par l'utilisateur. Le système autorise donc le travail collaboratif en permettant de partager des fenêtres virtuelles entre plusieurs utilisateurs \reffigureETSp{Serrano2015a.jpg}. Enfin, les fenêtres peuvent suivre l'utilisateur dans ses déplacements et lui permettre d'interagir avec en utilisant son téléphone, une tablette ou des gestes de la main ou pour interagir, créant ainsi un concept de bureau mobile \reffigureETSp{Serrano2015a.jpg}. La métaphore du bureau sur l'ordinateur personnel est donc reprise et augmentée à \"partout\" et en \"tout temps\" : tout dispositif d'entrée peut être utilisé et tout dispositif d'entrée peut être augmenté dans son affichage.\\
Le concept recoupe celui de MultiFi \citep{Grubert2015}, et le généralise des appareils mobiles seuls à tous les appareils visibles par l'utilisateur. Une autre différence majeure est que MultiFi a été conçu pour faciliter les taches demandant d'utiliser plusieurs appareils, alors que Desktop-Gluey propose de travailler avec ces fenêtres virtuelles : on peut dire que c'est une application du Personal Cockpit \citep{EnsFinneganIrani2014} dans une IHM tangible et située autour d'appareils mobiles (comme MultiFi).\\
\citeauthor{Serrano2015a} ne présentent dans cet article que le concept mais n'en réalise aucun prototype, ni aucune évaluation. Nous pensons qu'il serait intéressant de montrer que ce concept fonctionne et d'implémenter ce système de bureau virtuel et mobile, afin d'explorer son espace de conception et d'en ajuster au mieux les paramètres.

\figureETS{Serrano2015a.jpg}{
  Illustrations du concept de Desktop-Gluey : à gauche, en utilisation de fenêtres virtuelles pour étendre des écrans physiques ; à droite, en mode mobile.\\
  Tiré de \cite{Serrano2015a}.
}

Ens2014a - Ethereal Planes : cadre de conceptions pour des fenetres 2D dans un espace de travail en RA. Redonner application au Personal Cockpit

\figureETS[0.5]{Ens2014a.jpg}{
  Illustrations du cadre de conception \texten{Ethereal Planes}, qui facile la conception de systèmes utilisant des fenêtres 2D dans des environnements de RA : (a) exemple de multi-taches, (b) exemple de travail en collaboration.\\
  Tiré de \cite{Ens2014a}.
}


\section{Visualisation et navigation de larges documents}
Dépasser les limites physiques d'un écran est un thème courant dans la littérature. En effet, un petit écran comme ceux des téléphones intelligents, ou des PCs dans les années 2000 \citep{Baudisch2002}, ne permet pas de visualiser et de naviguer correctement de grands documents : il n'est pas possible de visualiser en même temps les détails et une vue d'ensemble du document \reffigureETSp{Guiard2004_1}.

\figureLayoutETS{Guiard2004_1}{%
  \subfigureETS[0.4]{Guiard2004_1.png}{La vue de l'écran sur le document représentée par un rectangle noir.}%
  \figurehspace%
  \subfigureETS[0.4]{Guiard2004_2.png}{Le document représenté à différents niveaux d'échelle, le long de l'axe S (\texten{scale}), la vue restant de taille constante.}%
  \figurehspace%
  \subfigureETS[0.4]{Guiard2004_3.png}{Vues du document à différentes échelles.}%
}{
  Visualisation d'un large document sur un écran plus petit.\\
  Adapté de \cite{Guiard2004}.
}

Une approche est d'utiliser des IHMs multi-échelles (\texten{multiscale interfaces}) : elles permettent à l'utilisateur de visualiser des informations plus larges que l'écran utilisé. Le principe est d'afficher à l'écran une ou plusieurs vues du document à différentes échelles et positions. Trois techniques sont décrites par \cite{Guiard2004} :
\begin{itemize}
  \item \texten{Pan+Zoom} : une vue peut-être déplacée sur le document par défilement (\texten{pan}), tandis que le zoom permet de changer l'échelle du document \reffigureETSp{Guiard2004_4.png}.
  \item \texten{Overview+Detail} : une vue zoomée (\texten{detail}) et une vue sur le document en entier (\texten{overview}) sont affichées simultanément \reffigureETSp{Guiard2004_5.png}. Elles sont placées côte-à-côte ou superposées et peuvent être déplacées indépendamment. Cependant, la navigation peut-être difficile si la différence d'échelle est trop grande entre les deux vues.
  \item \texten{Focus+Context} : une seule vue permet d'afficher le document en entier et le document zoomé, grâce à une distortion optique \reffigureETSp{Guiard2004_6.png}. Aucune partie du document n'est cachée, mais il peut être difficile d'interpréter le contenu de la vue à cause de la distortion.
\end{itemize}

\figureLayoutETS{Guiard2004_1}{%
  \subfigureETS[0.2]{Guiard2004_4.png}{\texten{Pan+Zoom} : permet de défiler et zoomer à travers une seule vue dans le document.}%
  \figurehspace%
  \subfigureETS[0.2]{Guiard2004_5.png}{\texten{Overview+Detail} : affiche simultanément une vue zoomée et une vue du document en entier.}%
  \figurehspace%
  \subfigureETS[0.2]{Guiard2004_6.png}{\texten{Focus+Context} : affiche vue du document en entier et un zoom par une distortion optique (sphérique à gauche et linéaire à droite).}%
}{
  Techniques de visualisation et de navigation d'un large document sur un écran plus petit.\\
  Adapté de \cite{Guiard2004}.
}

\cite{Baudisch2002} - Keeping things in context: a comparative evaluation of focus plus context screens, overviews, and zooming

\figureETS[0.5]{Baudisch2002.jpg}{
  Combinaison d'un écran d'un écran d'ordinateur haute résolution (\texten{focus}) aligné avec un projecteur basse résolution (\texten{context}).\\
  Tiré de \cite{Baudisch2002}.
}

\cite{Liu2014} - Effects of display size and navigation type on a classification task : est-ce que les connaissances sur les écrans de taille d'un ordinateur s'appliquent à ces nouveaux écrans ? Ces nouveaux écrans ont la même haute densité que les écrans de bureau, mais leur résolution est généralement 10 fois élevée en nombre de pixels : un utilisateur doit donc s'approcher physiquement pour pouvoir voir le détail et reculer pour avoir une vue d'ensemble. (voir fin cahier gris)\\
« while the desktop can be faster than the wall for simple tasks, the wall gains a sizable advantage as the task becomes more difficult. A follow-up study shows that other desktop techniques (overview+detail, lens) do not perform better than pan-and-zoom and are therefore slower than the wall for difficult tasks (manupilating elements in a complex decision making task tha requires expertise and quick access to full content) -> eg. third task of personal cockpit or google maps (the typical task with wedge, or overview+detail) »

\figureETS[0.6]{Liu2014.jpg}{
  Photo de l'affichage mural dans l'expérience de \cite{Liu2014}. Les disques rouges doivent être déplacés pour être classés dans le bon écran. Le participant navigue le contenu par un \texten{Pan+Zoom} physique : en se déplaçant face à l'affichage mural et en se rapprochant plus ou moins des écrans.\\
  Tiré de \cite{Liu2014}.
}

\cite{Berge2014} explorent les interactions d'un téléphone intelligent combiné avec un grand écran. Cette IHM \texten{Overview+Detail} permet à des utilisateurs d'interagir via leur téléphone (la vue \texten{detail}) avec les écran présents dans les espaces publics (l'\texten{overview}) \reffigureETSp{Berge2014.jpg}. 
Les utilisateurs ont préféré les interactions Mid-Air Phone et Mid-Air Hand (avec ou sans entraînement) versus les interactions touchscreen dans une interface Overview+Detail d'exploration de contenu 3D : un écran faisait overview et un téléphone mobile de detail.
Nous on ne ce pose pas la question de l'entraînement : car on considère que c'est une IHM qui serait utilisée au quotidien ou professionnellement : donc c'est correct d'avoir un temps d'apprentissage.
Cependant le non-alignement entre les deux écrans pose problème en divisant l'attention de l'utilisateur.

\figureETS{Berge2014.jpg}{
  Combinaison d'un téléphone et d'un grand écran dans une IHM \texten{Overview+Detail} : (a) illustration du concept, le téléphone est une vue \texten{detail}, projetté sur le projecteur en \texten{overview}, (b) la vue est déplacée avec le téléphone, (c) la vue est déplacée avec une main virtuelle.\\
  Tiré de \cite{Berge2014}.
}


\section{Affichages étendus}
L'agrandissement d'affichages est un thème déjà existant dans la littérature. \cite{Bi2011} proposent par exemple de remplacer un bureau de PC par une table tactile \reffigureETSp{Bi2011.jpg} : des fenêtres et barres d'outils peuvent être déplacées de l'ordinateur vers la table tactile, des miniatures pour interagir avec des fenêtres ouvertes y sont affichées en bas de la table tactile et le clavier et la souris sont augmentés par une IHM les suivant. \citeauthor{Bi2011} ont évalué la faisaibilité de leur concept : les participants pouvaient facilement faire aller et venir leurs mains du clavier et de la souris pour interagir à une main ou à deux mains sur la table tactile, en particulier au dessus et en dessous du clavier mais plus difficilement sur les côtés gauche et droit de la table tactile.

\figureETS[0.6]{Bi2011.jpg}{
  Photo de MagicDesk : le clavier et la souris d'un ordinateur sont posés sur et augmentés par une table tactile.\\
  Tiré de \cite{Bi2011}.
}

\subsection{Affichages étendus par un projecteur}
Proche du prototype de \cite{Baudisch2002}, l'aggrandissement d'un écran de télévision à été exploré par \cite{Jones2013}, qui ont créé une IHM \texten{Focus+Context} avec un projecteur \reffigureETSp{Jones2013.jpg}. Ils ont alors conçus 11 visualisations différentes qu'ils ont évaluées auprès de joueurs et de game designers : les participants ont tous été impressionnés et enthousiasmés par le concept. Cependant l'effet peut être trop envahissant sur le long terme : un équilibre doit alors être trouvé entre contenu et confort (\reffigureETS{Jones2013.jpg} (b)). 

De manière similaire, \cite{Benko2015} ont étendu le champs de vision d'un visiocasque de RA à l'aide d'un projecteur \reffigureETSp{Benko2015.jpg}. En effet, la plupart des visiocasques de l'industrie ont de petits champs de vision ($\approx$\ang{40} horizontalement), ce qui limite la taille du contenu virtuel visible. L'avantage du visiocasque par rapport à la télévision de \cite{Jones2013} est qu'il peut afficher du contenu en 3D et non projetté mais leur système est fixé dans la pièce, ce qui limite les applications possibles.

Notre concept de VESAD a ainsi l'avantage d'être portable par rapport à \cite{Jones2013} et \cite{Benko2015}. Il aussi permet d'afficher le contenu en périphérie en 3D sans être limité par le plan du mur. Enfin, les interactions peuvent profiter de l'écran tactile.

\figureETS[0.6]{Jones2013.jpg}{
  Photos de Illumiroom : (a) le champs de vision de la télévision haute-définition (\texten{focus}) est étendu par un projecteur (\texten{context}), (b)  une partie du champs de vision de la télévision est étendu (ici les balles du fusil).\\
  Adapté de \cite{Jones2013}.
}

\figureETS{Benko2015.jpg}{
  Photo de FoveAR : (a) le visiocasque de RA utilisé, (b) vue depuis le visiocasque, (c) vue depuis le visiocasque étendu par un projecteur, (d) illustration du champs de vision du visiocasque étendu.\\
  Tiré de \cite{Benko2015}.
}

\subsection{Aggrandissement d'appareils mobiles}
\cite{Grubert2015} proposent déjà d'utiliser les visiocasques pour étendre et joindre les affichages des différents appareils mobiles portés sur soi : téléphones intelligents, tablettes ou montres connectées. En effet, ces appareils mobiles ne sont souvent pas conçus pour travailler ensemble, mais gagneraient à l'être. 

Leur article explore tout d'abord les facteurs de conception de ce concept, dont le type d'alignement \reffigureETSp{Grubert2015_1.jpg} : (1) le contenu projetté par le visiocasque a pour référence le corps de l'utilisateur (similaire à \cite{Ens2014}), l'appareil mobile formant une vue \texten{detail} sur le contenu (similaire à \cite{Berge2014}) dans le \texten{body-aligned mode}, (2) le contenu est centré sur l'appareil mobile et agrandi au-delà de l'écran par le visiocasque dans le \texten{device-aligned mode}, que nous appellons un VESAD, (3) l'appareil mobile et le visiocasque affichent indépendemment leur contenu dans le \texten{side-by-side mode}. D'autres facteurs de conceptions sont : le type d'interaction, directement ou indirectement sur le contenu, et la précision possible des affichages et interactions de chaque appareil \reffigureETSp{Grubert2015_2.jpg}.

\figureETS{Grubert2015_1.jpg}{
  Illustrations des alignements possibles de contenu entre un visiocasque de RA et un appareil mobile : à gauche, le \texten{body-aligned mode}, au milieu, le \texten{device-aligned mode} et à droite, le \texten{side-by-side mode}.\\
  Tiré de \cite{Grubert2015}.
}

\figureETS[0.5]{Grubert2015_2.jpg}{
  Photo d'une montre intelligente agrandie en haut. En bas, un téléphone intelligent agissant comme une vue \texten{detail} sur une partie du contenu étendu.\\
  Tiré de \cite{Grubert2015}.
}

Leurs résultats expérimentaux montrent, dans deux tâches de navigation, que le couplage des appareils mobiles avec un visiocasque de RA peut permettre des temps plus rapides par rapport aux appareils seuls (visiocasque seul ou appareil mobile seul), mais au détriment d'un plus grand effort perçu par les utilisateurs. De plus, les préférences des utilisateurs étaient variées : cela montre que le choix du couplage entre appareils mobiles et visiocasque doit être laissé à l'utilisateur.
En outre, le visiocasque utilisé était lourd et avait un petit champs de vision (\ang{30.5} horizontalement et \ang{17.15} verticalement) ne permettant pas de visualiser de large contenus, ce qui limite leurs résultats expérimentaux. Aussi, les interactions n'étaient possibles que sur les écrans tactiles : il serait intéressant de les comparer avec des interactions utilisant une main virtuelle.


\section{Problématique}
Cette revue de littérature a permit d'identifier un besoin de conception d'IHMs en RA s'appuyant sur des visiocasques. En particulier, nous souhaitons explorer la conception d'une IHM pour un visiocasque de RA combiné avec un téléphone intelligent. Ainsi, nous définissons la problématique de ce mémoire ainsi : est-ce qu'un téléphone augmenté par un VESAD donne un avantage à un utilisateur par rapport à un téléphone seul ? Quelles seraient les meileures techniques d'interactions à utiliser sur un tel téléphone augmenté ?

Nous formulons les hypothèses suivantes par rapport à cette problématique :
\begin{enumerate}[label={(H\arabic*)}]
  \item Notre système	permet d'être plus performant sur des tâches de navigation, de classification ou demandant d'utiliser plusieurs applications en parallèle que sur un téléphone seul, quelle que soit la technique d'interaction utilisée.
  \item Les utilisateurs apprécieront d'avantage pouvoir interagir directement avec l'écran étendu autour du téléphone sur notre système.
  \item Les utilisateurs seront en revanche plus performants en interagissant seulement avec l'écran tactile du téléphone sur notre système.
\end{enumerate}

Enfin, pour y répondre, nous divisons cette problématique en quatre sous-problèmes :
\begin{enumerate}
  \item Concevoir une IHM d'un téléphone à l'écran agrandi par RA.
  \item Développer un visiocasque de RA à large champs de vision.
  \item Développer un prototype de cette IHM à l'aide du visiocasque.
  \item Réaliser une expérimentation évaluant différentes interfaces et techniques d'interactions sur ce prototype sur une tâche de classification.
\end{enumerate}

Les résultats à ces objectifs permettront de donner des recommandations pour de futures recherches d'IHM en RA.