\chapter{Revue de littérature}
\label{ch:litterature}

\section{Historique et concepts de la RA}
C'est \cite{Sutherland1968} qui conçoit le premier visiocasque de RA \reffigureETSp{Sutherland1968.png} : ce prototype permet déjà de visualiser du contenu 3D affiché dans l'espace réel de l'utilisateur, donnant l'illusion que le contenu virtuel fait réellement partie de la pièce. Par la suite, la recherche académique en RA se développe lentement : les applications développées sont surtout à visées militaires et gouvernementales \citep{VanKrevelen2010}. Il faut alors attendre les années 1990, avec la miniaturisation des PCs, pour que le domaine de recherche s'établisse enfin. Plusieurs conférences dédiées à la RA sont notamment créées, fusionnées aujourd'hui sous le nom de International Symposium on Mixed and Augmented Reality (ISMAR), une conférence d'importance pour la recherche et l'industrie en RA \citep{Azuma2001}.

\figureETS{Sutherland1968.png}{
  Visiocasque de RA de Sutherland.\\
  Tiré de \cite{Sutherland1968}
}

\cite{Milgram1994} donne un premier cadre théorique au domaine, en proposant une échelle ordonnée nommée \texten{Reality-Virtuality Continuum} \reffigureETSp{Milgram1994.png}. \citeauthor{Milgram1994} y oppose deux extrèmes : les environnements réels et les environnements virtuels. Les IHMs habituelles sur ordinateurs et téléphones, ou encore \texten{graphical user interface} (GUI), font partie de la première catégorie, tandis qu'un visiocasque de RV, qui immerge totalement son utilisateur dans un monde virtuel, de la seconde. Entre ces deux extrêmes, les environnements de Réalité Mixte (RM), comme la RA, vont mélanger éléments réels et virtuels. La force de cette représentation est qu'il n'existe pas de catégories séparées entre réel, RA et RV mais que la RA peut se trouver d'un extrème à un autre le long de cette échelle.

Le second enseignement de l'échelle de \citeauthor{Milgram1994} est que RA et RV sont techniquement très proches : les dispositifs de localisation, d'affichage et de géneration de contenu sont les mêmes \citep{Billinghurst2015}. Cependant, ces deux technologies n'ont pas les mêmes attentes. En effet, pour que l'immersion en RV fonctionne, il est nécessaire d'avoir un large champs de vision et un très bon réalisme dans le contenu 3D affiché. La RA va en revanche demander tout d'abord une très grande précision et rapidité dans la localisation de l'utilisateur et des objets à augmenter pour donner le sentiment de « présence » du virtuel dans l'environnement réel.

\figureETS{Milgram1994.png}{
  L'échelle du \texten{Reality-Virtuality Continuum} de Milgram.\\
  Tiré de \citet[p. 3]{Milgram1994}
}

\cite{Rekimoto1995} vont apporter un second cadre théorique à la RA \reffigureETSp{Rekimoto1995.png}. Ils montrent que les GUI sont coupées des interactions avec l'environment réel, tandis que la RV isole l'utilisateur dans des IHMs totalement virtuelles. Deux approches à ces extrèmes sont la RA et l'informatique ubiquitaire (en anglais: \texten{ubiquitous computing}) : cette dernière permet à l'utilisateur d'interagir avec des ordinateur intégrés dans l'environnement réel, par exemple avec téléphones intelligents et des objets connectés, tandis que la RA fusionne réel et virtuel en un seul environnement pour l'utilisateur. Ainsi, avec une IHM bien faite, la RA s'intègre naturellement à l'environment réel et devient invisible à l'utilisation.

\figureETS{Rekimoto1995.png}{
  Comparaison de quatre styles d'IHMs : (a) les GUI, coupées de l'environment réel, (b) les IHMs en VR, isolant l'utilisateur dans un environment virtuel, (c) l'informatique ubiquitaire, faite d'ordinateurs faisant partie intégrante de l'environment réel et (d) les IHMs en RA faisant interface entre l'utilisateur et l'environment réel.\\
  Tiré de \citet{Rekimoto1995}
}

Une première définition formelle de la RA est par la suite proposée par \cite{Azuma1997} dans le premier état de l'art du domaine. Ainsi, la RA :
\begin{enumerate}
  \item combine des éléments réels et virtuels ;
  \item est interactive en temps réel ;
  \item aligne les éléments virtuels avec les éléments réels.
\end{enumerate}
Ce sont les trois conditions techniques à respecter en RA et qui vont permettre le \emph{sentiment de la présence du virtuel} dans l'environnement réel, c'est-à-dire à notre cerveau de voir virtuel et réel comme un seul et même environment. Cette définition a le mérite d'être assez générale pour s'appliquer tout autant à la RA visuelle, qu'au RA auditives ou haptiques.

Enfin, \cite{Buxton1998} repris par \cite{Bimber2005} catégorisent les différents dispositifs d'affichage en RA \reffigureETSp{Bimber2005.png}. On retient essentiellement (nous excluons volontairement les projecteurs) :
\begin{itemize}
  \item Le \texten{cave automatic virtual environment} (CAVE) : c'est un environnment immersif sous forme de cube, chaque face comportant un écran \reffigureETSp{CAVE.jpg}. Les capteurs portés par l'utilisateur permettent au CAVE de suivre son mouvement, et ainsi recalculer son champs de vision en temps réel. C'est un dispositif coûteux et encombrant, mais très utile pour prototyper des concepts.
  \item Les affichages fixes : la RA est affichée à travers un écran fixé dans l'environment \reffigureETSp{Lee2013.png}. Ils sont utiles pour des démonstrations pour plusieurs personnes à la fois.
  \item Les appareils mobiles : identique à un affichage fixe mais l'écran est tenu en main, comme une \emph{fenêtre sur le contenu} en RA \reffigureETSp{MobileAR.jpg}. Ils sont populaires mais limités en taille et en puissance \citep{Huang2013}.
  \item Les \texten{head-mounted display} (HMD) ou visiocasques en français : ils sont portés sur la tête et projetent les images virtuelles directement aux yeux de l'utilisateur. On distingue deux technologies :
  \begin{itemize}
    \item Les visiocasques vidéo : des caméras placées devant le casque filment l'environment de l'utilisateur, les images capturées sont ensuite combinées avec le contenu virtuel puis affichées sur un écran \reffigureETSp{ARRift.jpg}.
    \item Les visiocasques optique : le contenu virtuel est projeté sur un écran transparent par un système de mirroirs \reffigureETSp{HoloLens.jpg}.
  \end{itemize}
  \item Les lentilles : elles sont identiques aux visiocasques mais posées directement sur les yeux. Peu utilisées, elles sont encore au stade de prototype.
\end{itemize}

Tout ces dispositifs ont été explorés dans la recherche, mais ce sont essentiellement des prototypes sur appareils mobiles qui ont été développés : en effet, à partir des années 2000, les téléphones intelligents ont eu des caméras intégrées d'une qualité suffisante, tous les capteurs nécessaire et assez de puissance de calcul pour rendre la RA possible \citep{Huang2013}. Cependant, avec l'arrivée du HoloLens de Microsoft en 2016 et bientôt du Magic Leap et du Meta en 2018, les visiocasques vont pouvoir être à leur tour plus utilisés dans des recherches en RA. Dans leur état de l'art du domaine, \cite{VanKrevelen2010} estiment quant à eux que les lentilles seront à l'avenir le dispotif de RA idéal.

\figureETS{Bimber2005.png}{
  Les différents dispositifs d'affichages en RA.\\
  Tiré de \citet[p. 72]{Bimber2005}
}

\figureETS{CAVE.jpg}{
  Un CAVE : un cube immersif d'écrans à taille humaine réagissant aux déplacement de l'utilisateur.\\
  Tiré de \citet{DWC2001}
}

\figureETS{Lee2013.png}{
  Photos de SpaceTop : l'utilisateur peut interagir avec du contenu 3D à travers un écran transparent.\\
  Tiré de \citet{Lee2013}
}

\figureETS{MobileAR.jpg}{
  Une application sur tablette affichant un tableau de musée en RA.\\
  Tiré de \citet{KWC2012}
}

Dans leur état de l'art \cite{Azuma2001} identifient les trois obstables à dépasser pour que la RA puisse être utilisable par le grand public : (1) les limites techniques, (2) les limites des IHM et (3) les problème d'acception sociale. Cependant, si des concepts et cadres théoriques pour la RA existent depuis plusieurs années, la recherche s'est malgré tout majoritairement consacrée aux limites techniques de la RA, comme le souligne \cite{Zhou2008}, \cite{VanKrevelen2010} et \cite{Billinghurst2015}, dans leurs états de l'art respectifs. Ils indiquent également que trop peu de travaux ont été consacrés aux IHM et à l'expérience utilisateur en RA : \textquote{there is a need to develop interface metaphors and interaction techniques specific to [augmented reality]} \citep{Billinghurst2015}.


\section{Conception et évaluation d'IHMs de RA}
\subsection{IHMs en RA}
\cite{Billinghurst2005} a exploré le premier des principes de conception d'IHM en RA. indiquait : 

\citet{VanKrevelen2010} rappellent qu'aucun paradigme d'IHM pertinent n'a encore été trouvé pour la RA. En effet, la métaphore du bureau WIMP (pour \texten{windows}, \texten{icons}, \texten{menus} et \texten{pointing device}) utilisé par les IHM des systèmes d'exploitations des ordinateurs ne fonctionne pas en RA, tout comme les dispositifs d'entrées en 2D tels que la souris : ils sont fait pour fonctionner sous la contrainte d'un plan 2D et restreignent alors l'expérience de la RA. \citep{VanKrevelen2010} A l'inverse, les dispositifs d'interactions dit naturels, sans aucune contraintes dans l'espace, c'est-à-dire avec six degrés de libertés (6 DoF), sont, en réalité, difficiles à manipuler avec dans un environnement virtuel. \citet{ChanKaoChenEtAl2010} ont en particulier montré que sans retour tactile il est difficile pour une personne d'estimer sans erreur la profondeur. Un paradigme d'IHM différent et approprié pour la RA, est donc à déterminer.

Discuter des IHM sur mobile pendant les années 2000 et 2010 (information Browsers dans la classification de Billinghurst2015)

VanDam1997 - Post Wimp user interfaces : changement IHM, difficulté conception novice vs expert, citer ihm-intention, consistent look n feel

Billinghurst2015, p. 165 : il y a maintenant plein de méthodes d'interactions mais encore besoin d'effort de conception
There have been a number of AR interface types developed since the 1960’s, including:\\
1) Information Browsers: Interfaces for showing AR information on the real world\\
2) 3D User Interfaces: Using 3D interaction techniques to manipulate content in space\\
3) Tangible User Interfaces: Using real objects to interact with AR virtual content\\
4) Natural User Interfaces: Using natural body input such as free hand gestures\\
5) Multimodal Interfaces: Using combined speech and gesture input\\

Billinghurst2015, p.179 : par contre clairement besoin de recherche dans les interfaces\\
+ étapes construction d'IHM pour la RA (le WIMP est passé par là) :\\
1) Prototype Demonstration\\
2) Adoption of Interaction techniques from other interface metaphors\\
3) Development of new interface metaphors appropriate to the medium\\
4) Development of formal theoretical models for modelling user in teractions\\
La plupart des IHM en RA ne vont pas au dela des étapes 1 et 2. Vs VR qui en est à l'étape 3 (Go-Go, controlleurs, gaze)\\
MacIntyre points out that AR design is driven by the need to define and fuse the relationship between entities in the physical world and virtual world [MacIntyre, 2002]

there are three components that must be designed in an AR application (see Figure 8.1): (1) the real physical objects, (2) the virtual elements to be displayed, and (3) the interaction metaphor that links the real and virtual elements together.\\
1 et 2 : affordances visuelles pour faire comprendre comment peuvent être manipulés. La technique d'interaction lie 1 et 2

\subsection{Interfaces Utilisateur Intangibles (IUT)}
Billinghurst2015, p.169 : interface tangibles (Tangible User Interface (TUI))\\
Kato et al. [2000] proposed the concept of Tangible AR (TAR). TAR uses Tangible UI as input interaction metaphor while using AR for visualizing virtual information overlaid on the physical object used for interaction. the interaction space and display space are seamlessly merged together\\
The basic goal of designing a Tangible AR interface is to map physical objects (input) with virtual objects (output) using an appropriate interaction metaphor.\\
Space multiplexed (ex la souris qui se déplace sur bureau) vs time multiplexed

Lee2011 : Graphical Menus Using a Mobile Phone for Wearable AR Systems\\
White2009 : shake menus

\subsection{Interactions en 3D}
Berard2009 : c'est quoi une technique d'interaction : « Interaction is not defined by an input device alone, but by the combination of a device and of an interaction technique. »

Bowman2004 : summarizes various types of 3D interactions into three categories: (1) navigation, (2) selection, and (3)
manipulation

Argelaguet2013 : pb des interactions 3D, sélection tâche + métaphores main et gaze, subjectivité vs performance, design technique, taxonomie technique,  (voir cahier gris)\\
Métaphore main virtuelle : cependant, cette technique amène un autre problème quand elle est utilisée en RA. En effet, en RA, le contenu virtuel est « imprimé par dessus » le contenu réel et peut donc masquer le contenu réel. Par exemple, je peux vouloir toucher un objet 3D en l'air avec ma main, mais cette dernière sera toujours masqué par l'objet même si ma main semble en avant de l'objet. Dès lors, il faut utiliser une technique d'occlusion, c'est-à-dire masquer l'objet 3D quand un objet réel, comme la main de l'utilisateur, se trouve devant \reffigureETSp{Piumsomboon2014_1.png}.

\figureETS{Argelaguet2013.jpg}{
  Différentes techniques de sélection. À gauche, une main virtuelle. À droite, un pointeur virtuel. Tiré de \cite{Argelaguet2013}.
}

\figureETS{Piumsomboon2014_1.png}{
  Différentes techniques d'occlusion de la main. À gauche, la main cache le contenu 3D si elle est devant. À droite, le contenu 3D est transparent que la main soit derrière ou devant. Tiré de \cite{Piumsomboon2014}.
}

Bowman2001 : Principal problème est qu'il n'y a aucun retour tactile « touching a menu item floating in space is much more difficult than selecting a menu item on the desktop, not only because the task has become 3-D, but also because the impor- tant constraint of the physical desk on which the mouse rest is missing. »\\
Chan2010 - Touching the void : mid air touch in intangible displays. Naturle car simplifie la manip d'objet : le display et l'interactions sont combinés (de la même manière qu'on manipule des objets réels). Expérience d'acquisition d'objets : les personnes évaluent mal la profondeur de leur doigt (donc quand elles ont touché la cible), car pb double vision : vise le doigt et donc cible est floue. Conclusion : il faut utiliser des retours visuels pour guider l'utilisateur. Deux types de feedbacks : continu pour situer sa main, discret pour confirmer une action.

Berard2009 : input en 2D est plus performant

Piumsomboon2013 : fait une taxonomie des gestes mid-air pour l'AR, sur le modèle de Wobbrock2009 -> à évaluer sur des interfaces

\cite{Piumsomboon2014} ont comparé différentes techniques de sélection et de manipulation d'objets 3D avec un visiocasque de RA : des interactions avec la main et des commandes vocales \reffigureETSp{Piumsomboon2014_2.jpg}. Une des première leçon de leur travail est sur l'occlusion des mains avec le contenu virtuel : dans une étude pilote, \citeauthor{Piumsomboon2014} ont remarqué que les utilisateurs n'avaient pas de préférence entre l'occlusion de la main avec le contenu 3D ou ajouter de la transparence à ce contenu virtuel. L'occlusion de la main virtuelle étant un problème difficile à résoudre et lourd en calcul, la transparence sur le contenu virtuel en RA est une solution simple à mettre en oeuvre, comme a pu le faire \cite{Lee2013} avec SpaceTop.\\
Dans une seconde étude, ils ont également trouvé que les participants préféraient utiliser et étaient plus performants avec les interactions manuelles plutôt que vocales sur les tâches de manipulation et de rotation d'objets. Les participants ont par contre préféré les commandes vocales pour modifier la taille des objets 3D, sans qu'il n'y ait de différence de performance avec les interactions manuelles. Les auteurs suggèrent donc de combiner les deux types d'interactions dans les IHM de RA.\\
Une limite de leur travail cependant est de n'avoir pas étudié de techniques de navigation. De plus, les interactions étaient conçues et étudiées pour manipuler des objets en 3D : une IHM de RA peut demander d'interagir avec des données plus abstraites, comme le sont nos interfaces graphiques sur ordinateur et téléphone actuellement. Les résultats de cette étude sont donc intéressant pour des métier manipulant de la 3D, mais il serait intéressant de savoir qu'elles IHM seraient adaptée pour un usage personnel au quotidien de la RA.

\figureETS{Piumsomboon2014_2.jpg}{
  Illustration de Grasp-Shell : (A) Configuration expérimentale, (B) Utilisateur saisissant un objet virtuel, (C) Vue de l'utilisateur.\\
  Tiré de \cite{Piumsomboon2014}.
}

\subsection{Évaluation d'IHM en RA}
Duenser2008 (p.203) : evaluation en RA\\
(1) Objective measurements : performance (temps, erreurs, efficacité)\\
(2) Subjective measurements : engagament, retour utilisateur\\
(3) Qualitative analysis : avis d'experts\\
(4) Usability evaluation techniques\\
(5) Informal evaluations


\section{Espaces de travail en RA}
Comme le souligne très justement \cite{Billinghurst2015}, la séparation entre les styles d'IHMs présentées \cite{Rekimoto1995} \reffigureETSp{Rekimoto1995.png} est artificielle : l'échelle de \cite{Milgram1994} nous montre que la RA devrait s'intégrer avec l'informatique ubiquitaire, les GUI et la RV. Par exemple, \cite{Heun2016} propose l'application RA mobile \texten{Reality Editor} pour interagir de façon naturelle avec les objets intelligents autour de soi \reffigureETSp{Heun2016.jpg}, tandis que le prototype \texten{SpaceTop} de \cite{Lee2013} permet de combiner un GUI sur ordinateur avec des interactions 3D de manière unifiée \reffigureETSp{Lee2013.png}.

\figureETS{Heun2016.jpg}{
  Photo de \texten{Reality Editor} de \cite{Heun2016}, un navigateur RA pour interagir avec les objets ubiquitaires autour de soi. Ici l'utilisateur paye un stationnement via le navigateur en pointant son téléphone vers le parc-mètre. La communication entre le téléphone et l'objet intelligent passant par internet est rendue visible grâce à la RA.\\
  Tiré de \cite{Heun2016}.
}

\citet{Ens2014} partent de ce même constat sur les IHM des visiocasques actuels : un visiocasque peut afficher du contenu en 3D dans l'environnement, et pourrait permettre d'exploiter l'espace de la vision périphérique de l'utilisateur. Ainsi, plusieurs fenêtres pourraient être affichées en même temps à des endroits différents dans l'espace de la vision d'un utilisateur.\\
Pour explorer les capacités des visiocasques, \citeauthor{Ens2014} réalisent un prototype de RA affichant de multiples fenêtres dans les airs \reffigureETSp{Ens2014.jpg}. Un utilisateur peut alors interagir directement avec les fenêtres par le toucher (c'est donc une \emph{interaction directe}). Ils explorent alors plusieurs facteurs de conception du prototype, au travers de quatre expériences : la taille d'affichage des fenêtres virtuelles, leur distance d'affichage, leur angle de placement par rapport à l'utilisateur, et enfin leur référentiel de placement des fenêtres virtuelle (fixées au corps ou fixées au monde). Les performances (temps et taux d'erreurs), la fatigue ainsi que les retours des utilisateurs étaient mesurés.\\ 
Leurs résultats ont montrés, entre autres, qu'un affichage courbe est important pour que toutes les fenêtres soient affichées à la même distance de l'utilisateur~; ce qui confirme les résultats de \citet{ShuppBallYostEtAl2006} pour les affichages virtuels. En outre, leur conception a montré qu'elle permettait un travail 40\% plus rapide qu'avec une fenêtre simple sur une tache multi-applications : en effet, leur prototype permet d'afficher plusieurs fenêtres simultanément et ainsi réduit le changement de fenêtre, donc d'application pour du multi-tâches, à un simple mouvement de tête. Ce résultat montre également que l'interaction directe est compatible avec du contenu virtuel sous forme de fenêtres. Enfin, le référentiel sur le corps avait de plus grands taux d'erreurs que le référentiel sur le monde, car l'action de sélection entraînait des perturbations involontaires des fenêtres par rapport à l'utilisateur.\\ 
Ce travail présente cependant quelques limites. Tout d'abord, leur IHM requiert des interactions directes, ce qui est à l'origine d'erreurs de sélection et de fatigue du bras des utilisateurs : il serait alors intéressant de savoir si un référentiel sur le corps permettrait d'être précis avec une interaction indirecte. En outre, le champ de vision était très limité : 30° à l'horizontal et 40° à la verticale, ce qui biaise les mesures de performances Enfin, le prototype a été réalisé dans un CAVE, c'est-à-dire des projections de l'image sur des écrans entourant l'utilisateur. Ainsi, il faudrait reproduire les expérimentations avec un visiocasque et différentes techniques d'interactions. On peut également se demander si l'utilisation de fenêtres est une représentation adaptée pour la RA, aucune contrainte d'écrans physiques n'étant présente, ou s'il existe une meilleure représentation de l'information virtuelle dans un espace de RA.

\figureETS{Ens2014.jpg}{
  À gauche, photo du Personal Cockpit. À droite, illustration du positionnement idéal de fenêtres virtuelles.\\
  Tiré de \citet{Ens2014}.
}

Dans un article plus récent, \cite{Grubert2015} constatent qu'il devient commun d'avoir plusieurs appareils mobiles sur soi, tels que les téléphones intelligents, les tablettes ou les montres connectées. Cependant, ces appareils sont conçus pour être utilisés seuls et pas pour qu'un utilisateur puisse interagir en même temps sur leurs multiples affichages.\\
\citeauthor{Grubert2015} proposent alors d'utiliser les visiocasques pour joindre les affichages et les interactions entre ces différents appareils : les entrées et les sorties de tous les appareils sont liées \reffigureETSp{Grubert2015.jpg}. Ainsi, ce travail prend la suite de celui de \citet{Ens2014}, en situant les fenêtres virtuelles autour des affichages des appareils de l'utilisateur. L'objectif de leur système est de permettre des actions faciles et rapides dans les tâches qui impliquent de multiples appareils mobiles. Ils explorent alors dans cet article ce nouvel espace de conception créé et mènent des expérimentations auprès d'utilisateurs pour valider leurs propositions de conception par rapport aux appareils utilisés seuls.\\
Un résultat intéressant de leur exploration sont les degrés possibles de couplage des appareils. Dans le premier mode dit \emph{body-aligned}, le contenu virtuel a pour référence le corps de l'utilisateur, les appareils mobiles servant comme fenêtre haute résolution en pointant sur ce contenu : la technique utilisée est \emph{overview+detail}. Dans le second mode dit \emph{device-aligned}, le contenu a pour référence un appareil mobile, le visiocasque augmentant son écran avec un affichage virtuel : le visiocasque étant de moins bonne résolution que l'appareil mobile, c'est une technique appelée \emph{focus+context} (Voir \cite{BaudischGoodStewart2001}). Enfin dans mode dit \emph{side-by-side}, les interactions entre les appareils se font sans lien spatial : par exemple, l'utilisateur se sert du téléphone pour interagir indirectement avec des informations affichées par son visiocasque.\\
Leurs résultats expérimentaux montrent que l'augmentation des appareils en RA par un visiocasque peut permettre des temps plus rapides par rapport aux appareils seuls (visiocasque seul ou téléphone seul), dans les taches de navigation et de sélection, mais au détriment d'un plus grand effort perçu par les utilisateurs. Un second résultat expérimental important est que les préférences des utilisateurs sont variées entre utiliser les appareils seuls, coupler les entrées d'un appareil aux sorties de l'autre sans lien spatial, et coupler spatialement les appareils : cela montre qu'il est important de proposer tout ce spectre pour que chaque utilisateur organise l'IHM selon ses préférences et selon le contexte.\\
Une limite importante de cet article sont de faibles résultats expérimentaux. En effet, le système testé était lourd, peu mobile, avait une grande latence (150 ms), un champ de vision limité : ainsi, les résultats ont seulement permis de montrer que coupler les affichages des appareils \emph{pourrait} permettre de meilleures performances que les appareils utilisés seuls. En outre, le système n'a été pensé que pour certaines tâches précises, et non pour un couplage global et constant dans le temps. Il serait donc intéressant de généraliser le concept, et de le répliquer avec un visiocasque plus léger et mobile pour obtenir des résultats avec de meilleures validités internes (en évitant les problèmes techniques des expérimentations de l'article) et externes (plus de taches testés, conception pensée pour un couplage global et constant dans le temps).

\figureETS{Grubert2015.jpg}{
  Illustrations du fonctionnement de Multifi.\\
  Tiré de \citet{Grubert2015}.
}

Ens2014a - Ethereal Planes : cadre de conceptions pour des fenetres 2D dans un espace de travail en RA. Redonner application au Personal Cockpit

Dans un article suivant, \cite{Serrano2015} généralisent le travail de conception réalisé avec MultiFi \citep{Grubert2015}. Ils proposent pour cela Gluey : une IHM qui utilise le visiocasque comme affichage pour unifier les entrées et sorties de tous les appareils, qu'ils soient mobiles ou de bureau. \citeauthor{Serrano2015} s'appuient pour cela sur les propriétés de l'affichage permanent aux yeux de l'utilisateur du visiocasque et de sa connaissance de sa position dans les environnements réels et virtuels. Ainsi, c'est un médium idéal de transmission de l'information et de redirection des entrées entre les appareils \reffigureETSp{Serrano2015.jpg}.
Un utilisateur peut donc interagir sur un appareil, et voir ses actions s'exécuter sur un autre appareil qu'il regarde. Il est intéressant que le système permette que n'importe quel appareil puisse être utilisé de manière transparente et flexible comme un dispositif d'entrée (souris, téléphone, tablette). Le système permet également de transmettre des données, par exemple pour copier des données ou les imprimer.\\
\citeauthor{Serrano2015} présentent avec ce travail un ensemble de pistes de conception pour un tel système. Leur idée était de pouvoir créer des interactions invisibles entre les appareils, pour faciliter les taches demandant d'utiliser plusieurs appareils. Pour cela, l'IHM doit permettre de : 
\begin{itemize}
  \item rediriger les entrées entre les appareils
  \item migrer du contenu entre les appareils
  \item tous les appareils doivent être compatibles
  \item d'enregistrer de nouveaux appareils
  \item de tenir un modèle spatial du système
  \item les retours du système doit toujours être visibles (ici par le visiocasque)
  \item le système doit être mobile
\end{itemize}. Ainsi, MultiFi \citep{Grubert2015} ne satisfait qu'aux critères 1, 5, 6 et 7.\\
Une limite toutefois de l'article relève dans la faiblesse de son évaluation, informelle, qui a seulement pu montrer que le concept et la preuve de concept réalisée étaient enthousiasmantes et intéressantes aux yeux des participants. Une évaluation formelle devrait être reconduite avec un prototype léger et totalement mobile. Le visiocasque présentant en outre des limites de champs de vision et de résolution : si ces caractéristiques vont s'améliorer dans le futur, nous pensons que leur impact sur la navigation et les performances des interactions des utilisateurs n'est pas encore bien connue.

\figureETS{Serrano2015.jpg}{
  Illustration du concept de Gluey.\\
  Tiré de \citet{Serrano2015}.
}

\citet{Serrano2015a} ont par la suite complété leur travail, en proposant Desktop-Gluey : ce concept étend les possibilités de Gluey en permettant à l'utilisateur d'étendre les écrans physiques de ses appareils par des fenêtres virtuelles. Les fenêtres peuvent être arrangés dans l'espace autour des écrans physique par l'utilisateur. Le système autorise donc le travail collaboratif en permettant de partager des fenêtres virtuelles entre plusieurs utilisateurs \reffigureETSp{Serrano2015a.jpg}. Enfin, les fenêtres peuvent suivre l'utilisateur dans ses déplacements et lui permettre d'interagir avec en utilisant son téléphone, une tablette ou des gestes de la main ou pour interagir, créant ainsi un concept de bureau mobile \reffigureETSp{Serrano2015a.jpg}. La métaphore du bureau sur l'ordinateur personnel est donc reprise et augmentée à \"partout\" et en \"tout temps\" : tout dispositif d'entrée peut être utilisé et tout dispositif d'entrée peut être augmenté dans son affichage.\\
Le concept recoupe celui de MultiFi \citep{Grubert2015}, et le généralise des appareils mobiles seuls à tous les appareils visibles par l'utilisateur. Une autre différence majeure est que MultiFi a été conçu pour faciliter les taches demandant d'utiliser plusieurs appareils, alors que Desktop-Gluey propose de travailler avec ces fenêtres virtuelles : on peut dire que c'est une application du Personal Cockpit \citep{EnsFinneganIrani2014} dans une IHM tangible et située autour d'appareils mobiles (comme MultiFi).\\
\citeauthor{Serrano2015a} ne présentent dans cet article que le concept mais n'en réalise aucun prototype, ni aucune évaluation. Nous pensons qu'il serait intéressant de montrer que ce concept fonctionne et d'implémenter ce système de bureau virtuel et mobile, afin d'explorer son espace de conception et d'en ajuster au mieux les paramètres.

\figureETS{Serrano2015a.jpg}{
  Illustrations du concept de Desktop-Gluey : à gauche, en utilisation de fenêtres virtuelles pour étendre des écrans physiques ; à droite, en mode mobile.\\
  Tiré de \citet{Serrano2015a}
}


\section{Affichages étendus}
\subsection{Combiner table tactile}
Bi2011 - MagicDesk : augmenter un clavier avec une table tactile

Houben2014 - ActivitySpace : augmenter plusieurs appreils (pc, téléphones) posés sur une table tactile

\subsection{Grands affichages}
Baudisch2002 - Keeping things in context: a comparative evaluation of focus plus context screens, overviews, and zooming

Liu2014 - Effects of display size and navigation type on a classification task : est-ce que les connaissances sur les écrans de taille d'un ordinateur s'appliquent à ces nouveaux écrans ? Ces nouveaux écrans ont la même haute densité que les écrans de bureau, mais leur résolution est généralement 10 fois élevée en nombre de pixels : un utilisateur doit donc s'approcher physiquement pour pouvoir voir le détail et reculer pour avoir une vue d'ensemble. (voir fin cahier gris)\\
« while the desktop can be faster than the wall for simple tasks, the wall gains a sizable advantage as the task becomes more difficult. A follow-up study shows that other desktop techniques (overview+detail, lens) do not perform better than pan-and-zoom and are therefore slower than the wall for difficult tasks (manupilating elements in a complex decision making task tha requires expertise and quick access to full content) -> eg. third task of personal cockpit or google maps (the typical task with wedge, or overview+detail) »

\subsection{Extension du champs de vision de visiocasques de RA}
Benko2015 - FoveAR

Xiao2016 - Sparse Peripheral Displays


\section{Problématique}
Cette revue de littérature a permit d'identifier un besoin de conception d'IHM en RA s'appuyant sur des visiocasques. En particulier, nous souhaitons explorer la conception d'IHM pour un système combinant un téléphone intelligent avec un visiocasque de RA. Ainsi, on définit la problématique de ce mémoire ainsi :

\begin{displayquote}
  Est-ce qu'un téléphone augmenté par un VESAD donne un avantage à un utilisateur par rapport à un téléphone seul ? Quelles seraient les meileures techniques d'interactions à utiliser sur un tel téléphone augmenté ?
\end{displayquote}

Nous formulons les hypothèses suivantes par rapport à cette problématique :
\begin{enumerate}[label={(H\arabic*)}]
  \item Notre système	permet d'être plus performant sur des tâches de navigation, de classification ou demandant d'utiliser plusieurs applications en parallèle que sur un téléphone seul, quelle que soit la technique d'interaction utilisée.
  \item Les utilisateurs apprécieront d'avantage pouvoir interagir directement avec l'écran étendu autour du téléphone sur notre système.
  \item Les utilisateurs seront en revanche plus performants en interagissant seulement avec l'écran tactile du téléphone sur notre système.
\end{enumerate}

Enfin, pour y répondre, nous divisons cette problématique en quatre sous-problèmes :
\begin{enumerate}
  \item Concevoir une IHM d'un téléphone à l'écran aggrandi par RA.
  \item Développer un visiocasque de RA à large champs de vision.
  \item Développer un prototype de cette IHM à l'aide du visiocasque.
  \item Réaliser une expérimentation évaluant différentes interfaces et techniques d'interactions sur ce prototype sur une tâche de classification.
\end{enumerate}

Les résultats à ces objectifs permettront de donner des recommandations pour de futures recherches d'IHM en RA.